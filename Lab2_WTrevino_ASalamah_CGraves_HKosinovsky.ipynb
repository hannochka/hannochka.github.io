{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2 \n",
    "#### Project by William Trevino, Alex Salamah, Christopher Graves, Hannah Kosinovsky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>student_num</th>\n",
       "      <th>lea_avg_student_num</th>\n",
       "      <th>st_avg_student_num</th>\n",
       "      <th>Math I_Size</th>\n",
       "      <th>lea_total_expense_num</th>\n",
       "      <th>lea_salary_expense_pct</th>\n",
       "      <th>lea_services_expense_pct</th>\n",
       "      <th>lea_supplies_expense_pct</th>\n",
       "      <th>lea_instruct_equip_exp_pct</th>\n",
       "      <th>...</th>\n",
       "      <th>esea_status_P</th>\n",
       "      <th>Grad_project_status_Y</th>\n",
       "      <th>SPG Grade_B</th>\n",
       "      <th>SPG Grade_C</th>\n",
       "      <th>SPG Grade_D</th>\n",
       "      <th>SPG Grade_F</th>\n",
       "      <th>SPG Grade_I</th>\n",
       "      <th>EVAAS Growth Status_Met</th>\n",
       "      <th>EVAAS Growth Status_NotMet</th>\n",
       "      <th>unit_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>954.0</td>\n",
       "      <td>837.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.011</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>539.0</td>\n",
       "      <td>518.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.011</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>547.0</td>\n",
       "      <td>518.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.011</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>800.0</td>\n",
       "      <td>768.0</td>\n",
       "      <td>665.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.011</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>664.0</td>\n",
       "      <td>518.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.011</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 255 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  student_num  lea_avg_student_num  st_avg_student_num  \\\n",
       "0           0         78.0                954.0               837.0   \n",
       "1           1        539.0                518.0               496.0   \n",
       "2           2        547.0                518.0               496.0   \n",
       "3           3        800.0                768.0               665.0   \n",
       "4           4        664.0                518.0               496.0   \n",
       "\n",
       "   Math I_Size  lea_total_expense_num  lea_salary_expense_pct  \\\n",
       "0          0.0                8028.59                   0.613   \n",
       "1          0.0                8028.59                   0.613   \n",
       "2          0.0                8028.59                   0.613   \n",
       "3         26.0                8028.59                   0.613   \n",
       "4          0.0                8028.59                   0.613   \n",
       "\n",
       "   lea_services_expense_pct  lea_supplies_expense_pct  \\\n",
       "0                     0.078                     0.086   \n",
       "1                     0.078                     0.086   \n",
       "2                     0.078                     0.086   \n",
       "3                     0.078                     0.086   \n",
       "4                     0.078                     0.086   \n",
       "\n",
       "   lea_instruct_equip_exp_pct    ...      esea_status_P  \\\n",
       "0                       0.011    ...                  0   \n",
       "1                       0.011    ...                  0   \n",
       "2                       0.011    ...                  0   \n",
       "3                       0.011    ...                  0   \n",
       "4                       0.011    ...                  0   \n",
       "\n",
       "   Grad_project_status_Y  SPG Grade_B  SPG Grade_C  SPG Grade_D  SPG Grade_F  \\\n",
       "0                      0            0            0            0            0   \n",
       "1                      0            0            1            0            0   \n",
       "2                      0            0            1            0            0   \n",
       "3                      0            0            0            0            1   \n",
       "4                      0            1            0            0            0   \n",
       "\n",
       "   SPG Grade_I  EVAAS Growth Status_Met  EVAAS Growth Status_NotMet  unit_code  \n",
       "0            0                        0                           0      10303  \n",
       "1            0                        0                           0      10304  \n",
       "2            0                        1                           0      10308  \n",
       "3            0                        0                           1      10310  \n",
       "4            0                        0                           0      10312  \n",
       "\n",
       "[5 rows x 255 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "#InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---We imported in data we previously cleaned in Lab 1 but for consistincy sake we included the code we used for the cleaning below---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base = 'MSDS 7331/Dataset/EducationDataNC-master/'\n",
    "file2014 = base + '2014/Machine Learning Datasets/PublicSchools2014_ML.csv'\n",
    "file2015 = base + '2015/Machine Learning Datasets/PublicSchools2015_ML.csv'\n",
    "file2016 = base + '2016/Machine Learning Datasets/PublicSchools2016_ML.csv'\n",
    "file2017 = base + '2017/Machine Learning Datasets/PublicSchools2017_ML.csv'\n",
    "df2014 = pd.read_csv(file2014)\n",
    "df2015 = pd.read_csv(file2015)\n",
    "df2016 = pd.read_csv(file2016)\n",
    "df2017 = pd.read_csv(file2017)\n",
    "\n",
    "df_full = pd.concat([df2014,df2015,df2016,df2017], axis=0, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Since the dataset we created is the combination of the 2014-2017 pre cleaned Machine Learning data sets found in the EducationDataNC repository, it  has already been cleared of missing data, duplicate data and outliers so none are present to fix.  However, we may have created NaN values when we concatinated the 4 datasets if there was one year where the data did not have a feature found in another year. \n",
    "\n",
    "###### We checked this below and found that while the 4 separate datasets had 0 nulls, our combined dataset had 931697 null values. \n",
    "\n",
    "###### WHILE we are not using all of hte attributes in this analysis we will clean the data in case we wish to explorer these exluded features later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We wanted to work with data that was present across all years. Meaning, we didn't want to include features in our model that were only measured sometimes between 2014-2017. Therefore, we decided to use the method shown below in order for our approach to be more balanced. \n",
    "\n",
    "#### Since we are dealing with 4 years it is expected that if a feature only exists in one year it would appear in at least 25% if all years had equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_temp = df_full.loc[:, df_full.isnull().mean() < .20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we want to remove the outliers by only keeping values within 3 standard deviations of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9722.000000\n",
       "mean        0.733897\n",
       "std         1.862658\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.250000\n",
       "75%         0.830000\n",
       "max        36.360000\n",
       "Name: crime_per_c_num, dtype: float64"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compare = df_temp[np.abs(df_temp-df_temp.mean()) <= (3*df_temp.std())]\n",
    "df_temp.shape\n",
    "df_compare.shape\n",
    "df_temp.equals(df_compare)\n",
    "df_temp['crime_per_c_num'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9589.000000\n",
       "mean        0.561148\n",
       "std         0.833770\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.240000\n",
       "75%         0.800000\n",
       "max         6.250000\n",
       "Name: crime_per_c_num, dtype: float64"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compare['crime_per_c_num'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62803"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compare.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output of the isnull() function above we see that there were a lot of values outside of 6 standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9731, 257)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full = df_compare\n",
    "df_full.shape\n",
    "df_temp.shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Part 2\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     9731.000000\n",
       "mean      8992.041761\n",
       "std       1023.850284\n",
       "min       7282.190000\n",
       "25%       8362.900000\n",
       "50%       8779.340000\n",
       "75%       9326.890000\n",
       "max      17801.850000\n",
       "Name: total_perpupil_num, dtype: float64"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()\n",
    "df['total_perpupil_num'] = df['lea_federal_perpupil_num'] + df['lea_local_perpupil_num'] + df['lea_state_perpupil_num']\n",
    "df['sat_above_average'] = np.where(df['lea_sat_avg_score_num'] > 1060, 1, 0)\n",
    "df['total_perpupil_num'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our variables:\n",
    "\n",
    "student_num - Number of students at school level - **float64 ranging from 1 to 2974**\n",
    "\n",
    "\n",
    "crime_per_c_num - Number of crimes or acts of violence per 100 students at School level - **float64 ranging from 0 to 1254**\n",
    "\n",
    "\n",
    "advance_dgr_pct - Percent of teachers with masters or higher degree at school level - **float64 ranging from 0 to 0.81**\n",
    "\n",
    "\n",
    "_1yr_tchr_trnovr_pct - One Year Teacher turnover percentage at school level - **float64 ranging from 0 to 0.82**\n",
    "\n",
    "\n",
    "lea_sat_avg_score_num - Average SAT Score (Critical Reading + Math) at the LEA Level. Local education agency, LEA, is a commonly used acronym for a school district.-  **float64 ranging from 588 to 2974**\n",
    "\n",
    "\n",
    "lea_federal_perpupil_num - Federal expense per pupil at school level - **float64 ranging from 588 to 2974**\n",
    "\n",
    "\n",
    "lea_local_perpupil_num - Local expense per pupil at LEA level - **float64 ranging from 588 to 2974**\n",
    "\n",
    "\n",
    "lea_state_perpupil_num - State expense per pupil at LEA level -  **float64 ranging from 588 to 2974**\n",
    "\n",
    "\n",
    "lea_salary_expense_pct - Percent of expense spent on Salaries at LEA level - **float64  ranging from 0.55 to 0.88**\n",
    "\n",
    "\n",
    "lea_supplies_expense_pct - Percent of expense spent on Supplies at LEA level - **float64  ranging from 0.03 to 0.14**\n",
    "\n",
    "\n",
    "lea_ap_participation_pct - Percentage of High School Students taking an AP exam at the LEA Level - **float64 ranging from 0 to 0.44**\n",
    "\n",
    "\n",
    "lea_sat_participation_pct - Percentage of High School Seniors taking the SAT at the LEA Level - **float64 ranging from 0 to 0.82**\n",
    "\n",
    "MinorityFemalePct - Percentage of students who are a minority race and female - **float64  ranging from 0 to .99**\n",
    "\n",
    "\n",
    "MinorityMalePct - - Percentage of students who are a minority race and male - **float64 ranging from 0 to 1**\n",
    "\n",
    "\n",
    "lea_avg_daily_attend_pct - Average daily attendance percentage at LEA level    - **float64 ranging from 0 to 0.99**\n",
    "\n",
    "\n",
    "###### New variables\n",
    "total_perpupil_num - a combined field of federal, state, and local per pupil funding - **float64 raning from 7282.19 to 17801.85 \n",
    "\n",
    "\n",
    "sat_above_average - 0/1 boolean value of if the SAT is above the 1060 US average - **float65 ranging from 0 to 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose and explain your evaluation metrics that you will use (i.e., accuracy,\n",
    "precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### In our regression models we we will be predicting SAT scores and in our classification models we will be classifying above and below average SAT scores.\n",
    "\n",
    "###### We expect that these models can be used to evaluate the potential of students in different schools and maybe see where curriculum can improve and where students are lacking.\n",
    "\n",
    "###### Classification model: We chose accuracy as the evaluation metric for our classification model.  Since we are attempting to classify the sat_above_average field, both the true positve (SAT score is above average) and the true negative (SAT score is below average) prediction rates are needed.  For this reason the (true postive + true negative) / total metric (accuracy) is most approtiate metric for this problem.\n",
    "\n",
    "###### Regression model: We chose MSE (mean square error) as the evaluation metric for our regression tasks. We originally wanted to use MAE as it is very easy to interpret given the values are not modified in scale compared to the actuals. Given the fact that some sklearn modules perform significantly worse in execution time when using MAE over MSE, we decided to use MSE as our metric. For all calculations we recorded the MSE (mean square error) and MAE for comparision's sake later on in the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the method you will use for dividing your data into training and\n",
    "testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why\n",
    "your chosen method is appropriate or use more than one method as appropriate. For\n",
    "example, if you are using time series data then you should be using continuous training\n",
    "and testing sets across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "df_foo = df.drop(columns=['lea_sat_avg_score_num', 'sat_above_average'])\n",
    "X = df_foo.values\n",
    "y = df['lea_sat_avg_score_num'].values #this is the subset for regression\n",
    "shuffle_split_reg = StratifiedShuffleSplit(n_splits=10, test_size=0.5, random_state=101)\n",
    "shuffle_split_reg.get_n_splits(X, y)\n",
    "\n",
    "X = df_foo.values\n",
    "y = df['sat_above_average'].values #this is the subset for classification\n",
    "shuffle_split_clas = StratifiedShuffleSplit(n_splits=10, test_size=0.5, random_state=101)\n",
    "shuffle_split_clas.get_n_splits(X, y)\n",
    "\n",
    "for train_index, test_index in shuffle_split_clas.split(X, y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index) #this is the index loop\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For our model training and testing split we will be using a 10 fold cross validation with stratified split.  We had previously noted in our data set that many of our features, including our classification target feature, had very skewed distributions in values, this is why we picked the stratified split. So, to avoid the fear of one or more of our CV splits being unbalanced and thus unhelpful we opted for a split that guaranteed preservation of each sample class.   Also, we made sure to only create our CV splits once and reuse the object to avoid any unfair comparisons between our models in later sections.  The potential drawback of our 10 fold validation is that the more folds we have the longer the computation will take because of the increased number of trained moels. In addition, the performance metric, overfitting, and variance generally decrease as folds increase. After we run the models we can analyze our accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create three different classification/regression models for each task (e.g.,\n",
    "random forest, KNN, and SVM for task one and the same or different algorithms for\n",
    "task two). Two modeling techniques must be new (but the third could be SVM or\n",
    "logistic regression). Adjust parameters as appropriate to increase generalization\n",
    "performance using your chosen metric. You must investigate different parameters\n",
    "of the algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Models: Logistic Regression, KNN, Random Forest Classifier, Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_cv(df, model_name, metric):\n",
    "    tmin = round(df.loc[df['Model Name'] == model_name][metric].min(),4)\n",
    "    tmax = round(df.loc[df['Model Name'] == model_name][metric].max(),4)\n",
    "    tmean = round(df.loc[df['Model Name'] == model_name][metric].mean(),4)\n",
    "    tstd = round(df.loc[df['Model Name'] == model_name][metric].std(),4)\n",
    "    print(metric)\n",
    "    print(\"Min:\" + str(tmin) + \"\\tMax:\" + str(tmax) + \"\\tMean:\" + str(tmean) + \"\\tStd:\" + str(tstd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_stats = pd.DataFrame( columns = ['Type', 'Model Name','Paramaters','CV','Accuracy', 'Execution Time', 'True Positive', 'False Positive', 'False Negative', 'True Negative', 'Precision', 'Recall', 'Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def run_model_class(shuffle_split, model_name, model_para, model):\n",
    "    count = 1\n",
    "    for train_index, test_index in shuffle_split_clas.split(X, y): \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        start = time.time()\n",
    "        model.fit(X_train,y_train)  \n",
    "        y_hat = model.predict(X_test) \n",
    "\n",
    "        acc = mt.accuracy_score(y_test,y_hat)\n",
    "        conf = mt.confusion_matrix(y_test,y_hat)\n",
    "        #print(\"accuracy\", acc )\n",
    "        #print(\"confusion matrix\\n\",conf)\n",
    "        class_stats.loc[len(class_stats)] =  ['classification', model_name, model_para, count, acc, round(time.time() - start,4), conf[0,0], conf[0,1], conf[1,0], conf[1,1], conf[0,0] / ( conf[0,0] + conf[0,1] ), conf[0,0] / ( conf[0,0] + conf[1,0]), model]\n",
    "        count = count + 1\n",
    "    avg_acc = round(class_stats.loc[(class_stats['Model Name'] == model_name) & (class_stats['Paramaters'] == model_para)]['Accuracy'].mean(),4)\n",
    "    avg_ex = round(class_stats.loc[(class_stats['Model Name'] == model_name) & (class_stats['Paramaters'] == model_para)]['Execution Time'].mean(),4)\n",
    "    avg_tp = round(class_stats.loc[(class_stats['Model Name'] == model_name) & (class_stats['Paramaters'] == model_para)]['True Positive'].mean(),4)\n",
    "    avg_fp = round(class_stats.loc[(class_stats['Model Name'] == model_name) & (class_stats['Paramaters'] == model_para)]['False Positive'].mean(),4)\n",
    "    avg_fn = round(class_stats.loc[(class_stats['Model Name'] == model_name) & (class_stats['Paramaters'] == model_para)]['False Negative'].mean(),4)\n",
    "    avg_tn = round(class_stats.loc[(class_stats['Model Name'] == model_name) & (class_stats['Paramaters'] == model_para)]['True Negative'].mean(),4)\n",
    "    avg_p = round(class_stats.loc[(class_stats['Model Name'] == model_name) & (class_stats['Paramaters'] == model_para)]['Precision'].mean(),4)\n",
    "    avg_r = round(class_stats.loc[(class_stats['Model Name'] == model_name) & (class_stats['Paramaters'] == model_para)]['Recall'].mean(),4)\n",
    "    class_stats.loc[len(class_stats)] =  ['classification', model_name, model_para, 'avg', avg_acc, avg_ex, avg_tp, avg_fp, avg_fn, avg_tn, avg_p, avg_r, None]\n",
    "    print(model_name + \" \" + model_para)\n",
    "#    eval_cv(class_stats, model_name, 'Accuracy')\n",
    "#   eval_cv(class_stats, model_name, 'Execution Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The following three blocks of code are setup functions needed to more easily train each model.  The first function eval_cv is used to get basic statistics out of a feature on a give model accross all of the CV splits.  The second line initiates a data frame for holding all model related information including the model itself.  The third block run_model_class function is a wrapper function that runs each CV for a given model, takes all evaluation metrics and creates an average performance score for each model based on the average CV performance that is also stored in the dataframe for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic L1 C=0.1 fit_intercept=True\n",
      "Logistic L2 C=0.1 fit_intercept=True\n",
      "Logistic L1 C=0.1 fit_intercept=False\n",
      "Logistic L2 C=0.1 fit_intercept=False\n",
      "Logistic L1 C=0.2 fit_intercept=True\n",
      "Logistic L2 C=0.2 fit_intercept=True\n",
      "Logistic L1 C=0.2 fit_intercept=False\n",
      "Logistic L2 C=0.2 fit_intercept=False\n",
      "Logistic L1 C=0.3 fit_intercept=True\n",
      "Logistic L2 C=0.3 fit_intercept=True\n",
      "Logistic L1 C=0.3 fit_intercept=False\n",
      "Logistic L2 C=0.3 fit_intercept=False\n",
      "Logistic L1 C=0.4 fit_intercept=True\n",
      "Logistic L2 C=0.4 fit_intercept=True\n",
      "Logistic L1 C=0.4 fit_intercept=False\n",
      "Logistic L2 C=0.4 fit_intercept=False\n",
      "Logistic L1 C=0.5 fit_intercept=True\n",
      "Logistic L2 C=0.5 fit_intercept=True\n",
      "Logistic L1 C=0.5 fit_intercept=False\n",
      "Logistic L2 C=0.5 fit_intercept=False\n",
      "Logistic L1 C=0.6 fit_intercept=True\n",
      "Logistic L2 C=0.6 fit_intercept=True\n",
      "Logistic L1 C=0.6 fit_intercept=False\n",
      "Logistic L2 C=0.6 fit_intercept=False\n",
      "Logistic L1 C=0.7 fit_intercept=True\n",
      "Logistic L2 C=0.7 fit_intercept=True\n",
      "Logistic L1 C=0.7 fit_intercept=False\n",
      "Logistic L2 C=0.7 fit_intercept=False\n",
      "Logistic L1 C=0.8 fit_intercept=True\n",
      "Logistic L2 C=0.8 fit_intercept=True\n",
      "Logistic L1 C=0.8 fit_intercept=False\n",
      "Logistic L2 C=0.8 fit_intercept=False\n",
      "Logistic L1 C=0.9 fit_intercept=True\n",
      "Logistic L2 C=0.9 fit_intercept=True\n",
      "Logistic L1 C=0.9 fit_intercept=False\n",
      "Logistic L2 C=0.9 fit_intercept=False\n",
      "Logistic L1 C=1.0 fit_intercept=True\n",
      "Logistic L2 C=1.0 fit_intercept=True\n",
      "Logistic L1 C=1.0 fit_intercept=False\n",
      "Logistic L2 C=1.0 fit_intercept=False\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "model_name = 'Logistic'\n",
    "for C in range(1, 11, 1):\n",
    "    model_para = 'L1 C=' + str(float(C/10)) + ' fit_intercept=True'\n",
    "    lr_clf = LogisticRegression(n_jobs=1, random_state=101, penalty='l1', C=float(C/10), class_weight=None, solver='liblinear', fit_intercept=True) \n",
    "    run_model_class(shuffle_split_clas, model_name, model_para, lr_clf )\n",
    "    model_para = 'L2 C=' + str(float(C/10)) + ' fit_intercept=True'\n",
    "    lr_clf = LogisticRegression(n_jobs=1, random_state=101, penalty='l2', C=float(C/10), class_weight=None, solver='liblinear', fit_intercept=True) \n",
    "    run_model_class(shuffle_split_clas, model_name, model_para, lr_clf )\n",
    "    model_para = 'L1 C=' + str(float(C/10)) + ' fit_intercept=False'\n",
    "    lr_clf = LogisticRegression(n_jobs=1, random_state=101, penalty='l1', C=float(C/10), class_weight=None, solver='liblinear', fit_intercept=False) \n",
    "    run_model_class(shuffle_split_clas, model_name, model_para, lr_clf )\n",
    "    model_para = 'L2 C=' + str(float(C/10)) + ' fit_intercept=False'\n",
    "    lr_clf = LogisticRegression(n_jobs=1, random_state=101, penalty='l2', C=float(C/10), class_weight=None, solver='liblinear', fit_intercept=False)\n",
    "    run_model_class(shuffle_split_clas, model_name, model_para, lr_clf )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Below are the preformance results for our classification model trained above sorted by our Accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Paramaters</th>\n",
       "      <th>CV</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Execution Time</th>\n",
       "      <th>True Positive</th>\n",
       "      <th>False Positive</th>\n",
       "      <th>False Negative</th>\n",
       "      <th>True Negative</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=1.0 fit_intercept=True</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9601</td>\n",
       "      <td>1.5609</td>\n",
       "      <td>3797</td>\n",
       "      <td>85</td>\n",
       "      <td>109.2</td>\n",
       "      <td>874.8</td>\n",
       "      <td>0.9781</td>\n",
       "      <td>0.9721</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=1.0 fit_intercept=False</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9599</td>\n",
       "      <td>1.8262</td>\n",
       "      <td>3796.8</td>\n",
       "      <td>85.2</td>\n",
       "      <td>110</td>\n",
       "      <td>874</td>\n",
       "      <td>0.9781</td>\n",
       "      <td>0.9719</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=0.9 fit_intercept=False</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9598</td>\n",
       "      <td>1.8347</td>\n",
       "      <td>3796.5</td>\n",
       "      <td>85.5</td>\n",
       "      <td>110.1</td>\n",
       "      <td>873.9</td>\n",
       "      <td>0.9780</td>\n",
       "      <td>0.9718</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=0.9 fit_intercept=True</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9595</td>\n",
       "      <td>1.4958</td>\n",
       "      <td>3796.3</td>\n",
       "      <td>85.7</td>\n",
       "      <td>111.3</td>\n",
       "      <td>872.7</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>0.9715</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=0.8 fit_intercept=True</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9594</td>\n",
       "      <td>1.8464</td>\n",
       "      <td>3795.8</td>\n",
       "      <td>86.2</td>\n",
       "      <td>111.2</td>\n",
       "      <td>872.8</td>\n",
       "      <td>0.9778</td>\n",
       "      <td>0.9716</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=0.8 fit_intercept=False</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9593</td>\n",
       "      <td>1.5242</td>\n",
       "      <td>3795.6</td>\n",
       "      <td>86.4</td>\n",
       "      <td>111.8</td>\n",
       "      <td>872.2</td>\n",
       "      <td>0.9777</td>\n",
       "      <td>0.9714</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=0.7 fit_intercept=True</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9589</td>\n",
       "      <td>1.5738</td>\n",
       "      <td>3794.7</td>\n",
       "      <td>87.3</td>\n",
       "      <td>112.7</td>\n",
       "      <td>871.3</td>\n",
       "      <td>0.9775</td>\n",
       "      <td>0.9712</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=0.7 fit_intercept=False</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9589</td>\n",
       "      <td>1.4565</td>\n",
       "      <td>3794.6</td>\n",
       "      <td>87.4</td>\n",
       "      <td>112.8</td>\n",
       "      <td>871.2</td>\n",
       "      <td>0.9775</td>\n",
       "      <td>0.9711</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=0.6 fit_intercept=True</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9581</td>\n",
       "      <td>1.7293</td>\n",
       "      <td>3792.2</td>\n",
       "      <td>89.8</td>\n",
       "      <td>114.3</td>\n",
       "      <td>869.7</td>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.9708</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=0.6 fit_intercept=False</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9581</td>\n",
       "      <td>1.3329</td>\n",
       "      <td>3792.9</td>\n",
       "      <td>89.1</td>\n",
       "      <td>114.7</td>\n",
       "      <td>869.3</td>\n",
       "      <td>0.9770</td>\n",
       "      <td>0.9707</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=0.5 fit_intercept=False</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9577</td>\n",
       "      <td>1.8154</td>\n",
       "      <td>3791.1</td>\n",
       "      <td>90.9</td>\n",
       "      <td>115.1</td>\n",
       "      <td>868.9</td>\n",
       "      <td>0.9766</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=0.5 fit_intercept=True</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9575</td>\n",
       "      <td>1.3475</td>\n",
       "      <td>3790.9</td>\n",
       "      <td>91.1</td>\n",
       "      <td>115.9</td>\n",
       "      <td>868.1</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>0.9703</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=0.4 fit_intercept=True</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9570</td>\n",
       "      <td>1.3215</td>\n",
       "      <td>3790.7</td>\n",
       "      <td>91.3</td>\n",
       "      <td>118.1</td>\n",
       "      <td>865.9</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>0.9698</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=0.4 fit_intercept=False</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9570</td>\n",
       "      <td>1.2907</td>\n",
       "      <td>3790.9</td>\n",
       "      <td>91.1</td>\n",
       "      <td>118.2</td>\n",
       "      <td>865.8</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>0.9698</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=0.3 fit_intercept=True</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9559</td>\n",
       "      <td>1.1836</td>\n",
       "      <td>3789.4</td>\n",
       "      <td>92.6</td>\n",
       "      <td>121.9</td>\n",
       "      <td>862.1</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.9688</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=0.3 fit_intercept=False</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9556</td>\n",
       "      <td>1.2029</td>\n",
       "      <td>3788.8</td>\n",
       "      <td>93.2</td>\n",
       "      <td>122.7</td>\n",
       "      <td>861.3</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>0.9686</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=0.2 fit_intercept=False</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9522</td>\n",
       "      <td>1.1125</td>\n",
       "      <td>3783.6</td>\n",
       "      <td>98.4</td>\n",
       "      <td>134.3</td>\n",
       "      <td>849.7</td>\n",
       "      <td>0.9747</td>\n",
       "      <td>0.9657</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=0.2 fit_intercept=True</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9520</td>\n",
       "      <td>1.0238</td>\n",
       "      <td>3783.2</td>\n",
       "      <td>98.8</td>\n",
       "      <td>134.7</td>\n",
       "      <td>849.3</td>\n",
       "      <td>0.9745</td>\n",
       "      <td>0.9656</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=0.1 fit_intercept=True</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.7430</td>\n",
       "      <td>3757</td>\n",
       "      <td>125</td>\n",
       "      <td>182.1</td>\n",
       "      <td>801.9</td>\n",
       "      <td>0.9678</td>\n",
       "      <td>0.9538</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=0.1 fit_intercept=False</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9368</td>\n",
       "      <td>0.6797</td>\n",
       "      <td>3756.8</td>\n",
       "      <td>125.2</td>\n",
       "      <td>182.3</td>\n",
       "      <td>801.7</td>\n",
       "      <td>0.9677</td>\n",
       "      <td>0.9537</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L2 C=0.3 fit_intercept=True</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9021</td>\n",
       "      <td>0.4241</td>\n",
       "      <td>3719.4</td>\n",
       "      <td>162.6</td>\n",
       "      <td>313.9</td>\n",
       "      <td>670.1</td>\n",
       "      <td>0.9581</td>\n",
       "      <td>0.9222</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L2 C=0.6 fit_intercept=True</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9020</td>\n",
       "      <td>0.4001</td>\n",
       "      <td>3718.8</td>\n",
       "      <td>163.2</td>\n",
       "      <td>313.9</td>\n",
       "      <td>670.1</td>\n",
       "      <td>0.9580</td>\n",
       "      <td>0.9222</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L2 C=0.4 fit_intercept=False</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9020</td>\n",
       "      <td>0.3837</td>\n",
       "      <td>3716.5</td>\n",
       "      <td>165.5</td>\n",
       "      <td>311.6</td>\n",
       "      <td>672.4</td>\n",
       "      <td>0.9574</td>\n",
       "      <td>0.9227</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L2 C=1.0 fit_intercept=True</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9020</td>\n",
       "      <td>0.4076</td>\n",
       "      <td>3720.1</td>\n",
       "      <td>161.9</td>\n",
       "      <td>315.1</td>\n",
       "      <td>668.9</td>\n",
       "      <td>0.9583</td>\n",
       "      <td>0.9219</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L2 C=0.7 fit_intercept=False</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9019</td>\n",
       "      <td>0.4380</td>\n",
       "      <td>3716.6</td>\n",
       "      <td>165.4</td>\n",
       "      <td>311.8</td>\n",
       "      <td>672.2</td>\n",
       "      <td>0.9574</td>\n",
       "      <td>0.9226</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L2 C=0.9 fit_intercept=False</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9018</td>\n",
       "      <td>0.4114</td>\n",
       "      <td>3718.2</td>\n",
       "      <td>163.8</td>\n",
       "      <td>314.1</td>\n",
       "      <td>669.9</td>\n",
       "      <td>0.9578</td>\n",
       "      <td>0.9221</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L2 C=0.5 fit_intercept=False</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9016</td>\n",
       "      <td>0.3959</td>\n",
       "      <td>3720.7</td>\n",
       "      <td>161.3</td>\n",
       "      <td>317.5</td>\n",
       "      <td>666.5</td>\n",
       "      <td>0.9584</td>\n",
       "      <td>0.9214</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L2 C=0.5 fit_intercept=True</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9016</td>\n",
       "      <td>0.4391</td>\n",
       "      <td>3716.6</td>\n",
       "      <td>165.4</td>\n",
       "      <td>313.2</td>\n",
       "      <td>670.8</td>\n",
       "      <td>0.9574</td>\n",
       "      <td>0.9223</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L2 C=0.7 fit_intercept=True</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9016</td>\n",
       "      <td>0.3941</td>\n",
       "      <td>3720.7</td>\n",
       "      <td>161.3</td>\n",
       "      <td>317.5</td>\n",
       "      <td>666.5</td>\n",
       "      <td>0.9584</td>\n",
       "      <td>0.9214</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L2 C=0.1 fit_intercept=True</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9016</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>3719.7</td>\n",
       "      <td>162.3</td>\n",
       "      <td>316.6</td>\n",
       "      <td>667.4</td>\n",
       "      <td>0.9582</td>\n",
       "      <td>0.9216</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L2 C=0.9 fit_intercept=True</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9016</td>\n",
       "      <td>0.3556</td>\n",
       "      <td>3722.3</td>\n",
       "      <td>159.7</td>\n",
       "      <td>318.9</td>\n",
       "      <td>665.1</td>\n",
       "      <td>0.9589</td>\n",
       "      <td>0.9211</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L2 C=0.4 fit_intercept=True</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9014</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>3719.3</td>\n",
       "      <td>162.7</td>\n",
       "      <td>316.9</td>\n",
       "      <td>667.1</td>\n",
       "      <td>0.9581</td>\n",
       "      <td>0.9215</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L2 C=0.3 fit_intercept=False</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9014</td>\n",
       "      <td>0.4095</td>\n",
       "      <td>3719</td>\n",
       "      <td>163</td>\n",
       "      <td>316.7</td>\n",
       "      <td>667.3</td>\n",
       "      <td>0.9580</td>\n",
       "      <td>0.9215</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L2 C=0.1 fit_intercept=False</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9013</td>\n",
       "      <td>0.4228</td>\n",
       "      <td>3716.3</td>\n",
       "      <td>165.7</td>\n",
       "      <td>314.7</td>\n",
       "      <td>669.3</td>\n",
       "      <td>0.9573</td>\n",
       "      <td>0.9219</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L2 C=0.8 fit_intercept=True</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9013</td>\n",
       "      <td>0.4445</td>\n",
       "      <td>3715.5</td>\n",
       "      <td>166.5</td>\n",
       "      <td>313.9</td>\n",
       "      <td>670.1</td>\n",
       "      <td>0.9571</td>\n",
       "      <td>0.9221</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L2 C=1.0 fit_intercept=False</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9012</td>\n",
       "      <td>0.4131</td>\n",
       "      <td>3718.3</td>\n",
       "      <td>163.7</td>\n",
       "      <td>317.3</td>\n",
       "      <td>666.7</td>\n",
       "      <td>0.9578</td>\n",
       "      <td>0.9214</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L2 C=0.8 fit_intercept=False</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9010</td>\n",
       "      <td>0.3814</td>\n",
       "      <td>3717.2</td>\n",
       "      <td>164.8</td>\n",
       "      <td>316.7</td>\n",
       "      <td>667.3</td>\n",
       "      <td>0.9575</td>\n",
       "      <td>0.9215</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L2 C=0.6 fit_intercept=False</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9010</td>\n",
       "      <td>0.3772</td>\n",
       "      <td>3723.4</td>\n",
       "      <td>158.6</td>\n",
       "      <td>323.1</td>\n",
       "      <td>660.9</td>\n",
       "      <td>0.9591</td>\n",
       "      <td>0.9202</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L2 C=0.2 fit_intercept=True</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9008</td>\n",
       "      <td>0.4018</td>\n",
       "      <td>3717.2</td>\n",
       "      <td>164.8</td>\n",
       "      <td>318</td>\n",
       "      <td>666</td>\n",
       "      <td>0.9575</td>\n",
       "      <td>0.9212</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L2 C=0.2 fit_intercept=False</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9005</td>\n",
       "      <td>0.4184</td>\n",
       "      <td>3714.9</td>\n",
       "      <td>167.1</td>\n",
       "      <td>317.2</td>\n",
       "      <td>666.8</td>\n",
       "      <td>0.9570</td>\n",
       "      <td>0.9213</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Type Model Name                    Paramaters   CV  Accuracy  \\\n",
       "406  classification   Logistic   L1 C=1.0 fit_intercept=True  avg    0.9601   \n",
       "428  classification   Logistic  L1 C=1.0 fit_intercept=False  avg    0.9599   \n",
       "384  classification   Logistic  L1 C=0.9 fit_intercept=False  avg    0.9598   \n",
       "362  classification   Logistic   L1 C=0.9 fit_intercept=True  avg    0.9595   \n",
       "318  classification   Logistic   L1 C=0.8 fit_intercept=True  avg    0.9594   \n",
       "340  classification   Logistic  L1 C=0.8 fit_intercept=False  avg    0.9593   \n",
       "274  classification   Logistic   L1 C=0.7 fit_intercept=True  avg    0.9589   \n",
       "296  classification   Logistic  L1 C=0.7 fit_intercept=False  avg    0.9589   \n",
       "230  classification   Logistic   L1 C=0.6 fit_intercept=True  avg    0.9581   \n",
       "252  classification   Logistic  L1 C=0.6 fit_intercept=False  avg    0.9581   \n",
       "208  classification   Logistic  L1 C=0.5 fit_intercept=False  avg    0.9577   \n",
       "186  classification   Logistic   L1 C=0.5 fit_intercept=True  avg    0.9575   \n",
       "142  classification   Logistic   L1 C=0.4 fit_intercept=True  avg    0.9570   \n",
       "164  classification   Logistic  L1 C=0.4 fit_intercept=False  avg    0.9570   \n",
       "98   classification   Logistic   L1 C=0.3 fit_intercept=True  avg    0.9559   \n",
       "120  classification   Logistic  L1 C=0.3 fit_intercept=False  avg    0.9556   \n",
       "76   classification   Logistic  L1 C=0.2 fit_intercept=False  avg    0.9522   \n",
       "54   classification   Logistic   L1 C=0.2 fit_intercept=True  avg    0.9520   \n",
       "10   classification   Logistic   L1 C=0.1 fit_intercept=True  avg    0.9369   \n",
       "32   classification   Logistic  L1 C=0.1 fit_intercept=False  avg    0.9368   \n",
       "109  classification   Logistic   L2 C=0.3 fit_intercept=True  avg    0.9021   \n",
       "241  classification   Logistic   L2 C=0.6 fit_intercept=True  avg    0.9020   \n",
       "175  classification   Logistic  L2 C=0.4 fit_intercept=False  avg    0.9020   \n",
       "417  classification   Logistic   L2 C=1.0 fit_intercept=True  avg    0.9020   \n",
       "307  classification   Logistic  L2 C=0.7 fit_intercept=False  avg    0.9019   \n",
       "395  classification   Logistic  L2 C=0.9 fit_intercept=False  avg    0.9018   \n",
       "219  classification   Logistic  L2 C=0.5 fit_intercept=False  avg    0.9016   \n",
       "197  classification   Logistic   L2 C=0.5 fit_intercept=True  avg    0.9016   \n",
       "285  classification   Logistic   L2 C=0.7 fit_intercept=True  avg    0.9016   \n",
       "21   classification   Logistic   L2 C=0.1 fit_intercept=True  avg    0.9016   \n",
       "373  classification   Logistic   L2 C=0.9 fit_intercept=True  avg    0.9016   \n",
       "153  classification   Logistic   L2 C=0.4 fit_intercept=True  avg    0.9014   \n",
       "131  classification   Logistic  L2 C=0.3 fit_intercept=False  avg    0.9014   \n",
       "43   classification   Logistic  L2 C=0.1 fit_intercept=False  avg    0.9013   \n",
       "329  classification   Logistic   L2 C=0.8 fit_intercept=True  avg    0.9013   \n",
       "439  classification   Logistic  L2 C=1.0 fit_intercept=False  avg    0.9012   \n",
       "351  classification   Logistic  L2 C=0.8 fit_intercept=False  avg    0.9010   \n",
       "263  classification   Logistic  L2 C=0.6 fit_intercept=False  avg    0.9010   \n",
       "65   classification   Logistic   L2 C=0.2 fit_intercept=True  avg    0.9008   \n",
       "87   classification   Logistic  L2 C=0.2 fit_intercept=False  avg    0.9005   \n",
       "\n",
       "     Execution Time True Positive False Positive False Negative True Negative  \\\n",
       "406          1.5609          3797             85          109.2         874.8   \n",
       "428          1.8262        3796.8           85.2            110           874   \n",
       "384          1.8347        3796.5           85.5          110.1         873.9   \n",
       "362          1.4958        3796.3           85.7          111.3         872.7   \n",
       "318          1.8464        3795.8           86.2          111.2         872.8   \n",
       "340          1.5242        3795.6           86.4          111.8         872.2   \n",
       "274          1.5738        3794.7           87.3          112.7         871.3   \n",
       "296          1.4565        3794.6           87.4          112.8         871.2   \n",
       "230          1.7293        3792.2           89.8          114.3         869.7   \n",
       "252          1.3329        3792.9           89.1          114.7         869.3   \n",
       "208          1.8154        3791.1           90.9          115.1         868.9   \n",
       "186          1.3475        3790.9           91.1          115.9         868.1   \n",
       "142          1.3215        3790.7           91.3          118.1         865.9   \n",
       "164          1.2907        3790.9           91.1          118.2         865.8   \n",
       "98           1.1836        3789.4           92.6          121.9         862.1   \n",
       "120          1.2029        3788.8           93.2          122.7         861.3   \n",
       "76           1.1125        3783.6           98.4          134.3         849.7   \n",
       "54           1.0238        3783.2           98.8          134.7         849.3   \n",
       "10           0.7430          3757            125          182.1         801.9   \n",
       "32           0.6797        3756.8          125.2          182.3         801.7   \n",
       "109          0.4241        3719.4          162.6          313.9         670.1   \n",
       "241          0.4001        3718.8          163.2          313.9         670.1   \n",
       "175          0.3837        3716.5          165.5          311.6         672.4   \n",
       "417          0.4076        3720.1          161.9          315.1         668.9   \n",
       "307          0.4380        3716.6          165.4          311.8         672.2   \n",
       "395          0.4114        3718.2          163.8          314.1         669.9   \n",
       "219          0.3959        3720.7          161.3          317.5         666.5   \n",
       "197          0.4391        3716.6          165.4          313.2         670.8   \n",
       "285          0.3941        3720.7          161.3          317.5         666.5   \n",
       "21           0.4083        3719.7          162.3          316.6         667.4   \n",
       "373          0.3556        3722.3          159.7          318.9         665.1   \n",
       "153          0.3889        3719.3          162.7          316.9         667.1   \n",
       "131          0.4095          3719            163          316.7         667.3   \n",
       "43           0.4228        3716.3          165.7          314.7         669.3   \n",
       "329          0.4445        3715.5          166.5          313.9         670.1   \n",
       "439          0.4131        3718.3          163.7          317.3         666.7   \n",
       "351          0.3814        3717.2          164.8          316.7         667.3   \n",
       "263          0.3772        3723.4          158.6          323.1         660.9   \n",
       "65           0.4018        3717.2          164.8            318           666   \n",
       "87           0.4184        3714.9          167.1          317.2         666.8   \n",
       "\n",
       "     Precision  Recall Model  \n",
       "406     0.9781  0.9721  None  \n",
       "428     0.9781  0.9719  None  \n",
       "384     0.9780  0.9718  None  \n",
       "362     0.9779  0.9715  None  \n",
       "318     0.9778  0.9716  None  \n",
       "340     0.9777  0.9714  None  \n",
       "274     0.9775  0.9712  None  \n",
       "296     0.9775  0.9711  None  \n",
       "230     0.9769  0.9708  None  \n",
       "252     0.9770  0.9707  None  \n",
       "208     0.9766  0.9705  None  \n",
       "186     0.9765  0.9703  None  \n",
       "142     0.9765  0.9698  None  \n",
       "164     0.9765  0.9698  None  \n",
       "98      0.9761  0.9688  None  \n",
       "120     0.9760  0.9686  None  \n",
       "76      0.9747  0.9657  None  \n",
       "54      0.9745  0.9656  None  \n",
       "10      0.9678  0.9538  None  \n",
       "32      0.9677  0.9537  None  \n",
       "109     0.9581  0.9222  None  \n",
       "241     0.9580  0.9222  None  \n",
       "175     0.9574  0.9227  None  \n",
       "417     0.9583  0.9219  None  \n",
       "307     0.9574  0.9226  None  \n",
       "395     0.9578  0.9221  None  \n",
       "219     0.9584  0.9214  None  \n",
       "197     0.9574  0.9223  None  \n",
       "285     0.9584  0.9214  None  \n",
       "21      0.9582  0.9216  None  \n",
       "373     0.9589  0.9211  None  \n",
       "153     0.9581  0.9215  None  \n",
       "131     0.9580  0.9215  None  \n",
       "43      0.9573  0.9219  None  \n",
       "329     0.9571  0.9221  None  \n",
       "439     0.9578  0.9214  None  \n",
       "351     0.9575  0.9215  None  \n",
       "263     0.9591  0.9202  None  \n",
       "65      0.9575  0.9212  None  \n",
       "87      0.9570  0.9213  None  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_stats.loc[(class_stats['Model Name'] == 'Logistic') & (class_stats['CV'] == 'avg')].sort_values(by=['Accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  For our logistic regression model we focused on a few parameters: C level (.2-1.0), L1 vs L2 penalty, and whether a bias constant should be added (fit_intercept).\n",
    "\n",
    "###### Like all models, we set a random state so the results are easy to replicate.  As for performance, L1 consistily outperformed L2 and while a bias term generally had a postive effect on accuracy, it was always a minimal bump in accuracy.  Finally, the accuracy had the best performance when the C level moves away from 0 but acts a bit sporadic and is in need of futher visualization to confirm behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Neighbors=1 algorithm=ball_tree\n",
      "KNN Neighbors=1 algorithm=kd_tree\n",
      "KNN Neighbors=1 algorithm=brute\n",
      "KNN Neighbors=2 algorithm=ball_tree\n",
      "KNN Neighbors=2 algorithm=kd_tree\n",
      "KNN Neighbors=2 algorithm=brute\n",
      "KNN Neighbors=3 algorithm=ball_tree\n",
      "KNN Neighbors=3 algorithm=kd_tree\n",
      "KNN Neighbors=3 algorithm=brute\n",
      "KNN Neighbors=4 algorithm=ball_tree\n",
      "KNN Neighbors=4 algorithm=kd_tree\n",
      "KNN Neighbors=4 algorithm=brute\n",
      "KNN Neighbors=5 algorithm=ball_tree\n",
      "KNN Neighbors=5 algorithm=kd_tree\n",
      "KNN Neighbors=5 algorithm=brute\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model_name = 'KNN'\n",
    "for K in range(1,6):\n",
    "    model_para = 'Neighbors=' + str(K) + ' algorithm=ball_tree'\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=K, weights='uniform', metric='euclidean', algorithm= 'ball_tree', n_jobs=-1)\n",
    "    run_model_class(shuffle_split_clas, model_name, model_para, knn_clf)\n",
    "    model_para = 'Neighbors=' + str(K) + ' algorithm=kd_tree'\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=K, weights='uniform', metric='euclidean', algorithm='kd_tree', n_jobs=-1)\n",
    "    run_model_class(shuffle_split_clas, model_name, model_para, knn_clf)\n",
    "    model_para = 'Neighbors=' + str(K) + ' algorithm=brute'\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=K, weights='uniform', metric='euclidean', algorithm='brute', n_jobs=-1)\n",
    "    run_model_class(shuffle_split_clas, model_name, model_para, knn_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Below are the preformance results for our classification model trained above sorted by our Accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Paramaters</th>\n",
       "      <th>CV</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Execution Time</th>\n",
       "      <th>True Positive</th>\n",
       "      <th>False Positive</th>\n",
       "      <th>False Negative</th>\n",
       "      <th>True Negative</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=1 algorithm=ball_tree</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.1456</td>\n",
       "      <td>3881.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>982.4</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=1 algorithm=kd_tree</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>3881.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>982.4</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=1 algorithm=brute</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>1.2569</td>\n",
       "      <td>3881.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>982.4</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=2 algorithm=ball_tree</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>0.1433</td>\n",
       "      <td>3881.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>978.5</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=2 algorithm=kd_tree</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>0.1469</td>\n",
       "      <td>3881.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>978.5</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=2 algorithm=brute</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>0.9589</td>\n",
       "      <td>3881.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>978.5</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=3 algorithm=ball_tree</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>0.1433</td>\n",
       "      <td>3876.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.5</td>\n",
       "      <td>978.5</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=3 algorithm=kd_tree</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>0.1467</td>\n",
       "      <td>3876.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.5</td>\n",
       "      <td>978.5</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=3 algorithm=brute</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>0.9738</td>\n",
       "      <td>3876.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.5</td>\n",
       "      <td>978.5</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=4 algorithm=ball_tree</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9953</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>3878.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>19.6</td>\n",
       "      <td>964.4</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=4 algorithm=kd_tree</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9953</td>\n",
       "      <td>0.1469</td>\n",
       "      <td>3878.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>19.6</td>\n",
       "      <td>964.4</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=4 algorithm=brute</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9953</td>\n",
       "      <td>1.1050</td>\n",
       "      <td>3878.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>19.6</td>\n",
       "      <td>964.4</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=5 algorithm=ball_tree</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9941</td>\n",
       "      <td>0.1439</td>\n",
       "      <td>3872.1</td>\n",
       "      <td>9.9</td>\n",
       "      <td>18.6</td>\n",
       "      <td>965.4</td>\n",
       "      <td>0.9974</td>\n",
       "      <td>0.9952</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=5 algorithm=kd_tree</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9941</td>\n",
       "      <td>0.1472</td>\n",
       "      <td>3872.1</td>\n",
       "      <td>9.9</td>\n",
       "      <td>18.6</td>\n",
       "      <td>965.4</td>\n",
       "      <td>0.9974</td>\n",
       "      <td>0.9952</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=5 algorithm=brute</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9941</td>\n",
       "      <td>1.1068</td>\n",
       "      <td>3872.1</td>\n",
       "      <td>9.9</td>\n",
       "      <td>18.6</td>\n",
       "      <td>965.4</td>\n",
       "      <td>0.9974</td>\n",
       "      <td>0.9952</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Type Model Name                       Paramaters   CV  \\\n",
       "450  classification        KNN  Neighbors=1 algorithm=ball_tree  avg   \n",
       "461  classification        KNN    Neighbors=1 algorithm=kd_tree  avg   \n",
       "472  classification        KNN      Neighbors=1 algorithm=brute  avg   \n",
       "483  classification        KNN  Neighbors=2 algorithm=ball_tree  avg   \n",
       "494  classification        KNN    Neighbors=2 algorithm=kd_tree  avg   \n",
       "505  classification        KNN      Neighbors=2 algorithm=brute  avg   \n",
       "516  classification        KNN  Neighbors=3 algorithm=ball_tree  avg   \n",
       "527  classification        KNN    Neighbors=3 algorithm=kd_tree  avg   \n",
       "538  classification        KNN      Neighbors=3 algorithm=brute  avg   \n",
       "549  classification        KNN  Neighbors=4 algorithm=ball_tree  avg   \n",
       "560  classification        KNN    Neighbors=4 algorithm=kd_tree  avg   \n",
       "571  classification        KNN      Neighbors=4 algorithm=brute  avg   \n",
       "582  classification        KNN  Neighbors=5 algorithm=ball_tree  avg   \n",
       "593  classification        KNN    Neighbors=5 algorithm=kd_tree  avg   \n",
       "604  classification        KNN      Neighbors=5 algorithm=brute  avg   \n",
       "\n",
       "     Accuracy  Execution Time True Positive False Positive False Negative  \\\n",
       "450    0.9995          0.1456        3881.1            0.9            1.6   \n",
       "461    0.9995          0.1465        3881.1            0.9            1.6   \n",
       "472    0.9995          1.2569        3881.1            0.9            1.6   \n",
       "483    0.9987          0.1433        3881.4            0.6            5.5   \n",
       "494    0.9987          0.1469        3881.4            0.6            5.5   \n",
       "505    0.9987          0.9589        3881.4            0.6            5.5   \n",
       "516    0.9978          0.1433        3876.7            5.3            5.5   \n",
       "527    0.9978          0.1467        3876.7            5.3            5.5   \n",
       "538    0.9978          0.9738        3876.7            5.3            5.5   \n",
       "549    0.9953          0.1442        3878.8            3.2           19.6   \n",
       "560    0.9953          0.1469        3878.8            3.2           19.6   \n",
       "571    0.9953          1.1050        3878.8            3.2           19.6   \n",
       "582    0.9941          0.1439        3872.1            9.9           18.6   \n",
       "593    0.9941          0.1472        3872.1            9.9           18.6   \n",
       "604    0.9941          1.1068        3872.1            9.9           18.6   \n",
       "\n",
       "    True Negative  Precision  Recall Model  \n",
       "450         982.4     0.9998  0.9996  None  \n",
       "461         982.4     0.9998  0.9996  None  \n",
       "472         982.4     0.9998  0.9996  None  \n",
       "483         978.5     0.9998  0.9986  None  \n",
       "494         978.5     0.9998  0.9986  None  \n",
       "505         978.5     0.9998  0.9986  None  \n",
       "516         978.5     0.9986  0.9986  None  \n",
       "527         978.5     0.9986  0.9986  None  \n",
       "538         978.5     0.9986  0.9986  None  \n",
       "549         964.4     0.9992  0.9950  None  \n",
       "560         964.4     0.9992  0.9950  None  \n",
       "571         964.4     0.9992  0.9950  None  \n",
       "582         965.4     0.9974  0.9952  None  \n",
       "593         965.4     0.9974  0.9952  None  \n",
       "604         965.4     0.9974  0.9952  None  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_stats.loc[(class_stats['Model Name'] == 'KNN') & (class_stats['CV'] == 'avg')].sort_values(by=['Accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### KNN performed very well with the data set getting >99% accuracy.  This score seemed to be best with fewer neighbors and appeared to be consistent regardless of the algorithm used to compute the nearest neighbor.  The only main difference observed is that the \"brute\" algorithm took about 10 times longer to run than the \"ball tree\" or \"kd tree\" algorithms.  However, this algorithm is still faster than many of the algorithms we will test.  Overall, the KNN models have some of our fastest execution times we will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forrest estimators=20 max_depht=2 max_features=auto\n",
      "Random Forrest estimators=20 max_depht=2 max_features=sqrt\n",
      "Random Forrest estimators=20 max_depht=2 max_features=log2\n",
      "Random Forrest estimators=20 max_depht=2 max_features=None\n",
      "Random Forrest estimators=20 max_depht=4 max_features=auto\n",
      "Random Forrest estimators=20 max_depht=4 max_features=sqrt\n",
      "Random Forrest estimators=20 max_depht=4 max_features=log2\n",
      "Random Forrest estimators=20 max_depht=4 max_features=None\n",
      "Random Forrest estimators=20 max_depht=6 max_features=auto\n",
      "Random Forrest estimators=20 max_depht=6 max_features=sqrt\n",
      "Random Forrest estimators=20 max_depht=6 max_features=log2\n",
      "Random Forrest estimators=20 max_depht=6 max_features=None\n",
      "Random Forrest estimators=20 max_depht=8 max_features=auto\n",
      "Random Forrest estimators=20 max_depht=8 max_features=sqrt\n",
      "Random Forrest estimators=20 max_depht=8 max_features=log2\n",
      "Random Forrest estimators=20 max_depht=8 max_features=None\n",
      "Random Forrest estimators=20 max_depht=10 max_features=auto\n",
      "Random Forrest estimators=20 max_depht=10 max_features=sqrt\n",
      "Random Forrest estimators=20 max_depht=10 max_features=log2\n",
      "Random Forrest estimators=20 max_depht=10 max_features=None\n",
      "Random Forrest estimators=40 max_depht=2 max_features=auto\n",
      "Random Forrest estimators=40 max_depht=2 max_features=sqrt\n",
      "Random Forrest estimators=40 max_depht=2 max_features=log2\n",
      "Random Forrest estimators=40 max_depht=2 max_features=None\n",
      "Random Forrest estimators=40 max_depht=4 max_features=auto\n",
      "Random Forrest estimators=40 max_depht=4 max_features=sqrt\n",
      "Random Forrest estimators=40 max_depht=4 max_features=log2\n",
      "Random Forrest estimators=40 max_depht=4 max_features=None\n",
      "Random Forrest estimators=40 max_depht=6 max_features=auto\n",
      "Random Forrest estimators=40 max_depht=6 max_features=sqrt\n",
      "Random Forrest estimators=40 max_depht=6 max_features=log2\n",
      "Random Forrest estimators=40 max_depht=6 max_features=None\n",
      "Random Forrest estimators=40 max_depht=8 max_features=auto\n",
      "Random Forrest estimators=40 max_depht=8 max_features=sqrt\n",
      "Random Forrest estimators=40 max_depht=8 max_features=log2\n",
      "Random Forrest estimators=40 max_depht=8 max_features=None\n",
      "Random Forrest estimators=40 max_depht=10 max_features=auto\n",
      "Random Forrest estimators=40 max_depht=10 max_features=sqrt\n",
      "Random Forrest estimators=40 max_depht=10 max_features=log2\n",
      "Random Forrest estimators=40 max_depht=10 max_features=None\n",
      "Random Forrest estimators=60 max_depht=2 max_features=auto\n",
      "Random Forrest estimators=60 max_depht=2 max_features=sqrt\n",
      "Random Forrest estimators=60 max_depht=2 max_features=log2\n",
      "Random Forrest estimators=60 max_depht=2 max_features=None\n",
      "Random Forrest estimators=60 max_depht=4 max_features=auto\n",
      "Random Forrest estimators=60 max_depht=4 max_features=sqrt\n",
      "Random Forrest estimators=60 max_depht=4 max_features=log2\n",
      "Random Forrest estimators=60 max_depht=4 max_features=None\n",
      "Random Forrest estimators=60 max_depht=6 max_features=auto\n",
      "Random Forrest estimators=60 max_depht=6 max_features=sqrt\n",
      "Random Forrest estimators=60 max_depht=6 max_features=log2\n",
      "Random Forrest estimators=60 max_depht=6 max_features=None\n",
      "Random Forrest estimators=60 max_depht=8 max_features=auto\n",
      "Random Forrest estimators=60 max_depht=8 max_features=sqrt\n",
      "Random Forrest estimators=60 max_depht=8 max_features=log2\n",
      "Random Forrest estimators=60 max_depht=8 max_features=None\n",
      "Random Forrest estimators=60 max_depht=10 max_features=auto\n",
      "Random Forrest estimators=60 max_depht=10 max_features=sqrt\n",
      "Random Forrest estimators=60 max_depht=10 max_features=log2\n",
      "Random Forrest estimators=60 max_depht=10 max_features=None\n",
      "Random Forrest estimators=80 max_depht=2 max_features=auto\n",
      "Random Forrest estimators=80 max_depht=2 max_features=sqrt\n",
      "Random Forrest estimators=80 max_depht=2 max_features=log2\n",
      "Random Forrest estimators=80 max_depht=2 max_features=None\n",
      "Random Forrest estimators=80 max_depht=4 max_features=auto\n",
      "Random Forrest estimators=80 max_depht=4 max_features=sqrt\n",
      "Random Forrest estimators=80 max_depht=4 max_features=log2\n",
      "Random Forrest estimators=80 max_depht=4 max_features=None\n",
      "Random Forrest estimators=80 max_depht=6 max_features=auto\n",
      "Random Forrest estimators=80 max_depht=6 max_features=sqrt\n",
      "Random Forrest estimators=80 max_depht=6 max_features=log2\n",
      "Random Forrest estimators=80 max_depht=6 max_features=None\n",
      "Random Forrest estimators=80 max_depht=8 max_features=auto\n",
      "Random Forrest estimators=80 max_depht=8 max_features=sqrt\n",
      "Random Forrest estimators=80 max_depht=8 max_features=log2\n",
      "Random Forrest estimators=80 max_depht=8 max_features=None\n",
      "Random Forrest estimators=80 max_depht=10 max_features=auto\n",
      "Random Forrest estimators=80 max_depht=10 max_features=sqrt\n",
      "Random Forrest estimators=80 max_depht=10 max_features=log2\n",
      "Random Forrest estimators=80 max_depht=10 max_features=None\n",
      "Random Forrest estimators=100 max_depht=2 max_features=auto\n",
      "Random Forrest estimators=100 max_depht=2 max_features=sqrt\n",
      "Random Forrest estimators=100 max_depht=2 max_features=log2\n",
      "Random Forrest estimators=100 max_depht=2 max_features=None\n",
      "Random Forrest estimators=100 max_depht=4 max_features=auto\n",
      "Random Forrest estimators=100 max_depht=4 max_features=sqrt\n",
      "Random Forrest estimators=100 max_depht=4 max_features=log2\n",
      "Random Forrest estimators=100 max_depht=4 max_features=None\n",
      "Random Forrest estimators=100 max_depht=6 max_features=auto\n",
      "Random Forrest estimators=100 max_depht=6 max_features=sqrt\n",
      "Random Forrest estimators=100 max_depht=6 max_features=log2\n",
      "Random Forrest estimators=100 max_depht=6 max_features=None\n",
      "Random Forrest estimators=100 max_depht=8 max_features=auto\n",
      "Random Forrest estimators=100 max_depht=8 max_features=sqrt\n",
      "Random Forrest estimators=100 max_depht=8 max_features=log2\n",
      "Random Forrest estimators=100 max_depht=8 max_features=None\n",
      "Random Forrest estimators=100 max_depht=10 max_features=auto\n",
      "Random Forrest estimators=100 max_depht=10 max_features=sqrt\n",
      "Random Forrest estimators=100 max_depht=10 max_features=log2\n",
      "Random Forrest estimators=100 max_depht=10 max_features=None\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_name = \"Random Forest\"\n",
    "for N in range(20, 120, 20):\n",
    "    for D in range(2, 12, 2):\n",
    "        model_para = \"estimators=\" + str(N) + \" max_depht=\" + str(D) + \" max_features=auto\"\n",
    "        rf_clf = RandomForestClassifier(max_features='auto' , n_estimators=N, max_depth=D, random_state=101, n_jobs=-1)\n",
    "        run_model_class(shuffle_split_clas, model_name, model_para, rf_clf)\n",
    "        model_para = \"estimators=\" + str(N) + \" max_depht=\" + str(D) + \" max_features=sqrt\"\n",
    "        rf_clf = RandomForestClassifier(max_features='sqrt' , n_estimators=N, max_depth=D, random_state=101, n_jobs=-1)\n",
    "        run_model_class(shuffle_split_clas, model_name, model_para, rf_clf)\n",
    "        model_para = \"estimators=\" + str(N) + \" max_depht=\" + str(D) + \" max_features=log2\"\n",
    "        rf_clf = RandomForestClassifier(max_features='log2' , n_estimators=N, max_depth=D, random_state=101, n_jobs=-1)\n",
    "        run_model_class(shuffle_split_clas, model_name, model_para, rf_clf)\n",
    "        model_para = \"estimators=\" + str(N) + \" max_depht=\" + str(D) + \" max_features=None\"\n",
    "        rf_clf = RandomForestClassifier(max_features=None , n_estimators=N, max_depth=D, random_state=101, n_jobs=-1)\n",
    "        run_model_class(shuffle_split_clas, model_name, model_para, rf_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Below are the preformance results for our classification model trained above sorted by our Accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Paramaters</th>\n",
       "      <th>CV</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Execution Time</th>\n",
       "      <th>True Positive</th>\n",
       "      <th>False Positive</th>\n",
       "      <th>False Negative</th>\n",
       "      <th>True Negative</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=100 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9966</td>\n",
       "      <td>1.3083</td>\n",
       "      <td>3877.8</td>\n",
       "      <td>4.2</td>\n",
       "      <td>12.3</td>\n",
       "      <td>971.7</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=80 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9966</td>\n",
       "      <td>1.0811</td>\n",
       "      <td>3878.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>12.7</td>\n",
       "      <td>971.3</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=60 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.8580</td>\n",
       "      <td>3877.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>12.4</td>\n",
       "      <td>971.6</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=40 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.6629</td>\n",
       "      <td>3877.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>12.3</td>\n",
       "      <td>971.7</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=80 max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>1.0203</td>\n",
       "      <td>3875.4</td>\n",
       "      <td>6.6</td>\n",
       "      <td>14.2</td>\n",
       "      <td>969.8</td>\n",
       "      <td>0.9983</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=20 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>0.4442</td>\n",
       "      <td>3875.1</td>\n",
       "      <td>6.9</td>\n",
       "      <td>13.8</td>\n",
       "      <td>970.2</td>\n",
       "      <td>0.9982</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=100 max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>1.2456</td>\n",
       "      <td>3874.9</td>\n",
       "      <td>7.1</td>\n",
       "      <td>14.5</td>\n",
       "      <td>969.5</td>\n",
       "      <td>0.9982</td>\n",
       "      <td>0.9963</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=60 max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.8213</td>\n",
       "      <td>3874.6</td>\n",
       "      <td>7.4</td>\n",
       "      <td>14.6</td>\n",
       "      <td>969.4</td>\n",
       "      <td>0.9981</td>\n",
       "      <td>0.9963</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=40 max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9954</td>\n",
       "      <td>0.6609</td>\n",
       "      <td>3874.3</td>\n",
       "      <td>7.7</td>\n",
       "      <td>14.5</td>\n",
       "      <td>969.5</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>0.9963</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=20 max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9949</td>\n",
       "      <td>0.4351</td>\n",
       "      <td>3873.2</td>\n",
       "      <td>8.8</td>\n",
       "      <td>15.9</td>\n",
       "      <td>968.1</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.9959</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=80 max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9920</td>\n",
       "      <td>0.9726</td>\n",
       "      <td>3872</td>\n",
       "      <td>10</td>\n",
       "      <td>29.1</td>\n",
       "      <td>954.9</td>\n",
       "      <td>0.9974</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=60 max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9920</td>\n",
       "      <td>0.7819</td>\n",
       "      <td>3871.9</td>\n",
       "      <td>10.1</td>\n",
       "      <td>28.6</td>\n",
       "      <td>955.4</td>\n",
       "      <td>0.9974</td>\n",
       "      <td>0.9927</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=40 max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9919</td>\n",
       "      <td>0.6094</td>\n",
       "      <td>3871.7</td>\n",
       "      <td>10.3</td>\n",
       "      <td>29.2</td>\n",
       "      <td>954.8</td>\n",
       "      <td>0.9973</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=100 max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9918</td>\n",
       "      <td>1.1608</td>\n",
       "      <td>3872</td>\n",
       "      <td>10</td>\n",
       "      <td>29.9</td>\n",
       "      <td>954.1</td>\n",
       "      <td>0.9974</td>\n",
       "      <td>0.9923</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=20 max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9911</td>\n",
       "      <td>0.4341</td>\n",
       "      <td>3870.8</td>\n",
       "      <td>11.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>951.8</td>\n",
       "      <td>0.9971</td>\n",
       "      <td>0.9918</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=100 max_depht=10 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9876</td>\n",
       "      <td>0.3639</td>\n",
       "      <td>3879.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>57.8</td>\n",
       "      <td>926.2</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9853</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=100 max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9876</td>\n",
       "      <td>0.3646</td>\n",
       "      <td>3879.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>57.8</td>\n",
       "      <td>926.2</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9853</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=80 max_depht=10 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>0.3551</td>\n",
       "      <td>3879.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>59.1</td>\n",
       "      <td>924.9</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=80 max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>0.3551</td>\n",
       "      <td>3879.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>59.1</td>\n",
       "      <td>924.9</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=60 max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.2574</td>\n",
       "      <td>3879.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>60</td>\n",
       "      <td>924</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=60 max_depht=10 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.2465</td>\n",
       "      <td>3879.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>60</td>\n",
       "      <td>924</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=40 max_depht=10 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9868</td>\n",
       "      <td>0.2387</td>\n",
       "      <td>3878.2</td>\n",
       "      <td>3.8</td>\n",
       "      <td>60.2</td>\n",
       "      <td>923.8</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>0.9847</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=40 max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9868</td>\n",
       "      <td>0.2384</td>\n",
       "      <td>3878.2</td>\n",
       "      <td>3.8</td>\n",
       "      <td>60.2</td>\n",
       "      <td>923.8</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>0.9847</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=20 max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9858</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>3875.9</td>\n",
       "      <td>6.1</td>\n",
       "      <td>62.8</td>\n",
       "      <td>921.2</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=20 max_depht=10 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9858</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>3875.9</td>\n",
       "      <td>6.1</td>\n",
       "      <td>62.8</td>\n",
       "      <td>921.2</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1627</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=100 max_depht=8 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9828</td>\n",
       "      <td>0.3631</td>\n",
       "      <td>3880.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>82</td>\n",
       "      <td>902</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.9793</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=100 max_depht=8 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9828</td>\n",
       "      <td>0.3635</td>\n",
       "      <td>3880.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>82</td>\n",
       "      <td>902</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.9793</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=40 max_depht=8 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9827</td>\n",
       "      <td>0.2381</td>\n",
       "      <td>3879.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>82.3</td>\n",
       "      <td>901.7</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=40 max_depht=8 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9827</td>\n",
       "      <td>0.2386</td>\n",
       "      <td>3879.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>82.3</td>\n",
       "      <td>901.7</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=80 max_depht=8 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9822</td>\n",
       "      <td>0.2649</td>\n",
       "      <td>3880.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>84.7</td>\n",
       "      <td>899.3</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.9786</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=80 max_depht=6 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9197</td>\n",
       "      <td>0.2546</td>\n",
       "      <td>3881.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>390.2</td>\n",
       "      <td>593.8</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9087</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=100 max_depht=4 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9196</td>\n",
       "      <td>0.2624</td>\n",
       "      <td>3881.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>390.9</td>\n",
       "      <td>593.1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9085</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=100 max_depht=4 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9196</td>\n",
       "      <td>0.2629</td>\n",
       "      <td>3881.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>390.9</td>\n",
       "      <td>593.1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9085</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=100 max_depht=6 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9187</td>\n",
       "      <td>0.2626</td>\n",
       "      <td>3881.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>395.3</td>\n",
       "      <td>588.7</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9076</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=60 max_depht=4 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9186</td>\n",
       "      <td>0.2464</td>\n",
       "      <td>3881.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>395.9</td>\n",
       "      <td>588.1</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9075</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=60 max_depht=4 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9186</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>3881.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>395.9</td>\n",
       "      <td>588.1</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9075</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=40 max_depht=6 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9185</td>\n",
       "      <td>0.2378</td>\n",
       "      <td>3881.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>396.2</td>\n",
       "      <td>587.8</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9075</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=60 max_depht=6 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9183</td>\n",
       "      <td>0.2464</td>\n",
       "      <td>3881.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>397.1</td>\n",
       "      <td>586.9</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9073</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=80 max_depht=4 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9176</td>\n",
       "      <td>0.2542</td>\n",
       "      <td>3881.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>400.8</td>\n",
       "      <td>583.2</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9064</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=80 max_depht=4 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9176</td>\n",
       "      <td>0.2548</td>\n",
       "      <td>3881.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>400.8</td>\n",
       "      <td>583.2</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9064</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=20 max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8932</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>3881.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>519.3</td>\n",
       "      <td>464.7</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.8820</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=40 max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8840</td>\n",
       "      <td>0.2384</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>564.6</td>\n",
       "      <td>419.4</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8730</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=60 max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8827</td>\n",
       "      <td>0.2462</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>570.9</td>\n",
       "      <td>413.1</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8718</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=100 max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8827</td>\n",
       "      <td>0.2622</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>570.7</td>\n",
       "      <td>413.3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8718</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=80 max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8826</td>\n",
       "      <td>0.2542</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>571.2</td>\n",
       "      <td>412.8</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8717</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=20 max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8696</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>634.7</td>\n",
       "      <td>349.3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8595</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=20 max_depht=2 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8696</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>634.7</td>\n",
       "      <td>349.3</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8595</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=40 max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8694</td>\n",
       "      <td>0.2386</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>635.4</td>\n",
       "      <td>348.6</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8593</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=40 max_depht=2 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8694</td>\n",
       "      <td>0.2383</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>635.4</td>\n",
       "      <td>348.6</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8593</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=60 max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8689</td>\n",
       "      <td>0.2465</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>638</td>\n",
       "      <td>346</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8589</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=60 max_depht=2 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8689</td>\n",
       "      <td>0.2469</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>638</td>\n",
       "      <td>346</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8589</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=80 max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8688</td>\n",
       "      <td>0.2541</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>638.3</td>\n",
       "      <td>345.7</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8588</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=80 max_depht=2 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8688</td>\n",
       "      <td>0.2548</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>638.3</td>\n",
       "      <td>345.7</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8588</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=100 max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8686</td>\n",
       "      <td>0.2627</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>639.3</td>\n",
       "      <td>344.7</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8586</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=100 max_depht=2 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8686</td>\n",
       "      <td>0.2628</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>639.3</td>\n",
       "      <td>344.7</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8586</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=20 max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8476</td>\n",
       "      <td>0.2295</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>741.5</td>\n",
       "      <td>242.5</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8396</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=100 max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8386</td>\n",
       "      <td>0.2620</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>785.5</td>\n",
       "      <td>198.5</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8317</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=40 max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8334</td>\n",
       "      <td>0.2383</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>810.6</td>\n",
       "      <td>173.4</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8273</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=60 max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8306</td>\n",
       "      <td>0.2470</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>824.2</td>\n",
       "      <td>159.8</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8249</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=80 max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8297</td>\n",
       "      <td>0.2541</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>828.5</td>\n",
       "      <td>155.5</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8241</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Type      Model Name  \\\n",
       "1704  classification  Random Forrest   \n",
       "1484  classification  Random Forrest   \n",
       "1264  classification  Random Forrest   \n",
       "1044  classification  Random Forrest   \n",
       "1440  classification  Random Forrest   \n",
       "824   classification  Random Forrest   \n",
       "1660  classification  Random Forrest   \n",
       "1220  classification  Random Forrest   \n",
       "1000  classification  Random Forrest   \n",
       "780   classification  Random Forrest   \n",
       "1396  classification  Random Forrest   \n",
       "1176  classification  Random Forrest   \n",
       "956   classification  Random Forrest   \n",
       "1616  classification  Random Forrest   \n",
       "736   classification  Random Forrest   \n",
       "1682  classification  Random Forrest   \n",
       "1671  classification  Random Forrest   \n",
       "1462  classification  Random Forrest   \n",
       "1451  classification  Random Forrest   \n",
       "1231  classification  Random Forrest   \n",
       "1242  classification  Random Forrest   \n",
       "1022  classification  Random Forrest   \n",
       "1011  classification  Random Forrest   \n",
       "791   classification  Random Forrest   \n",
       "802   classification  Random Forrest   \n",
       "1627  classification  Random Forrest   \n",
       "1638  classification  Random Forrest   \n",
       "967   classification  Random Forrest   \n",
       "978   classification  Random Forrest   \n",
       "1418  classification  Random Forrest   \n",
       "...              ...             ...   \n",
       "1385  classification  Random Forrest   \n",
       "1539  classification  Random Forrest   \n",
       "1550  classification  Random Forrest   \n",
       "1605  classification  Random Forrest   \n",
       "1110  classification  Random Forrest   \n",
       "1099  classification  Random Forrest   \n",
       "945   classification  Random Forrest   \n",
       "1165  classification  Random Forrest   \n",
       "1330  classification  Random Forrest   \n",
       "1319  classification  Random Forrest   \n",
       "681   classification  Random Forrest   \n",
       "901   classification  Random Forrest   \n",
       "1121  classification  Random Forrest   \n",
       "1561  classification  Random Forrest   \n",
       "1341  classification  Random Forrest   \n",
       "626   classification  Random Forrest   \n",
       "615   classification  Random Forrest   \n",
       "846   classification  Random Forrest   \n",
       "835   classification  Random Forrest   \n",
       "1066  classification  Random Forrest   \n",
       "1055  classification  Random Forrest   \n",
       "1286  classification  Random Forrest   \n",
       "1275  classification  Random Forrest   \n",
       "1506  classification  Random Forrest   \n",
       "1495  classification  Random Forrest   \n",
       "637   classification  Random Forrest   \n",
       "1517  classification  Random Forrest   \n",
       "857   classification  Random Forrest   \n",
       "1077  classification  Random Forrest   \n",
       "1297  classification  Random Forrest   \n",
       "\n",
       "                                         Paramaters   CV  Accuracy  \\\n",
       "1704  estimators=100 max_depht=10 max_features=None  avg    0.9966   \n",
       "1484   estimators=80 max_depht=10 max_features=None  avg    0.9966   \n",
       "1264   estimators=60 max_depht=10 max_features=None  avg    0.9965   \n",
       "1044   estimators=40 max_depht=10 max_features=None  avg    0.9965   \n",
       "1440    estimators=80 max_depht=8 max_features=None  avg    0.9957   \n",
       "824    estimators=20 max_depht=10 max_features=None  avg    0.9957   \n",
       "1660   estimators=100 max_depht=8 max_features=None  avg    0.9956   \n",
       "1220    estimators=60 max_depht=8 max_features=None  avg    0.9955   \n",
       "1000    estimators=40 max_depht=8 max_features=None  avg    0.9954   \n",
       "780     estimators=20 max_depht=8 max_features=None  avg    0.9949   \n",
       "1396    estimators=80 max_depht=6 max_features=None  avg    0.9920   \n",
       "1176    estimators=60 max_depht=6 max_features=None  avg    0.9920   \n",
       "956     estimators=40 max_depht=6 max_features=None  avg    0.9919   \n",
       "1616   estimators=100 max_depht=6 max_features=None  avg    0.9918   \n",
       "736     estimators=20 max_depht=6 max_features=None  avg    0.9911   \n",
       "1682  estimators=100 max_depht=10 max_features=sqrt  avg    0.9876   \n",
       "1671  estimators=100 max_depht=10 max_features=auto  avg    0.9876   \n",
       "1462   estimators=80 max_depht=10 max_features=sqrt  avg    0.9873   \n",
       "1451   estimators=80 max_depht=10 max_features=auto  avg    0.9873   \n",
       "1231   estimators=60 max_depht=10 max_features=auto  avg    0.9871   \n",
       "1242   estimators=60 max_depht=10 max_features=sqrt  avg    0.9871   \n",
       "1022   estimators=40 max_depht=10 max_features=sqrt  avg    0.9868   \n",
       "1011   estimators=40 max_depht=10 max_features=auto  avg    0.9868   \n",
       "791    estimators=20 max_depht=10 max_features=auto  avg    0.9858   \n",
       "802    estimators=20 max_depht=10 max_features=sqrt  avg    0.9858   \n",
       "1627   estimators=100 max_depht=8 max_features=auto  avg    0.9828   \n",
       "1638   estimators=100 max_depht=8 max_features=sqrt  avg    0.9828   \n",
       "967     estimators=40 max_depht=8 max_features=auto  avg    0.9827   \n",
       "978     estimators=40 max_depht=8 max_features=sqrt  avg    0.9827   \n",
       "1418    estimators=80 max_depht=8 max_features=sqrt  avg    0.9822   \n",
       "...                                             ...  ...       ...   \n",
       "1385    estimators=80 max_depht=6 max_features=log2  avg    0.9197   \n",
       "1539   estimators=100 max_depht=4 max_features=auto  avg    0.9196   \n",
       "1550   estimators=100 max_depht=4 max_features=sqrt  avg    0.9196   \n",
       "1605   estimators=100 max_depht=6 max_features=log2  avg    0.9187   \n",
       "1110    estimators=60 max_depht=4 max_features=sqrt  avg    0.9186   \n",
       "1099    estimators=60 max_depht=4 max_features=auto  avg    0.9186   \n",
       "945     estimators=40 max_depht=6 max_features=log2  avg    0.9185   \n",
       "1165    estimators=60 max_depht=6 max_features=log2  avg    0.9183   \n",
       "1330    estimators=80 max_depht=4 max_features=sqrt  avg    0.9176   \n",
       "1319    estimators=80 max_depht=4 max_features=auto  avg    0.9176   \n",
       "681     estimators=20 max_depht=4 max_features=log2  avg    0.8932   \n",
       "901     estimators=40 max_depht=4 max_features=log2  avg    0.8840   \n",
       "1121    estimators=60 max_depht=4 max_features=log2  avg    0.8827   \n",
       "1561   estimators=100 max_depht=4 max_features=log2  avg    0.8827   \n",
       "1341    estimators=80 max_depht=4 max_features=log2  avg    0.8826   \n",
       "626     estimators=20 max_depht=2 max_features=sqrt  avg    0.8696   \n",
       "615     estimators=20 max_depht=2 max_features=auto  avg    0.8696   \n",
       "846     estimators=40 max_depht=2 max_features=sqrt  avg    0.8694   \n",
       "835     estimators=40 max_depht=2 max_features=auto  avg    0.8694   \n",
       "1066    estimators=60 max_depht=2 max_features=sqrt  avg    0.8689   \n",
       "1055    estimators=60 max_depht=2 max_features=auto  avg    0.8689   \n",
       "1286    estimators=80 max_depht=2 max_features=sqrt  avg    0.8688   \n",
       "1275    estimators=80 max_depht=2 max_features=auto  avg    0.8688   \n",
       "1506   estimators=100 max_depht=2 max_features=sqrt  avg    0.8686   \n",
       "1495   estimators=100 max_depht=2 max_features=auto  avg    0.8686   \n",
       "637     estimators=20 max_depht=2 max_features=log2  avg    0.8476   \n",
       "1517   estimators=100 max_depht=2 max_features=log2  avg    0.8386   \n",
       "857     estimators=40 max_depht=2 max_features=log2  avg    0.8334   \n",
       "1077    estimators=60 max_depht=2 max_features=log2  avg    0.8306   \n",
       "1297    estimators=80 max_depht=2 max_features=log2  avg    0.8297   \n",
       "\n",
       "      Execution Time True Positive False Positive False Negative  \\\n",
       "1704          1.3083        3877.8            4.2           12.3   \n",
       "1484          1.0811        3878.1            3.9           12.7   \n",
       "1264          0.8580        3877.2            4.8           12.4   \n",
       "1044          0.6629        3877.3            4.7           12.3   \n",
       "1440          1.0203        3875.4            6.6           14.2   \n",
       "824           0.4442        3875.1            6.9           13.8   \n",
       "1660          1.2456        3874.9            7.1           14.5   \n",
       "1220          0.8213        3874.6            7.4           14.6   \n",
       "1000          0.6609        3874.3            7.7           14.5   \n",
       "780           0.4351        3873.2            8.8           15.9   \n",
       "1396          0.9726          3872             10           29.1   \n",
       "1176          0.7819        3871.9           10.1           28.6   \n",
       "956           0.6094        3871.7           10.3           29.2   \n",
       "1616          1.1608          3872             10           29.9   \n",
       "736           0.4341        3870.8           11.2           32.2   \n",
       "1682          0.3639        3879.5            2.5           57.8   \n",
       "1671          0.3646        3879.5            2.5           57.8   \n",
       "1462          0.3551        3879.3            2.7           59.1   \n",
       "1451          0.3551        3879.3            2.7           59.1   \n",
       "1231          0.2574        3879.3            2.7             60   \n",
       "1242          0.2465        3879.3            2.7             60   \n",
       "1022          0.2387        3878.2            3.8           60.2   \n",
       "1011          0.2384        3878.2            3.8           60.2   \n",
       "791           0.2300        3875.9            6.1           62.8   \n",
       "802           0.2300        3875.9            6.1           62.8   \n",
       "1627          0.3631        3880.5            1.5             82   \n",
       "1638          0.3635        3880.5            1.5             82   \n",
       "967           0.2381        3879.9            2.1           82.3   \n",
       "978           0.2386        3879.9            2.1           82.3   \n",
       "1418          0.2649        3880.1            1.9           84.7   \n",
       "...              ...           ...            ...            ...   \n",
       "1385          0.2546        3881.7            0.3          390.2   \n",
       "1539          0.2624        3881.9            0.1          390.9   \n",
       "1550          0.2629        3881.9            0.1          390.9   \n",
       "1605          0.2626        3881.6            0.4          395.3   \n",
       "1110          0.2464        3881.7            0.3          395.9   \n",
       "1099          0.2467        3881.7            0.3          395.9   \n",
       "945           0.2378        3881.5            0.5          396.2   \n",
       "1165          0.2464        3881.6            0.4          397.1   \n",
       "1330          0.2542        3881.7            0.3          400.8   \n",
       "1319          0.2548        3881.7            0.3          400.8   \n",
       "681           0.2300        3881.4            0.6          519.3   \n",
       "901           0.2384          3882              0          564.6   \n",
       "1121          0.2462          3882              0          570.9   \n",
       "1561          0.2622          3882              0          570.7   \n",
       "1341          0.2542          3882              0          571.2   \n",
       "626           0.2300          3882              0          634.7   \n",
       "615           0.2296          3882              0          634.7   \n",
       "846           0.2386          3882              0          635.4   \n",
       "835           0.2383          3882              0          635.4   \n",
       "1066          0.2465          3882              0            638   \n",
       "1055          0.2469          3882              0            638   \n",
       "1286          0.2541          3882              0          638.3   \n",
       "1275          0.2548          3882              0          638.3   \n",
       "1506          0.2627          3882              0          639.3   \n",
       "1495          0.2628          3882              0          639.3   \n",
       "637           0.2295          3882              0          741.5   \n",
       "1517          0.2620          3882              0          785.5   \n",
       "857           0.2383          3882              0          810.6   \n",
       "1077          0.2470          3882              0          824.2   \n",
       "1297          0.2541          3882              0          828.5   \n",
       "\n",
       "     True Negative  Precision  Recall Model  \n",
       "1704         971.7     0.9989  0.9968  None  \n",
       "1484         971.3     0.9990  0.9967  None  \n",
       "1264         971.6     0.9988  0.9968  None  \n",
       "1044         971.7     0.9988  0.9968  None  \n",
       "1440         969.8     0.9983  0.9964  None  \n",
       "824          970.2     0.9982  0.9965  None  \n",
       "1660         969.5     0.9982  0.9963  None  \n",
       "1220         969.4     0.9981  0.9963  None  \n",
       "1000         969.5     0.9980  0.9963  None  \n",
       "780          968.1     0.9977  0.9959  None  \n",
       "1396         954.9     0.9974  0.9926  None  \n",
       "1176         955.4     0.9974  0.9927  None  \n",
       "956          954.8     0.9973  0.9925  None  \n",
       "1616         954.1     0.9974  0.9923  None  \n",
       "736          951.8     0.9971  0.9918  None  \n",
       "1682         926.2     0.9994  0.9853  None  \n",
       "1671         926.2     0.9994  0.9853  None  \n",
       "1462         924.9     0.9993  0.9850  None  \n",
       "1451         924.9     0.9993  0.9850  None  \n",
       "1231           924     0.9993  0.9848  None  \n",
       "1242           924     0.9993  0.9848  None  \n",
       "1022         923.8     0.9990  0.9847  None  \n",
       "1011         923.8     0.9990  0.9847  None  \n",
       "791          921.2     0.9984  0.9841  None  \n",
       "802          921.2     0.9984  0.9841  None  \n",
       "1627           902     0.9996  0.9793  None  \n",
       "1638           902     0.9996  0.9793  None  \n",
       "967          901.7     0.9995  0.9792  None  \n",
       "978          901.7     0.9995  0.9792  None  \n",
       "1418         899.3     0.9995  0.9786  None  \n",
       "...            ...        ...     ...   ...  \n",
       "1385         593.8     0.9999  0.9087  None  \n",
       "1539         593.1     1.0000  0.9085  None  \n",
       "1550         593.1     1.0000  0.9085  None  \n",
       "1605         588.7     0.9999  0.9076  None  \n",
       "1110         588.1     0.9999  0.9075  None  \n",
       "1099         588.1     0.9999  0.9075  None  \n",
       "945          587.8     0.9999  0.9075  None  \n",
       "1165         586.9     0.9999  0.9073  None  \n",
       "1330         583.2     0.9999  0.9064  None  \n",
       "1319         583.2     0.9999  0.9064  None  \n",
       "681          464.7     0.9998  0.8820  None  \n",
       "901          419.4     1.0000  0.8730  None  \n",
       "1121         413.1     1.0000  0.8718  None  \n",
       "1561         413.3     1.0000  0.8718  None  \n",
       "1341         412.8     1.0000  0.8717  None  \n",
       "626          349.3     1.0000  0.8595  None  \n",
       "615          349.3     1.0000  0.8595  None  \n",
       "846          348.6     1.0000  0.8593  None  \n",
       "835          348.6     1.0000  0.8593  None  \n",
       "1066           346     1.0000  0.8589  None  \n",
       "1055           346     1.0000  0.8589  None  \n",
       "1286         345.7     1.0000  0.8588  None  \n",
       "1275         345.7     1.0000  0.8588  None  \n",
       "1506         344.7     1.0000  0.8586  None  \n",
       "1495         344.7     1.0000  0.8586  None  \n",
       "637          242.5     1.0000  0.8396  None  \n",
       "1517         198.5     1.0000  0.8317  None  \n",
       "857          173.4     1.0000  0.8273  None  \n",
       "1077         159.8     1.0000  0.8249  None  \n",
       "1297         155.5     1.0000  0.8241  None  \n",
       "\n",
       "[100 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_stats.loc[(class_stats['Model Name'] == 'Random Forrest') & (class_stats['CV'] == 'avg')].sort_values(by=['Accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Forest also has >99% performance like KNN but appeared to do so with a longer runtime.  Both of these will be compared in more detail in the evaluation and visualizaton sections.  For this model we focused on many parameters.  The \"max_feature\" parameter seemed to have the largest effect as almost all of the top performers had the \"max_features=None\" feature restriction.  After that the \"sqrt\" and \"auto\" parameter values appear to have the second largest effect (because in random forest classifiers they are the same) with the \"log2\"  parameter value in a distant last place. The next most important parameter is \"max_depth\" as the top 3 performers share the same \"max_depth\" (which was the highest \"max_depth\" evaluated). However, this makes us cautious of the possibility of over fitting our model with these deeper trees and ending up with a less useful and less generalized model.  The behavior as estimators increase shows a overall performance growth with little worry about causing over fitting but it appears to come with a execution time increase\n",
    "\n",
    "###### Please note we had a typo of \"Forrest\" instead of \"Forest\" in our model name variable and did not have time to rerun the training models to update the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting estimators=20 max_depht=2 max_features=auto\n",
      "Gradient Boosting estimators=20 max_depht=2 max_features=sqrt\n",
      "Gradient Boosting estimators=20 max_depht=2 max_features=log2\n",
      "Gradient Boosting estimators=20 max_depht=2 max_features=None\n",
      "Gradient Boosting estimators=20 max_depht=4 max_features=auto\n",
      "Gradient Boosting estimators=20 max_depht=4 max_features=sqrt\n",
      "Gradient Boosting estimators=20 max_depht=4 max_features=log2\n",
      "Gradient Boosting estimators=20 max_depht=4 max_features=None\n",
      "Gradient Boosting estimators=20 max_depht=6 max_features=auto\n",
      "Gradient Boosting estimators=20 max_depht=6 max_features=sqrt\n",
      "Gradient Boosting estimators=20 max_depht=6 max_features=log2\n",
      "Gradient Boosting estimators=20 max_depht=6 max_features=None\n",
      "Gradient Boosting estimators=20 max_depht=8 max_features=auto\n",
      "Gradient Boosting estimators=20 max_depht=8 max_features=sqrt\n",
      "Gradient Boosting estimators=20 max_depht=8 max_features=log2\n",
      "Gradient Boosting estimators=20 max_depht=8 max_features=None\n",
      "Gradient Boosting estimators=20 max_depht=10 max_features=auto\n",
      "Gradient Boosting estimators=20 max_depht=10 max_features=sqrt\n",
      "Gradient Boosting estimators=20 max_depht=10 max_features=log2\n",
      "Gradient Boosting estimators=20 max_depht=10 max_features=None\n",
      "Gradient Boosting estimators=40 max_depht=2 max_features=auto\n",
      "Gradient Boosting estimators=40 max_depht=2 max_features=sqrt\n",
      "Gradient Boosting estimators=40 max_depht=2 max_features=log2\n",
      "Gradient Boosting estimators=40 max_depht=2 max_features=None\n",
      "Gradient Boosting estimators=40 max_depht=4 max_features=auto\n",
      "Gradient Boosting estimators=40 max_depht=4 max_features=sqrt\n",
      "Gradient Boosting estimators=40 max_depht=4 max_features=log2\n",
      "Gradient Boosting estimators=40 max_depht=4 max_features=None\n",
      "Gradient Boosting estimators=40 max_depht=6 max_features=auto\n",
      "Gradient Boosting estimators=40 max_depht=6 max_features=sqrt\n",
      "Gradient Boosting estimators=40 max_depht=6 max_features=log2\n",
      "Gradient Boosting estimators=40 max_depht=6 max_features=None\n",
      "Gradient Boosting estimators=40 max_depht=8 max_features=auto\n",
      "Gradient Boosting estimators=40 max_depht=8 max_features=sqrt\n",
      "Gradient Boosting estimators=40 max_depht=8 max_features=log2\n",
      "Gradient Boosting estimators=40 max_depht=8 max_features=None\n",
      "Gradient Boosting estimators=40 max_depht=10 max_features=auto\n",
      "Gradient Boosting estimators=40 max_depht=10 max_features=sqrt\n",
      "Gradient Boosting estimators=40 max_depht=10 max_features=log2\n",
      "Gradient Boosting estimators=40 max_depht=10 max_features=None\n",
      "Gradient Boosting estimators=60 max_depht=2 max_features=auto\n",
      "Gradient Boosting estimators=60 max_depht=2 max_features=sqrt\n",
      "Gradient Boosting estimators=60 max_depht=2 max_features=log2\n",
      "Gradient Boosting estimators=60 max_depht=2 max_features=None\n",
      "Gradient Boosting estimators=60 max_depht=4 max_features=auto\n",
      "Gradient Boosting estimators=60 max_depht=4 max_features=sqrt\n",
      "Gradient Boosting estimators=60 max_depht=4 max_features=log2\n",
      "Gradient Boosting estimators=60 max_depht=4 max_features=None\n",
      "Gradient Boosting estimators=60 max_depht=6 max_features=auto\n",
      "Gradient Boosting estimators=60 max_depht=6 max_features=sqrt\n",
      "Gradient Boosting estimators=60 max_depht=6 max_features=log2\n",
      "Gradient Boosting estimators=60 max_depht=6 max_features=None\n",
      "Gradient Boosting estimators=60 max_depht=8 max_features=auto\n",
      "Gradient Boosting estimators=60 max_depht=8 max_features=sqrt\n",
      "Gradient Boosting estimators=60 max_depht=8 max_features=log2\n",
      "Gradient Boosting estimators=60 max_depht=8 max_features=None\n",
      "Gradient Boosting estimators=60 max_depht=10 max_features=auto\n",
      "Gradient Boosting estimators=60 max_depht=10 max_features=sqrt\n",
      "Gradient Boosting estimators=60 max_depht=10 max_features=log2\n",
      "Gradient Boosting estimators=60 max_depht=10 max_features=None\n",
      "Gradient Boosting estimators=80 max_depht=2 max_features=auto\n",
      "Gradient Boosting estimators=80 max_depht=2 max_features=sqrt\n",
      "Gradient Boosting estimators=80 max_depht=2 max_features=log2\n",
      "Gradient Boosting estimators=80 max_depht=2 max_features=None\n",
      "Gradient Boosting estimators=80 max_depht=4 max_features=auto\n",
      "Gradient Boosting estimators=80 max_depht=4 max_features=sqrt\n",
      "Gradient Boosting estimators=80 max_depht=4 max_features=log2\n",
      "Gradient Boosting estimators=80 max_depht=4 max_features=None\n",
      "Gradient Boosting estimators=80 max_depht=6 max_features=auto\n",
      "Gradient Boosting estimators=80 max_depht=6 max_features=sqrt\n",
      "Gradient Boosting estimators=80 max_depht=6 max_features=log2\n",
      "Gradient Boosting estimators=80 max_depht=6 max_features=None\n",
      "Gradient Boosting estimators=80 max_depht=8 max_features=auto\n",
      "Gradient Boosting estimators=80 max_depht=8 max_features=sqrt\n",
      "Gradient Boosting estimators=80 max_depht=8 max_features=log2\n",
      "Gradient Boosting estimators=80 max_depht=8 max_features=None\n",
      "Gradient Boosting estimators=80 max_depht=10 max_features=auto\n",
      "Gradient Boosting estimators=80 max_depht=10 max_features=sqrt\n",
      "Gradient Boosting estimators=80 max_depht=10 max_features=log2\n",
      "Gradient Boosting estimators=80 max_depht=10 max_features=None\n",
      "Gradient Boosting estimators=100 max_depht=2 max_features=auto\n",
      "Gradient Boosting estimators=100 max_depht=2 max_features=sqrt\n",
      "Gradient Boosting estimators=100 max_depht=2 max_features=log2\n",
      "Gradient Boosting estimators=100 max_depht=2 max_features=None\n",
      "Gradient Boosting estimators=100 max_depht=4 max_features=auto\n",
      "Gradient Boosting estimators=100 max_depht=4 max_features=sqrt\n",
      "Gradient Boosting estimators=100 max_depht=4 max_features=log2\n",
      "Gradient Boosting estimators=100 max_depht=4 max_features=None\n",
      "Gradient Boosting estimators=100 max_depht=6 max_features=auto\n",
      "Gradient Boosting estimators=100 max_depht=6 max_features=sqrt\n",
      "Gradient Boosting estimators=100 max_depht=6 max_features=log2\n",
      "Gradient Boosting estimators=100 max_depht=6 max_features=None\n",
      "Gradient Boosting estimators=100 max_depht=8 max_features=auto\n",
      "Gradient Boosting estimators=100 max_depht=8 max_features=sqrt\n",
      "Gradient Boosting estimators=100 max_depht=8 max_features=log2\n",
      "Gradient Boosting estimators=100 max_depht=8 max_features=None\n",
      "Gradient Boosting estimators=100 max_depht=10 max_features=auto\n",
      "Gradient Boosting estimators=100 max_depht=10 max_features=sqrt\n",
      "Gradient Boosting estimators=100 max_depht=10 max_features=log2\n",
      "Gradient Boosting estimators=100 max_depht=10 max_features=None\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model_name = \"Gradient Boosting\"\n",
    "for N in range(20, 120, 20):\n",
    "    for D in range(2, 12, 2):\n",
    "        model_para = \"estimators=\" + str(N) + \" max_depht=\" + str(D) + \" max_features=auto\"\n",
    "        gb_clf = GradientBoostingClassifier(max_features='auto' , n_estimators=N, max_depth=D, random_state=101)\n",
    "        run_model_class(shuffle_split_clas, model_name, model_para, gb_clf)\n",
    "        model_para = \"estimators=\" + str(N) + \" max_depht=\" + str(D) + \" max_features=sqrt\"\n",
    "        gb_clf = GradientBoostingClassifier(max_features='sqrt' , n_estimators=N, max_depth=D, random_state=101)\n",
    "        run_model_class(shuffle_split_clas, model_name, model_para, gb_clf)\n",
    "        model_para = \"estimators=\" + str(N) + \" max_depht=\" + str(D) + \" max_features=log2\"\n",
    "        gb_clf = GradientBoostingClassifier(max_features='log2' , n_estimators=N, max_depth=D, random_state=101)\n",
    "        run_model_class(shuffle_split_clas, model_name, model_para, gb_clf)\n",
    "        model_para = \"estimators=\" + str(N) + \" max_depht=\" + str(D) + \" max_features=None\"\n",
    "        gb_clf = GradientBoostingClassifier(max_features=None , n_estimators=N, max_depth=D, random_state=101)\n",
    "        run_model_class(shuffle_split_clas, model_name, model_para, gb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Below are the preformance results for our classification model trained above sorted by our Accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Paramaters</th>\n",
       "      <th>CV</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Execution Time</th>\n",
       "      <th>True Positive</th>\n",
       "      <th>False Positive</th>\n",
       "      <th>False Negative</th>\n",
       "      <th>True Negative</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=100 max_depht=4 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>5.5422</td>\n",
       "      <td>3881.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>979.8</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=100 max_depht=4 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>5.4914</td>\n",
       "      <td>3881.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>979.8</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=80 max_depht=4 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>4.4239</td>\n",
       "      <td>3881.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>979.2</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=80 max_depht=4 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>4.4829</td>\n",
       "      <td>3881.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>979.2</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=60 max_depht=4 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9983</td>\n",
       "      <td>3.3706</td>\n",
       "      <td>3881.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>7.3</td>\n",
       "      <td>976.7</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9981</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=60 max_depht=4 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9983</td>\n",
       "      <td>3.3247</td>\n",
       "      <td>3881.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>7.3</td>\n",
       "      <td>976.7</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9981</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=80 max_depht=6 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>6.6095</td>\n",
       "      <td>3880.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>8</td>\n",
       "      <td>976</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=80 max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>6.6048</td>\n",
       "      <td>3880.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>8</td>\n",
       "      <td>976</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2683</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=100 max_depht=6 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>7.1794</td>\n",
       "      <td>3880.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>8</td>\n",
       "      <td>976</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2716</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=100 max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>7.1522</td>\n",
       "      <td>3880.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>8</td>\n",
       "      <td>976</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=40 max_depht=4 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>2.2687</td>\n",
       "      <td>3880.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>974.4</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=40 max_depht=4 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>2.3059</td>\n",
       "      <td>3880.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>974.4</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=60 max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>5.1102</td>\n",
       "      <td>3878.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.2</td>\n",
       "      <td>975.8</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=60 max_depht=6 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>5.1758</td>\n",
       "      <td>3878.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.2</td>\n",
       "      <td>975.8</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2056</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=40 max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9973</td>\n",
       "      <td>3.4102</td>\n",
       "      <td>3877.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.2</td>\n",
       "      <td>975.8</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=40 max_depht=6 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9973</td>\n",
       "      <td>3.4587</td>\n",
       "      <td>3877.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.2</td>\n",
       "      <td>975.8</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1836</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=20 max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9966</td>\n",
       "      <td>1.7195</td>\n",
       "      <td>3878.3</td>\n",
       "      <td>3.7</td>\n",
       "      <td>12.7</td>\n",
       "      <td>971.3</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=20 max_depht=6 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9966</td>\n",
       "      <td>1.7153</td>\n",
       "      <td>3878.3</td>\n",
       "      <td>3.7</td>\n",
       "      <td>12.7</td>\n",
       "      <td>971.3</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2727</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=100 max_depht=8 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>6.4034</td>\n",
       "      <td>3876.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>11.1</td>\n",
       "      <td>972.9</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.9971</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=60 max_depht=8 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>5.4183</td>\n",
       "      <td>3875.9</td>\n",
       "      <td>6.1</td>\n",
       "      <td>10.9</td>\n",
       "      <td>973.1</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=60 max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>5.4442</td>\n",
       "      <td>3875.9</td>\n",
       "      <td>6.1</td>\n",
       "      <td>10.9</td>\n",
       "      <td>973.1</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=80 max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>6.3397</td>\n",
       "      <td>3876.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>11.4</td>\n",
       "      <td>972.6</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.9971</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2760</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=100 max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>6.3256</td>\n",
       "      <td>3876.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>11.1</td>\n",
       "      <td>972.9</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.9971</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=80 max_depht=8 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>6.2971</td>\n",
       "      <td>3876.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>11.4</td>\n",
       "      <td>972.6</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.9971</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=40 max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>3.8190</td>\n",
       "      <td>3875.6</td>\n",
       "      <td>6.4</td>\n",
       "      <td>11.2</td>\n",
       "      <td>972.8</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.9971</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=40 max_depht=8 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>3.7933</td>\n",
       "      <td>3875.6</td>\n",
       "      <td>6.4</td>\n",
       "      <td>11.2</td>\n",
       "      <td>972.8</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.9971</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=100 max_depht=6 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9963</td>\n",
       "      <td>1.0748</td>\n",
       "      <td>3880.7</td>\n",
       "      <td>1.3</td>\n",
       "      <td>16.7</td>\n",
       "      <td>967.3</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=20 max_depht=8 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>1.9231</td>\n",
       "      <td>3876</td>\n",
       "      <td>6</td>\n",
       "      <td>12.7</td>\n",
       "      <td>971.3</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=20 max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>1.8830</td>\n",
       "      <td>3876</td>\n",
       "      <td>6</td>\n",
       "      <td>12.7</td>\n",
       "      <td>971.3</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2584</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=80 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>6.2940</td>\n",
       "      <td>3874</td>\n",
       "      <td>8</td>\n",
       "      <td>12.8</td>\n",
       "      <td>971.2</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2661</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=100 max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.4120</td>\n",
       "      <td>3871</td>\n",
       "      <td>11</td>\n",
       "      <td>73.9</td>\n",
       "      <td>910.1</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>0.9813</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=60 max_depht=2 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>1.3158</td>\n",
       "      <td>3875.8</td>\n",
       "      <td>6.2</td>\n",
       "      <td>81.1</td>\n",
       "      <td>902.9</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.9795</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2188</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=60 max_depht=2 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>1.3144</td>\n",
       "      <td>3875.8</td>\n",
       "      <td>6.2</td>\n",
       "      <td>81.1</td>\n",
       "      <td>902.9</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.9795</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=40 max_depht=4 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9820</td>\n",
       "      <td>0.2795</td>\n",
       "      <td>3877.8</td>\n",
       "      <td>4.2</td>\n",
       "      <td>83.3</td>\n",
       "      <td>900.7</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>0.9790</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=40 max_depht=8 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.5997</td>\n",
       "      <td>3877.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>83.9</td>\n",
       "      <td>900.1</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.9788</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2133</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=40 max_depht=10 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>1.0791</td>\n",
       "      <td>3878.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>91.9</td>\n",
       "      <td>892.1</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>0.9769</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=20 max_depht=6 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>0.2747</td>\n",
       "      <td>3879.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>95.1</td>\n",
       "      <td>888.9</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=80 max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9796</td>\n",
       "      <td>0.3313</td>\n",
       "      <td>3867.9</td>\n",
       "      <td>14.1</td>\n",
       "      <td>85.1</td>\n",
       "      <td>898.9</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>0.9785</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=40 max_depht=6 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>0.3338</td>\n",
       "      <td>3874.6</td>\n",
       "      <td>7.4</td>\n",
       "      <td>100.1</td>\n",
       "      <td>883.9</td>\n",
       "      <td>0.9981</td>\n",
       "      <td>0.9748</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2606</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=100 max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>0.2939</td>\n",
       "      <td>3866.8</td>\n",
       "      <td>15.2</td>\n",
       "      <td>101.5</td>\n",
       "      <td>882.5</td>\n",
       "      <td>0.9961</td>\n",
       "      <td>0.9744</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=40 max_depht=2 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9751</td>\n",
       "      <td>0.9410</td>\n",
       "      <td>3869.8</td>\n",
       "      <td>12.2</td>\n",
       "      <td>109.1</td>\n",
       "      <td>874.9</td>\n",
       "      <td>0.9969</td>\n",
       "      <td>0.9726</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=40 max_depht=2 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9751</td>\n",
       "      <td>0.9083</td>\n",
       "      <td>3869.8</td>\n",
       "      <td>12.2</td>\n",
       "      <td>109.1</td>\n",
       "      <td>874.9</td>\n",
       "      <td>0.9969</td>\n",
       "      <td>0.9726</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=60 max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9743</td>\n",
       "      <td>0.2653</td>\n",
       "      <td>3866.7</td>\n",
       "      <td>15.3</td>\n",
       "      <td>109.8</td>\n",
       "      <td>874.2</td>\n",
       "      <td>0.9961</td>\n",
       "      <td>0.9724</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=80 max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9726</td>\n",
       "      <td>0.2477</td>\n",
       "      <td>3868</td>\n",
       "      <td>14</td>\n",
       "      <td>119.1</td>\n",
       "      <td>864.9</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>0.9701</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=20 max_depht=10 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9681</td>\n",
       "      <td>0.5706</td>\n",
       "      <td>3878.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>151.7</td>\n",
       "      <td>832.3</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>0.9624</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1869</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=20 max_depht=8 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9661</td>\n",
       "      <td>0.3389</td>\n",
       "      <td>3878.3</td>\n",
       "      <td>3.7</td>\n",
       "      <td>161.4</td>\n",
       "      <td>822.6</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>0.9601</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2166</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=60 max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9657</td>\n",
       "      <td>0.2025</td>\n",
       "      <td>3868.7</td>\n",
       "      <td>13.3</td>\n",
       "      <td>153.7</td>\n",
       "      <td>830.3</td>\n",
       "      <td>0.9966</td>\n",
       "      <td>0.9618</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=40 max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9645</td>\n",
       "      <td>0.1997</td>\n",
       "      <td>3873.7</td>\n",
       "      <td>8.3</td>\n",
       "      <td>164.5</td>\n",
       "      <td>819.5</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.9593</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=20 max_depht=4 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9634</td>\n",
       "      <td>0.1734</td>\n",
       "      <td>3878.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>174</td>\n",
       "      <td>810</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>0.9571</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=20 max_depht=2 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9608</td>\n",
       "      <td>0.5008</td>\n",
       "      <td>3865.3</td>\n",
       "      <td>16.7</td>\n",
       "      <td>174</td>\n",
       "      <td>810</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>0.9569</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=20 max_depht=2 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9608</td>\n",
       "      <td>0.5170</td>\n",
       "      <td>3865.3</td>\n",
       "      <td>16.7</td>\n",
       "      <td>174</td>\n",
       "      <td>810</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>0.9569</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2617</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=100 max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9546</td>\n",
       "      <td>0.2180</td>\n",
       "      <td>3854.6</td>\n",
       "      <td>27.4</td>\n",
       "      <td>193.4</td>\n",
       "      <td>790.6</td>\n",
       "      <td>0.9929</td>\n",
       "      <td>0.9522</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=40 max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9523</td>\n",
       "      <td>0.1643</td>\n",
       "      <td>3872.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>222.5</td>\n",
       "      <td>761.5</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>0.9457</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=20 max_depht=6 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>3875.8</td>\n",
       "      <td>6.2</td>\n",
       "      <td>226.7</td>\n",
       "      <td>757.3</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.9448</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=80 max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9461</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>3858.3</td>\n",
       "      <td>23.7</td>\n",
       "      <td>238.8</td>\n",
       "      <td>745.2</td>\n",
       "      <td>0.9939</td>\n",
       "      <td>0.9417</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2177</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=60 max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9272</td>\n",
       "      <td>0.1560</td>\n",
       "      <td>3867.7</td>\n",
       "      <td>14.3</td>\n",
       "      <td>340.1</td>\n",
       "      <td>643.9</td>\n",
       "      <td>0.9963</td>\n",
       "      <td>0.9192</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=20 max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9165</td>\n",
       "      <td>0.1333</td>\n",
       "      <td>3878.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>402.6</td>\n",
       "      <td>581.4</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>0.9060</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=40 max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8988</td>\n",
       "      <td>0.1303</td>\n",
       "      <td>3875.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>485.7</td>\n",
       "      <td>498.3</td>\n",
       "      <td>0.9983</td>\n",
       "      <td>0.8886</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=20 max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8961</td>\n",
       "      <td>0.1155</td>\n",
       "      <td>3881</td>\n",
       "      <td>1</td>\n",
       "      <td>504.7</td>\n",
       "      <td>479.3</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=20 max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8423</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>3881.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>766.7</td>\n",
       "      <td>217.3</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.8351</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Type         Model Name  \\\n",
       "2639  classification  Gradient Boosting   \n",
       "2672  classification  Gradient Boosting   \n",
       "2452  classification  Gradient Boosting   \n",
       "2419  classification  Gradient Boosting   \n",
       "2199  classification  Gradient Boosting   \n",
       "2232  classification  Gradient Boosting   \n",
       "2463  classification  Gradient Boosting   \n",
       "2496  classification  Gradient Boosting   \n",
       "2683  classification  Gradient Boosting   \n",
       "2716  classification  Gradient Boosting   \n",
       "2012  classification  Gradient Boosting   \n",
       "1979  classification  Gradient Boosting   \n",
       "2276  classification  Gradient Boosting   \n",
       "2243  classification  Gradient Boosting   \n",
       "2056  classification  Gradient Boosting   \n",
       "2023  classification  Gradient Boosting   \n",
       "1836  classification  Gradient Boosting   \n",
       "1803  classification  Gradient Boosting   \n",
       "2727  classification  Gradient Boosting   \n",
       "2287  classification  Gradient Boosting   \n",
       "2320  classification  Gradient Boosting   \n",
       "2540  classification  Gradient Boosting   \n",
       "2760  classification  Gradient Boosting   \n",
       "2507  classification  Gradient Boosting   \n",
       "2100  classification  Gradient Boosting   \n",
       "2067  classification  Gradient Boosting   \n",
       "2694  classification  Gradient Boosting   \n",
       "1847  classification  Gradient Boosting   \n",
       "1880  classification  Gradient Boosting   \n",
       "2584  classification  Gradient Boosting   \n",
       "...              ...                ...   \n",
       "2661  classification  Gradient Boosting   \n",
       "2155  classification  Gradient Boosting   \n",
       "2188  classification  Gradient Boosting   \n",
       "1990  classification  Gradient Boosting   \n",
       "2089  classification  Gradient Boosting   \n",
       "2133  classification  Gradient Boosting   \n",
       "1814  classification  Gradient Boosting   \n",
       "2441  classification  Gradient Boosting   \n",
       "2045  classification  Gradient Boosting   \n",
       "2606  classification  Gradient Boosting   \n",
       "1935  classification  Gradient Boosting   \n",
       "1968  classification  Gradient Boosting   \n",
       "2221  classification  Gradient Boosting   \n",
       "2386  classification  Gradient Boosting   \n",
       "1913  classification  Gradient Boosting   \n",
       "1869  classification  Gradient Boosting   \n",
       "2166  classification  Gradient Boosting   \n",
       "2001  classification  Gradient Boosting   \n",
       "1770  classification  Gradient Boosting   \n",
       "1748  classification  Gradient Boosting   \n",
       "1715  classification  Gradient Boosting   \n",
       "2617  classification  Gradient Boosting   \n",
       "1946  classification  Gradient Boosting   \n",
       "1825  classification  Gradient Boosting   \n",
       "2397  classification  Gradient Boosting   \n",
       "2177  classification  Gradient Boosting   \n",
       "1781  classification  Gradient Boosting   \n",
       "1957  classification  Gradient Boosting   \n",
       "1726  classification  Gradient Boosting   \n",
       "1737  classification  Gradient Boosting   \n",
       "\n",
       "                                        Paramaters   CV  Accuracy  \\\n",
       "2639  estimators=100 max_depht=4 max_features=auto  avg    0.9991   \n",
       "2672  estimators=100 max_depht=4 max_features=None  avg    0.9991   \n",
       "2452   estimators=80 max_depht=4 max_features=None  avg    0.9989   \n",
       "2419   estimators=80 max_depht=4 max_features=auto  avg    0.9989   \n",
       "2199   estimators=60 max_depht=4 max_features=auto  avg    0.9983   \n",
       "2232   estimators=60 max_depht=4 max_features=None  avg    0.9983   \n",
       "2463   estimators=80 max_depht=6 max_features=auto  avg    0.9980   \n",
       "2496   estimators=80 max_depht=6 max_features=None  avg    0.9980   \n",
       "2683  estimators=100 max_depht=6 max_features=auto  avg    0.9980   \n",
       "2716  estimators=100 max_depht=6 max_features=None  avg    0.9980   \n",
       "2012   estimators=40 max_depht=4 max_features=None  avg    0.9978   \n",
       "1979   estimators=40 max_depht=4 max_features=auto  avg    0.9978   \n",
       "2276   estimators=60 max_depht=6 max_features=None  avg    0.9976   \n",
       "2243   estimators=60 max_depht=6 max_features=auto  avg    0.9976   \n",
       "2056   estimators=40 max_depht=6 max_features=None  avg    0.9973   \n",
       "2023   estimators=40 max_depht=6 max_features=auto  avg    0.9973   \n",
       "1836   estimators=20 max_depht=6 max_features=None  avg    0.9966   \n",
       "1803   estimators=20 max_depht=6 max_features=auto  avg    0.9966   \n",
       "2727  estimators=100 max_depht=8 max_features=auto  avg    0.9965   \n",
       "2287   estimators=60 max_depht=8 max_features=auto  avg    0.9965   \n",
       "2320   estimators=60 max_depht=8 max_features=None  avg    0.9965   \n",
       "2540   estimators=80 max_depht=8 max_features=None  avg    0.9965   \n",
       "2760  estimators=100 max_depht=8 max_features=None  avg    0.9965   \n",
       "2507   estimators=80 max_depht=8 max_features=auto  avg    0.9965   \n",
       "2100   estimators=40 max_depht=8 max_features=None  avg    0.9964   \n",
       "2067   estimators=40 max_depht=8 max_features=auto  avg    0.9964   \n",
       "2694  estimators=100 max_depht=6 max_features=sqrt  avg    0.9963   \n",
       "1847   estimators=20 max_depht=8 max_features=auto  avg    0.9962   \n",
       "1880   estimators=20 max_depht=8 max_features=None  avg    0.9962   \n",
       "2584  estimators=80 max_depht=10 max_features=None  avg    0.9957   \n",
       "...                                            ...  ...       ...   \n",
       "2661  estimators=100 max_depht=4 max_features=log2  avg    0.9826   \n",
       "2155   estimators=60 max_depht=2 max_features=auto  avg    0.9821   \n",
       "2188   estimators=60 max_depht=2 max_features=None  avg    0.9821   \n",
       "1990   estimators=40 max_depht=4 max_features=sqrt  avg    0.9820   \n",
       "2089   estimators=40 max_depht=8 max_features=log2  avg    0.9818   \n",
       "2133  estimators=40 max_depht=10 max_features=log2  avg    0.9804   \n",
       "1814   estimators=20 max_depht=6 max_features=sqrt  avg    0.9799   \n",
       "2441   estimators=80 max_depht=4 max_features=log2  avg    0.9796   \n",
       "2045   estimators=40 max_depht=6 max_features=log2  avg    0.9779   \n",
       "2606  estimators=100 max_depht=2 max_features=sqrt  avg    0.9760   \n",
       "1935   estimators=40 max_depht=2 max_features=auto  avg    0.9751   \n",
       "1968   estimators=40 max_depht=2 max_features=None  avg    0.9751   \n",
       "2221   estimators=60 max_depht=4 max_features=log2  avg    0.9743   \n",
       "2386   estimators=80 max_depht=2 max_features=sqrt  avg    0.9726   \n",
       "1913  estimators=20 max_depht=10 max_features=log2  avg    0.9681   \n",
       "1869   estimators=20 max_depht=8 max_features=log2  avg    0.9661   \n",
       "2166   estimators=60 max_depht=2 max_features=sqrt  avg    0.9657   \n",
       "2001   estimators=40 max_depht=4 max_features=log2  avg    0.9645   \n",
       "1770   estimators=20 max_depht=4 max_features=sqrt  avg    0.9634   \n",
       "1748   estimators=20 max_depht=2 max_features=None  avg    0.9608   \n",
       "1715   estimators=20 max_depht=2 max_features=auto  avg    0.9608   \n",
       "2617  estimators=100 max_depht=2 max_features=log2  avg    0.9546   \n",
       "1946   estimators=40 max_depht=2 max_features=sqrt  avg    0.9523   \n",
       "1825   estimators=20 max_depht=6 max_features=log2  avg    0.9521   \n",
       "2397   estimators=80 max_depht=2 max_features=log2  avg    0.9461   \n",
       "2177   estimators=60 max_depht=2 max_features=log2  avg    0.9272   \n",
       "1781   estimators=20 max_depht=4 max_features=log2  avg    0.9165   \n",
       "1957   estimators=40 max_depht=2 max_features=log2  avg    0.8988   \n",
       "1726   estimators=20 max_depht=2 max_features=sqrt  avg    0.8961   \n",
       "1737   estimators=20 max_depht=2 max_features=log2  avg    0.8423   \n",
       "\n",
       "      Execution Time True Positive False Positive False Negative  \\\n",
       "2639          5.5422        3881.6            0.4            4.2   \n",
       "2672          5.4914        3881.6            0.4            4.2   \n",
       "2452          4.4239        3881.4            0.6            4.8   \n",
       "2419          4.4829        3881.4            0.6            4.8   \n",
       "2199          3.3706        3881.2            0.8            7.3   \n",
       "2232          3.3247        3881.2            0.8            7.3   \n",
       "2463          6.6095        3880.5            1.5              8   \n",
       "2496          6.6048        3880.5            1.5              8   \n",
       "2683          7.1794        3880.4            1.6              8   \n",
       "2716          7.1522        3880.4            1.6              8   \n",
       "2012          2.2687        3880.8            1.2            9.6   \n",
       "1979          2.3059        3880.8            1.2            9.6   \n",
       "2276          5.1102        3878.6            3.4            8.2   \n",
       "2243          5.1758        3878.6            3.4            8.2   \n",
       "2056          3.4102        3877.3            4.7            8.2   \n",
       "2023          3.4587        3877.3            4.7            8.2   \n",
       "1836          1.7195        3878.3            3.7           12.7   \n",
       "1803          1.7153        3878.3            3.7           12.7   \n",
       "2727          6.4034        3876.3            5.7           11.1   \n",
       "2287          5.4183        3875.9            6.1           10.9   \n",
       "2320          5.4442        3875.9            6.1           10.9   \n",
       "2540          6.3397        3876.3            5.7           11.4   \n",
       "2760          6.3256        3876.3            5.7           11.1   \n",
       "2507          6.2971        3876.3            5.7           11.4   \n",
       "2100          3.8190        3875.6            6.4           11.2   \n",
       "2067          3.7933        3875.6            6.4           11.2   \n",
       "2694          1.0748        3880.7            1.3           16.7   \n",
       "1847          1.9231          3876              6           12.7   \n",
       "1880          1.8830          3876              6           12.7   \n",
       "2584          6.2940          3874              8           12.8   \n",
       "...              ...           ...            ...            ...   \n",
       "2661          0.4120          3871             11           73.9   \n",
       "2155          1.3158        3875.8            6.2           81.1   \n",
       "2188          1.3144        3875.8            6.2           81.1   \n",
       "1990          0.2795        3877.8            4.2           83.3   \n",
       "2089          0.5997        3877.2            4.8           83.9   \n",
       "2133          1.0791        3878.7            3.3           91.9   \n",
       "1814          0.2747        3879.5            2.5           95.1   \n",
       "2441          0.3313        3867.9           14.1           85.1   \n",
       "2045          0.3338        3874.6            7.4          100.1   \n",
       "2606          0.2939        3866.8           15.2          101.5   \n",
       "1935          0.9410        3869.8           12.2          109.1   \n",
       "1968          0.9083        3869.8           12.2          109.1   \n",
       "2221          0.2653        3866.7           15.3          109.8   \n",
       "2386          0.2477          3868             14          119.1   \n",
       "1913          0.5706        3878.7            3.3          151.7   \n",
       "1869          0.3389        3878.3            3.7          161.4   \n",
       "2166          0.2025        3868.7           13.3          153.7   \n",
       "2001          0.1997        3873.7            8.3          164.5   \n",
       "1770          0.1734        3878.1            3.9            174   \n",
       "1748          0.5008        3865.3           16.7            174   \n",
       "1715          0.5170        3865.3           16.7            174   \n",
       "2617          0.2180        3854.6           27.4          193.4   \n",
       "1946          0.1643        3872.5            9.5          222.5   \n",
       "1825          0.2028        3875.8            6.2          226.7   \n",
       "2397          0.1860        3858.3           23.7          238.8   \n",
       "2177          0.1560        3867.7           14.3          340.1   \n",
       "1781          0.1333        3878.5            3.5          402.6   \n",
       "1957          0.1303        3875.5            6.5          485.7   \n",
       "1726          0.1155          3881              1          504.7   \n",
       "1737          0.0964        3881.5            0.5          766.7   \n",
       "\n",
       "     True Negative  Precision  Recall Model  \n",
       "2639         979.8     0.9999  0.9989  None  \n",
       "2672         979.8     0.9999  0.9989  None  \n",
       "2452         979.2     0.9998  0.9988  None  \n",
       "2419         979.2     0.9998  0.9988  None  \n",
       "2199         976.7     0.9998  0.9981  None  \n",
       "2232         976.7     0.9998  0.9981  None  \n",
       "2463           976     0.9996  0.9979  None  \n",
       "2496           976     0.9996  0.9979  None  \n",
       "2683           976     0.9996  0.9979  None  \n",
       "2716           976     0.9996  0.9979  None  \n",
       "2012         974.4     0.9997  0.9975  None  \n",
       "1979         974.4     0.9997  0.9975  None  \n",
       "2276         975.8     0.9991  0.9979  None  \n",
       "2243         975.8     0.9991  0.9979  None  \n",
       "2056         975.8     0.9988  0.9979  None  \n",
       "2023         975.8     0.9988  0.9979  None  \n",
       "1836         971.3     0.9990  0.9967  None  \n",
       "1803         971.3     0.9990  0.9967  None  \n",
       "2727         972.9     0.9985  0.9971  None  \n",
       "2287         973.1     0.9984  0.9972  None  \n",
       "2320         973.1     0.9984  0.9972  None  \n",
       "2540         972.6     0.9985  0.9971  None  \n",
       "2760         972.9     0.9985  0.9971  None  \n",
       "2507         972.6     0.9985  0.9971  None  \n",
       "2100         972.8     0.9984  0.9971  None  \n",
       "2067         972.8     0.9984  0.9971  None  \n",
       "2694         967.3     0.9997  0.9957  None  \n",
       "1847         971.3     0.9985  0.9967  None  \n",
       "1880         971.3     0.9985  0.9967  None  \n",
       "2584         971.2     0.9979  0.9967  None  \n",
       "...            ...        ...     ...   ...  \n",
       "2661         910.1     0.9972  0.9813  None  \n",
       "2155         902.9     0.9984  0.9795  None  \n",
       "2188         902.9     0.9984  0.9795  None  \n",
       "1990         900.7     0.9989  0.9790  None  \n",
       "2089         900.1     0.9988  0.9788  None  \n",
       "2133         892.1     0.9991  0.9769  None  \n",
       "1814         888.9     0.9994  0.9761  None  \n",
       "2441         898.9     0.9964  0.9785  None  \n",
       "2045         883.9     0.9981  0.9748  None  \n",
       "2606         882.5     0.9961  0.9744  None  \n",
       "1935         874.9     0.9969  0.9726  None  \n",
       "1968         874.9     0.9969  0.9726  None  \n",
       "2221         874.2     0.9961  0.9724  None  \n",
       "2386         864.9     0.9964  0.9701  None  \n",
       "1913         832.3     0.9991  0.9624  None  \n",
       "1869         822.6     0.9990  0.9601  None  \n",
       "2166         830.3     0.9966  0.9618  None  \n",
       "2001         819.5     0.9979  0.9593  None  \n",
       "1770           810     0.9990  0.9571  None  \n",
       "1748           810     0.9957  0.9569  None  \n",
       "1715           810     0.9957  0.9569  None  \n",
       "2617         790.6     0.9929  0.9522  None  \n",
       "1946         761.5     0.9976  0.9457  None  \n",
       "1825         757.3     0.9984  0.9448  None  \n",
       "2397         745.2     0.9939  0.9417  None  \n",
       "2177         643.9     0.9963  0.9192  None  \n",
       "1781         581.4     0.9991  0.9060  None  \n",
       "1957         498.3     0.9983  0.8886  None  \n",
       "1726         479.3     0.9997  0.8849  None  \n",
       "1737         217.3     0.9999  0.8351  None  \n",
       "\n",
       "[100 rows x 13 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_stats.loc[(class_stats['Model Name'] == 'Gradient Boosting') & (class_stats['CV'] == 'avg')].sort_values(by=['Accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Our Gradient Boosting algorithm has >99% Accuracy and, as expected, accuracy climbs with the number of estimators.  But curiously enough, a \"max_depth\" of 4 seems to be the sweet spot in this algorithm/data set as all of our top 5 performers share this value.  As for the \"max_features\" value, we notice that the \"max_features=None\" and \"max_features=auto\" settings far out-perform the \"log2\" setting and we might conclude that this algorithm is friendlier to large feature sets.  Finally, we looked at average execution time and found that this algorithm is by far most effected by our parameter selection (especially number of estimators and max depth) sometimes running 10 times longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Regression Models: Decision Tree, Random Forest, Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_stats = pd.DataFrame( columns = ['Type', 'Model Name','Paramaters','CV', 'Execution Time', 'MAE', 'MSE', 'Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def run_model_reg(shuffle_split, model_name, model_para, model):\n",
    "    count = 1\n",
    "    for train_index, test_index in shuffle_split_clas.split(X, y): \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        start = time.time()\n",
    "        model.fit(X_train,y_train)  \n",
    "        y_hat = model.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, y_hat)\n",
    "        mse = mean_squared_error(y_test, y_hat)  \n",
    "        reg_stats.loc[len(reg_stats)] =  ['regression', model_name, model_para, count, round(time.time() - start,4), mae, mse, model]\n",
    "        count = count + 1\n",
    "    avg_ex = round(reg_stats.loc[(reg_stats['Model Name'] == model_name) & (reg_stats['Paramaters'] == model_para)]['Execution Time'].mean(),4)\n",
    "    avg_mae = round(reg_stats.loc[(reg_stats['Model Name'] == model_name) & (reg_stats['Paramaters'] == model_para)]['MAE'].mean(),4)\n",
    "    avg_mse = round(reg_stats.loc[(reg_stats['Model Name'] == model_name) & (reg_stats['Paramaters'] == model_para)]['MSE'].mean(),4)\n",
    "    reg_stats.loc[len(reg_stats)] =  ['regression', model_name, model_para, 'avg', avg_ex, avg_mae, avg_mse, None]\n",
    "    print(model_name + \" \" + model_para)\n",
    "#    eval_cv(reg_stats, model_name, 'MAE')\n",
    "#    eval_cv(reg_stats, model_name, 'Execution Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Like in our Classification problem, we started off with two blocks of code for setup functions needed to more easily train each model.  The first line is simply initializing a data frame for holding all model related information including the model itself.  The second block contains the run_model_reg wrapper function that runs each CV for a given model, takes all evaluation metrics and creates an average performance score for each model based on the average CV performance. This is nearly identical to the classification one but with the exception of the test statistics collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree splitter=best max_depht=2 max_features=auto\n",
      "Decision Tree splitter=best max_depht=2 max_features=sqrt\n",
      "Decision Tree splitter=best max_depht=2 max_features=log2\n",
      "Decision Tree splitter=best max_depht=2 max_features=None\n",
      "Decision Tree splitter=random max_depht=2 max_features=auto\n",
      "Decision Tree splitter=random max_depht=2 max_features=sqrt\n",
      "Decision Tree splitter=random max_depht=2 max_features=log2\n",
      "Decision Tree splitter=random max_depht=2 max_features=None\n",
      "Decision Tree splitter=best max_depht=4 max_features=auto\n",
      "Decision Tree splitter=best max_depht=4 max_features=sqrt\n",
      "Decision Tree splitter=best max_depht=4 max_features=log2\n",
      "Decision Tree splitter=best max_depht=4 max_features=None\n",
      "Decision Tree splitter=random max_depht=4 max_features=auto\n",
      "Decision Tree splitter=random max_depht=4 max_features=sqrt\n",
      "Decision Tree splitter=random max_depht=4 max_features=log2\n",
      "Decision Tree splitter=random max_depht=4 max_features=None\n",
      "Decision Tree splitter=best max_depht=6 max_features=auto\n",
      "Decision Tree splitter=best max_depht=6 max_features=sqrt\n",
      "Decision Tree splitter=best max_depht=6 max_features=log2\n",
      "Decision Tree splitter=best max_depht=6 max_features=None\n",
      "Decision Tree splitter=random max_depht=6 max_features=auto\n",
      "Decision Tree splitter=random max_depht=6 max_features=sqrt\n",
      "Decision Tree splitter=random max_depht=6 max_features=log2\n",
      "Decision Tree splitter=random max_depht=6 max_features=None\n",
      "Decision Tree splitter=best max_depht=8 max_features=auto\n",
      "Decision Tree splitter=best max_depht=8 max_features=sqrt\n",
      "Decision Tree splitter=best max_depht=8 max_features=log2\n",
      "Decision Tree splitter=best max_depht=8 max_features=None\n",
      "Decision Tree splitter=random max_depht=8 max_features=auto\n",
      "Decision Tree splitter=random max_depht=8 max_features=sqrt\n",
      "Decision Tree splitter=random max_depht=8 max_features=log2\n",
      "Decision Tree splitter=random max_depht=8 max_features=None\n",
      "Decision Tree splitter=best max_depht=10 max_features=auto\n",
      "Decision Tree splitter=best max_depht=10 max_features=sqrt\n",
      "Decision Tree splitter=best max_depht=10 max_features=log2\n",
      "Decision Tree splitter=best max_depht=10 max_features=None\n",
      "Decision Tree splitter=random max_depht=10 max_features=auto\n",
      "Decision Tree splitter=random max_depht=10 max_features=sqrt\n",
      "Decision Tree splitter=random max_depht=10 max_features=log2\n",
      "Decision Tree splitter=random max_depht=10 max_features=None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model_name = \"Decision Tree\"\n",
    "for D in range(2, 12, 2):\n",
    "    model_para = \"splitter=best max_depth=\" + str(D) + \" max_features=auto\"\n",
    "    rf_reg = DecisionTreeRegressor(max_features='auto' , splitter='best', max_depth=D, random_state=101)\n",
    "    run_model_reg(shuffle_split_reg, model_name, model_para, rf_reg)\n",
    "    model_para = \"splitter=best max_depth=\" + str(D) + \" max_features=sqrt\"\n",
    "    rf_reg = DecisionTreeRegressor(max_features='sqrt' , splitter='best', max_depth=D, random_state=101)\n",
    "    run_model_reg(shuffle_split_reg, model_name, model_para, rf_reg)\n",
    "    model_para = \"splitter=best max_depth=\" + str(D) + \" max_features=log2\"\n",
    "    rf_reg = DecisionTreeRegressor(max_features='log2' , splitter='best', max_depth=D, random_state=101)\n",
    "    run_model_reg(shuffle_split_reg, model_name, model_para, rf_reg)\n",
    "    model_para = \"splitter=best max_depth=\" + str(D) + \" max_features=None\"\n",
    "    rf_reg = DecisionTreeRegressor(max_features=None , splitter='best', max_depth=D, random_state=101)\n",
    "    run_model_reg(shuffle_split_reg, model_name, model_para, rf_reg)\n",
    "    model_para = \"splitter=random max_depth=\" + str(D) + \" max_features=auto\"\n",
    "    rf_reg = DecisionTreeRegressor(max_features='auto' , splitter='random', max_depth=D, random_state=101)\n",
    "    run_model_reg(shuffle_split_reg, model_name, model_para, rf_reg)\n",
    "    model_para = \"splitter=random max_depth=\" + str(D) + \" max_features=sqrt\"\n",
    "    rf_reg = DecisionTreeRegressor(max_features='sqrt' , splitter='random', max_depth=D, random_state=101)\n",
    "    run_model_reg(shuffle_split_reg, model_name, model_para, rf_reg)\n",
    "    model_para = \"splitter=random max_depth=\" + str(D) + \" max_features=log2\"\n",
    "    rf_reg = DecisionTreeRegressor(max_features='log2' , splitter='random', max_depth=D, random_state=101)\n",
    "    run_model_reg(shuffle_split_reg, model_name, model_para, rf_reg)\n",
    "    model_para = \"splitter=random max_depth=\" + str(D) + \" max_features=None\"\n",
    "    rf_reg = DecisionTreeRegressor(max_features=None , splitter='random', max_depth=D, random_state=101)\n",
    "    run_model_reg(shuffle_split_reg, model_name, model_para, rf_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Below are the preformance results for our regression model trained above sorted by our MSE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Paramaters</th>\n",
       "      <th>CV</th>\n",
       "      <th>Execution Time</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.1817</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=8 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.1822</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=random max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0459</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=random max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=random max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=random max_depht=8 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0431</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=6 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.1715</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.1697</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=random max_depht=6 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0372</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=random max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=4 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>0.0280</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=4 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.1412</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>0.0280</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=8 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=10 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0253</td>\n",
       "      <td>0.0352</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=6 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.0634</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=random max_depht=4 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.1004</td>\n",
       "      <td>0.0511</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=random max_depht=4 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0291</td>\n",
       "      <td>0.1004</td>\n",
       "      <td>0.0511</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=4 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>0.0514</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=10 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>0.0711</td>\n",
       "      <td>0.0563</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=8 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0173</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>0.0576</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=2 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0777</td>\n",
       "      <td>0.1162</td>\n",
       "      <td>0.0586</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=2 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>0.1162</td>\n",
       "      <td>0.0586</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=6 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.1205</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.1611</td>\n",
       "      <td>0.0834</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=random max_depht=10 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.1546</td>\n",
       "      <td>0.0908</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.1896</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=random max_depht=8 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.1895</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=random max_depht=2 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.2014</td>\n",
       "      <td>0.1013</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=random max_depht=2 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.2014</td>\n",
       "      <td>0.1013</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=random max_depht=6 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.2183</td>\n",
       "      <td>0.1115</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.2294</td>\n",
       "      <td>0.1154</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=random max_depht=10 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.2221</td>\n",
       "      <td>0.1232</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=random max_depht=8 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>0.1237</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=random max_depht=6 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.2512</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=random max_depht=4 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.2660</td>\n",
       "      <td>0.1338</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=random max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.1371</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=random max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.2989</td>\n",
       "      <td>0.1501</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=random max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.3072</td>\n",
       "      <td>0.1540</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Type     Model Name  \\\n",
       "395  regression  Decision Tree   \n",
       "362  regression  Decision Tree   \n",
       "307  regression  Decision Tree   \n",
       "274  regression  Decision Tree   \n",
       "439  regression  Decision Tree   \n",
       "406  regression  Decision Tree   \n",
       "351  regression  Decision Tree   \n",
       "318  regression  Decision Tree   \n",
       "186  regression  Decision Tree   \n",
       "219  regression  Decision Tree   \n",
       "230  regression  Decision Tree   \n",
       "263  regression  Decision Tree   \n",
       "131  regression  Decision Tree   \n",
       "98   regression  Decision Tree   \n",
       "285  regression  Decision Tree   \n",
       "373  regression  Decision Tree   \n",
       "197  regression  Decision Tree   \n",
       "142  regression  Decision Tree   \n",
       "175  regression  Decision Tree   \n",
       "109  regression  Decision Tree   \n",
       "384  regression  Decision Tree   \n",
       "296  regression  Decision Tree   \n",
       "10   regression  Decision Tree   \n",
       "43   regression  Decision Tree   \n",
       "208  regression  Decision Tree   \n",
       "120  regression  Decision Tree   \n",
       "417  regression  Decision Tree   \n",
       "21   regression  Decision Tree   \n",
       "329  regression  Decision Tree   \n",
       "54   regression  Decision Tree   \n",
       "87   regression  Decision Tree   \n",
       "241  regression  Decision Tree   \n",
       "32   regression  Decision Tree   \n",
       "428  regression  Decision Tree   \n",
       "340  regression  Decision Tree   \n",
       "252  regression  Decision Tree   \n",
       "153  regression  Decision Tree   \n",
       "164  regression  Decision Tree   \n",
       "65   regression  Decision Tree   \n",
       "76   regression  Decision Tree   \n",
       "\n",
       "                                         Paramaters   CV  Execution Time  \\\n",
       "395    splitter=best max_depht=10 max_features=None  avg          0.1898   \n",
       "362    splitter=best max_depht=10 max_features=auto  avg          0.1847   \n",
       "307     splitter=best max_depht=8 max_features=None  avg          0.1817   \n",
       "274     splitter=best max_depht=8 max_features=auto  avg          0.1822   \n",
       "439  splitter=random max_depht=10 max_features=None  avg          0.0459   \n",
       "406  splitter=random max_depht=10 max_features=auto  avg          0.0455   \n",
       "351   splitter=random max_depht=8 max_features=None  avg          0.0428   \n",
       "318   splitter=random max_depht=8 max_features=auto  avg          0.0431   \n",
       "186     splitter=best max_depht=6 max_features=auto  avg          0.1715   \n",
       "219     splitter=best max_depht=6 max_features=None  avg          0.1697   \n",
       "230   splitter=random max_depht=6 max_features=auto  avg          0.0372   \n",
       "263   splitter=random max_depht=6 max_features=None  avg          0.0374   \n",
       "131     splitter=best max_depht=4 max_features=None  avg          0.1410   \n",
       "98      splitter=best max_depht=4 max_features=auto  avg          0.1412   \n",
       "285     splitter=best max_depht=8 max_features=sqrt  avg          0.0245   \n",
       "373    splitter=best max_depht=10 max_features=sqrt  avg          0.0253   \n",
       "197     splitter=best max_depht=6 max_features=sqrt  avg          0.0221   \n",
       "142   splitter=random max_depht=4 max_features=auto  avg          0.0289   \n",
       "175   splitter=random max_depht=4 max_features=None  avg          0.0291   \n",
       "109     splitter=best max_depht=4 max_features=sqrt  avg          0.0181   \n",
       "384    splitter=best max_depht=10 max_features=log2  avg          0.0192   \n",
       "296     splitter=best max_depht=8 max_features=log2  avg          0.0173   \n",
       "10      splitter=best max_depht=2 max_features=auto  avg          0.0777   \n",
       "43      splitter=best max_depht=2 max_features=None  avg          0.0770   \n",
       "208     splitter=best max_depht=6 max_features=log2  avg          0.0160   \n",
       "120     splitter=best max_depht=4 max_features=log2  avg          0.0135   \n",
       "417  splitter=random max_depht=10 max_features=sqrt  avg          0.0129   \n",
       "21      splitter=best max_depht=2 max_features=sqrt  avg          0.0136   \n",
       "329   splitter=random max_depht=8 max_features=sqrt  avg          0.0115   \n",
       "54    splitter=random max_depht=2 max_features=auto  avg          0.0193   \n",
       "87    splitter=random max_depht=2 max_features=None  avg          0.0193   \n",
       "241   splitter=random max_depht=6 max_features=sqrt  avg          0.0113   \n",
       "32      splitter=best max_depht=2 max_features=log2  avg          0.0114   \n",
       "428  splitter=random max_depht=10 max_features=log2  avg          0.0110   \n",
       "340   splitter=random max_depht=8 max_features=log2  avg          0.0106   \n",
       "252   splitter=random max_depht=6 max_features=log2  avg          0.0103   \n",
       "153   splitter=random max_depht=4 max_features=sqrt  avg          0.0108   \n",
       "164   splitter=random max_depht=4 max_features=log2  avg          0.0100   \n",
       "65    splitter=random max_depht=2 max_features=sqrt  avg          0.0101   \n",
       "76    splitter=random max_depht=2 max_features=log2  avg          0.0096   \n",
       "\n",
       "        MAE     MSE Model  \n",
       "395  0.0065  0.0064  None  \n",
       "362  0.0065  0.0064  None  \n",
       "307  0.0084  0.0072  None  \n",
       "274  0.0084  0.0072  None  \n",
       "439  0.0088  0.0079  None  \n",
       "406  0.0088  0.0079  None  \n",
       "351  0.0174  0.0113  None  \n",
       "318  0.0174  0.0113  None  \n",
       "186  0.0205  0.0129  None  \n",
       "219  0.0205  0.0129  None  \n",
       "230  0.0395  0.0214  None  \n",
       "263  0.0395  0.0214  None  \n",
       "131  0.0525  0.0280  None  \n",
       "98   0.0525  0.0280  None  \n",
       "285  0.0422  0.0299  None  \n",
       "373  0.0352  0.0300  None  \n",
       "197  0.0634  0.0357  None  \n",
       "142  0.1004  0.0511  None  \n",
       "175  0.1004  0.0511  None  \n",
       "109  0.0985  0.0514  None  \n",
       "384  0.0711  0.0563  None  \n",
       "296  0.0873  0.0576  None  \n",
       "10   0.1162  0.0586  None  \n",
       "43   0.1162  0.0586  None  \n",
       "208  0.1205  0.0666  None  \n",
       "120  0.1611  0.0834  None  \n",
       "417  0.1546  0.0908  None  \n",
       "21   0.1896  0.0958  None  \n",
       "329  0.1895  0.1010  None  \n",
       "54   0.2014  0.1013  None  \n",
       "87   0.2014  0.1013  None  \n",
       "241  0.2183  0.1115  None  \n",
       "32   0.2294  0.1154  None  \n",
       "428  0.2221  0.1232  None  \n",
       "340  0.2370  0.1237  None  \n",
       "252  0.2512  0.1282  None  \n",
       "153  0.2660  0.1338  None  \n",
       "164  0.2732  0.1371  None  \n",
       "65   0.2989  0.1501  None  \n",
       "76   0.3072  0.1540  None  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stats.loc[(reg_stats['Model Name'] == 'Decision Tree') & (reg_stats['CV'] == 'avg')].sort_values(by=['MSE'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  For our Decision Tree models we can see that overall this algorithm is very fast and wins out in speed due to its simplicity.  In the case with a random split and max depth of 2 we see ~0.01 seconds and below for execution time.  As for the splitter parameter (method used for deciding the split at each node) the \"best\" option almost always out performs the \"random\" option but at the cost of about 4x execution time.  Again, like we noted in most all of our tree based alogrithms, it is very happy to have a higher depth and a larger set of features to work with.  Finally, performance on MSE is very good seeing values of  <0.01 for our top 5 and in most cases <0.1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest estimators=20 max_depht=2 max_features=auto\n",
      "Random Forest estimators=20 max_depht=2 max_features=sqrt\n",
      "Random Forest estimators=20 max_depht=2 max_features=log2\n",
      "Random Forest estimators=20 max_depht=2 max_features=None\n",
      "Random Forest estimators=20 max_depht=4 max_features=auto\n",
      "Random Forest estimators=20 max_depht=4 max_features=sqrt\n",
      "Random Forest estimators=20 max_depht=4 max_features=log2\n",
      "Random Forest estimators=20 max_depht=4 max_features=None\n",
      "Random Forest estimators=20 max_depht=6 max_features=auto\n",
      "Random Forest estimators=20 max_depht=6 max_features=sqrt\n",
      "Random Forest estimators=20 max_depht=6 max_features=log2\n",
      "Random Forest estimators=20 max_depht=6 max_features=None\n",
      "Random Forest estimators=20 max_depht=8 max_features=auto\n",
      "Random Forest estimators=20 max_depht=8 max_features=sqrt\n",
      "Random Forest estimators=20 max_depht=8 max_features=log2\n",
      "Random Forest estimators=20 max_depht=8 max_features=None\n",
      "Random Forest estimators=20 max_depht=10 max_features=auto\n",
      "Random Forest estimators=20 max_depht=10 max_features=sqrt\n",
      "Random Forest estimators=20 max_depht=10 max_features=log2\n",
      "Random Forest estimators=20 max_depht=10 max_features=None\n",
      "Random Forest estimators=40 max_depht=2 max_features=auto\n",
      "Random Forest estimators=40 max_depht=2 max_features=sqrt\n",
      "Random Forest estimators=40 max_depht=2 max_features=log2\n",
      "Random Forest estimators=40 max_depht=2 max_features=None\n",
      "Random Forest estimators=40 max_depht=4 max_features=auto\n",
      "Random Forest estimators=40 max_depht=4 max_features=sqrt\n",
      "Random Forest estimators=40 max_depht=4 max_features=log2\n",
      "Random Forest estimators=40 max_depht=4 max_features=None\n",
      "Random Forest estimators=40 max_depht=6 max_features=auto\n",
      "Random Forest estimators=40 max_depht=6 max_features=sqrt\n",
      "Random Forest estimators=40 max_depht=6 max_features=log2\n",
      "Random Forest estimators=40 max_depht=6 max_features=None\n",
      "Random Forest estimators=40 max_depht=8 max_features=auto\n",
      "Random Forest estimators=40 max_depht=8 max_features=sqrt\n",
      "Random Forest estimators=40 max_depht=8 max_features=log2\n",
      "Random Forest estimators=40 max_depht=8 max_features=None\n",
      "Random Forest estimators=40 max_depht=10 max_features=auto\n",
      "Random Forest estimators=40 max_depht=10 max_features=sqrt\n",
      "Random Forest estimators=40 max_depht=10 max_features=log2\n",
      "Random Forest estimators=40 max_depht=10 max_features=None\n",
      "Random Forest estimators=60 max_depht=2 max_features=auto\n",
      "Random Forest estimators=60 max_depht=2 max_features=sqrt\n",
      "Random Forest estimators=60 max_depht=2 max_features=log2\n",
      "Random Forest estimators=60 max_depht=2 max_features=None\n",
      "Random Forest estimators=60 max_depht=4 max_features=auto\n",
      "Random Forest estimators=60 max_depht=4 max_features=sqrt\n",
      "Random Forest estimators=60 max_depht=4 max_features=log2\n",
      "Random Forest estimators=60 max_depht=4 max_features=None\n",
      "Random Forest estimators=60 max_depht=6 max_features=auto\n",
      "Random Forest estimators=60 max_depht=6 max_features=sqrt\n",
      "Random Forest estimators=60 max_depht=6 max_features=log2\n",
      "Random Forest estimators=60 max_depht=6 max_features=None\n",
      "Random Forest estimators=60 max_depht=8 max_features=auto\n",
      "Random Forest estimators=60 max_depht=8 max_features=sqrt\n",
      "Random Forest estimators=60 max_depht=8 max_features=log2\n",
      "Random Forest estimators=60 max_depht=8 max_features=None\n",
      "Random Forest estimators=60 max_depht=10 max_features=auto\n",
      "Random Forest estimators=60 max_depht=10 max_features=sqrt\n",
      "Random Forest estimators=60 max_depht=10 max_features=log2\n",
      "Random Forest estimators=60 max_depht=10 max_features=None\n",
      "Random Forest estimators=80 max_depht=2 max_features=auto\n",
      "Random Forest estimators=80 max_depht=2 max_features=sqrt\n",
      "Random Forest estimators=80 max_depht=2 max_features=log2\n",
      "Random Forest estimators=80 max_depht=2 max_features=None\n",
      "Random Forest estimators=80 max_depht=4 max_features=auto\n",
      "Random Forest estimators=80 max_depht=4 max_features=sqrt\n",
      "Random Forest estimators=80 max_depht=4 max_features=log2\n",
      "Random Forest estimators=80 max_depht=4 max_features=None\n",
      "Random Forest estimators=80 max_depht=6 max_features=auto\n",
      "Random Forest estimators=80 max_depht=6 max_features=sqrt\n",
      "Random Forest estimators=80 max_depht=6 max_features=log2\n",
      "Random Forest estimators=80 max_depht=6 max_features=None\n",
      "Random Forest estimators=80 max_depht=8 max_features=auto\n",
      "Random Forest estimators=80 max_depht=8 max_features=sqrt\n",
      "Random Forest estimators=80 max_depht=8 max_features=log2\n",
      "Random Forest estimators=80 max_depht=8 max_features=None\n",
      "Random Forest estimators=80 max_depht=10 max_features=auto\n",
      "Random Forest estimators=80 max_depht=10 max_features=sqrt\n",
      "Random Forest estimators=80 max_depht=10 max_features=log2\n",
      "Random Forest estimators=80 max_depht=10 max_features=None\n",
      "Random Forest estimators=100 max_depht=2 max_features=auto\n",
      "Random Forest estimators=100 max_depht=2 max_features=sqrt\n",
      "Random Forest estimators=100 max_depht=2 max_features=log2\n",
      "Random Forest estimators=100 max_depht=2 max_features=None\n",
      "Random Forest estimators=100 max_depht=4 max_features=auto\n",
      "Random Forest estimators=100 max_depht=4 max_features=sqrt\n",
      "Random Forest estimators=100 max_depht=4 max_features=log2\n",
      "Random Forest estimators=100 max_depht=4 max_features=None\n",
      "Random Forest estimators=100 max_depht=6 max_features=auto\n",
      "Random Forest estimators=100 max_depht=6 max_features=sqrt\n",
      "Random Forest estimators=100 max_depht=6 max_features=log2\n",
      "Random Forest estimators=100 max_depht=6 max_features=None\n",
      "Random Forest estimators=100 max_depht=8 max_features=auto\n",
      "Random Forest estimators=100 max_depht=8 max_features=sqrt\n",
      "Random Forest estimators=100 max_depht=8 max_features=log2\n",
      "Random Forest estimators=100 max_depht=8 max_features=None\n",
      "Random Forest estimators=100 max_depht=10 max_features=auto\n",
      "Random Forest estimators=100 max_depht=10 max_features=sqrt\n",
      "Random Forest estimators=100 max_depht=10 max_features=log2\n",
      "Random Forest estimators=100 max_depht=10 max_features=None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_name = \"Random Forest\"\n",
    "for N in range(20, 120, 20):\n",
    "    for D in range(2, 12, 2):\n",
    "        model_para = \"estimators=\" + str(N) + \" max_depth=\" + str(D) + \" max_features=auto\"\n",
    "        rf_reg = RandomForestRegressor(max_features='auto' , n_estimators=N, max_depth=D, random_state=101, n_jobs=-1)\n",
    "        run_model_reg(shuffle_split_reg, model_name, model_para, rf_reg)\n",
    "        model_para = \"estimators=\" + str(N) + \" max_depth=\" + str(D) + \" max_features=sqrt\"\n",
    "        rf_reg = RandomForestRegressor(max_features='sqrt' , n_estimators=N, max_depth=D, random_state=101, n_jobs=-1)\n",
    "        run_model_reg(shuffle_split_reg, model_name, model_para, rf_reg)\n",
    "        model_para = \"estimators=\" + str(N) + \" max_depth=\" + str(D) + \" max_features=log2\"\n",
    "        rf_reg = RandomForestRegressor(max_features='log2' , n_estimators=N, max_depth=D, random_state=101, n_jobs=-1)\n",
    "        run_model_reg(shuffle_split_reg, model_name, model_para, rf_reg)\n",
    "        model_para = \"estimators=\" + str(N) + \" max_depth=\" + str(D) + \" max_features=None\"\n",
    "        rf_reg = RandomForestRegressor(max_features=None , n_estimators=N, max_depth=D, random_state=101, n_jobs=-1)\n",
    "        run_model_reg(shuffle_split_reg, model_name, model_para, rf_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Below are the preformance results for our regression model trained above sorted by our MSE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Paramaters</th>\n",
       "      <th>CV</th>\n",
       "      <th>Execution Time</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=100 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>1.1851</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=100 max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>1.1974</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=80 max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>1.0132</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=80 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>1.0129</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=60 max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.8093</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=60 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.7955</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=40 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.6191</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=40 max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.6376</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=100 max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>1.1658</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=100 max_depht=8 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>1.1886</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=60 max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.7776</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=80 max_depht=8 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=80 max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9749</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=60 max_depht=8 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.7620</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=20 max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.4353</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=20 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.4372</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=40 max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=40 max_depht=8 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.5958</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=20 max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.4280</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=20 max_depht=8 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.4278</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=100 max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>1.1081</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=80 max_depht=6 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9083</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=60 max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.7394</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=80 max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9087</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=100 max_depht=6 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>1.0907</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=60 max_depht=6 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.7434</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=40 max_depht=6 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.5555</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=40 max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.5685</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=20 max_depht=6 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.4246</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=20 max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.4334</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=80 max_depht=4 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2383</td>\n",
       "      <td>0.1635</td>\n",
       "      <td>0.0547</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=100 max_depht=6 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2434</td>\n",
       "      <td>0.1644</td>\n",
       "      <td>0.0547</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=60 max_depht=4 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2330</td>\n",
       "      <td>0.1636</td>\n",
       "      <td>0.0548</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=60 max_depht=6 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2327</td>\n",
       "      <td>0.1648</td>\n",
       "      <td>0.0551</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=100 max_depht=2 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.5605</td>\n",
       "      <td>0.1172</td>\n",
       "      <td>0.0552</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=100 max_depht=2 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.5628</td>\n",
       "      <td>0.1172</td>\n",
       "      <td>0.0552</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=60 max_depht=2 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.4354</td>\n",
       "      <td>0.1173</td>\n",
       "      <td>0.0553</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=60 max_depht=2 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.4356</td>\n",
       "      <td>0.1173</td>\n",
       "      <td>0.0553</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=80 max_depht=2 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.5080</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.0554</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=80 max_depht=2 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.4938</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.0554</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=40 max_depht=2 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.3294</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=40 max_depht=2 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.3295</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=40 max_depht=6 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2278</td>\n",
       "      <td>0.1654</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=20 max_depht=2 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.3234</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.0558</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=20 max_depht=2 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.3230</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.0558</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=20 max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2223</td>\n",
       "      <td>0.1993</td>\n",
       "      <td>0.0737</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=40 max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2278</td>\n",
       "      <td>0.2077</td>\n",
       "      <td>0.0774</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=100 max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2434</td>\n",
       "      <td>0.2086</td>\n",
       "      <td>0.0779</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=80 max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2378</td>\n",
       "      <td>0.2103</td>\n",
       "      <td>0.0785</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=60 max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2329</td>\n",
       "      <td>0.2094</td>\n",
       "      <td>0.0785</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=100 max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2432</td>\n",
       "      <td>0.2311</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=80 max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2383</td>\n",
       "      <td>0.2330</td>\n",
       "      <td>0.0927</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=20 max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2223</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.0930</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=60 max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2331</td>\n",
       "      <td>0.2334</td>\n",
       "      <td>0.0930</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=40 max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2275</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>0.0937</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=20 max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2224</td>\n",
       "      <td>0.2568</td>\n",
       "      <td>0.1079</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=100 max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2432</td>\n",
       "      <td>0.2627</td>\n",
       "      <td>0.1119</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=40 max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>0.2631</td>\n",
       "      <td>0.1120</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=80 max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2379</td>\n",
       "      <td>0.2646</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=60 max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2327</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>0.1141</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Type     Model Name  \\\n",
       "1539  regression  Random Forest   \n",
       "1506  regression  Random Forest   \n",
       "1286  regression  Random Forest   \n",
       "1319  regression  Random Forest   \n",
       "1066  regression  Random Forest   \n",
       "1099  regression  Random Forest   \n",
       "879   regression  Random Forest   \n",
       "846   regression  Random Forest   \n",
       "1495  regression  Random Forest   \n",
       "1462  regression  Random Forest   \n",
       "1055  regression  Random Forest   \n",
       "1242  regression  Random Forest   \n",
       "1275  regression  Random Forest   \n",
       "1022  regression  Random Forest   \n",
       "626   regression  Random Forest   \n",
       "659   regression  Random Forest   \n",
       "835   regression  Random Forest   \n",
       "802   regression  Random Forest   \n",
       "615   regression  Random Forest   \n",
       "582   regression  Random Forest   \n",
       "1451  regression  Random Forest   \n",
       "1198  regression  Random Forest   \n",
       "1011  regression  Random Forest   \n",
       "1231  regression  Random Forest   \n",
       "1418  regression  Random Forest   \n",
       "978   regression  Random Forest   \n",
       "758   regression  Random Forest   \n",
       "791   regression  Random Forest   \n",
       "538   regression  Random Forest   \n",
       "571   regression  Random Forest   \n",
       "...          ...            ...   \n",
       "1165  regression  Random Forest   \n",
       "1440  regression  Random Forest   \n",
       "945   regression  Random Forest   \n",
       "1000  regression  Random Forest   \n",
       "1363  regression  Random Forest   \n",
       "1330  regression  Random Forest   \n",
       "890   regression  Random Forest   \n",
       "923   regression  Random Forest   \n",
       "1143  regression  Random Forest   \n",
       "1110  regression  Random Forest   \n",
       "670   regression  Random Forest   \n",
       "703   regression  Random Forest   \n",
       "780   regression  Random Forest   \n",
       "450   regression  Random Forest   \n",
       "483   regression  Random Forest   \n",
       "516   regression  Random Forest   \n",
       "736   regression  Random Forest   \n",
       "1396  regression  Random Forest   \n",
       "1176  regression  Random Forest   \n",
       "956   regression  Random Forest   \n",
       "1341  regression  Random Forest   \n",
       "1121  regression  Random Forest   \n",
       "461   regression  Random Forest   \n",
       "901   regression  Random Forest   \n",
       "681   regression  Random Forest   \n",
       "472   regression  Random Forest   \n",
       "1352  regression  Random Forest   \n",
       "692   regression  Random Forest   \n",
       "1132  regression  Random Forest   \n",
       "912   regression  Random Forest   \n",
       "\n",
       "                                         Paramaters   CV  Execution Time  \\\n",
       "1539  estimators=100 max_depht=10 max_features=None  avg          1.1851   \n",
       "1506  estimators=100 max_depht=10 max_features=auto  avg          1.1974   \n",
       "1286   estimators=80 max_depht=10 max_features=auto  avg          1.0132   \n",
       "1319   estimators=80 max_depht=10 max_features=None  avg          1.0129   \n",
       "1066   estimators=60 max_depht=10 max_features=auto  avg          0.8093   \n",
       "1099   estimators=60 max_depht=10 max_features=None  avg          0.7955   \n",
       "879    estimators=40 max_depht=10 max_features=None  avg          0.6191   \n",
       "846    estimators=40 max_depht=10 max_features=auto  avg          0.6376   \n",
       "1495   estimators=100 max_depht=8 max_features=None  avg          1.1658   \n",
       "1462   estimators=100 max_depht=8 max_features=auto  avg          1.1886   \n",
       "1055    estimators=60 max_depht=8 max_features=None  avg          0.7776   \n",
       "1242    estimators=80 max_depht=8 max_features=auto  avg          0.9782   \n",
       "1275    estimators=80 max_depht=8 max_features=None  avg          0.9749   \n",
       "1022    estimators=60 max_depht=8 max_features=auto  avg          0.7620   \n",
       "626    estimators=20 max_depht=10 max_features=auto  avg          0.4353   \n",
       "659    estimators=20 max_depht=10 max_features=None  avg          0.4372   \n",
       "835     estimators=40 max_depht=8 max_features=None  avg          0.5875   \n",
       "802     estimators=40 max_depht=8 max_features=auto  avg          0.5958   \n",
       "615     estimators=20 max_depht=8 max_features=None  avg          0.4280   \n",
       "582     estimators=20 max_depht=8 max_features=auto  avg          0.4278   \n",
       "1451   estimators=100 max_depht=6 max_features=None  avg          1.1081   \n",
       "1198    estimators=80 max_depht=6 max_features=auto  avg          0.9083   \n",
       "1011    estimators=60 max_depht=6 max_features=None  avg          0.7394   \n",
       "1231    estimators=80 max_depht=6 max_features=None  avg          0.9087   \n",
       "1418   estimators=100 max_depht=6 max_features=auto  avg          1.0907   \n",
       "978     estimators=60 max_depht=6 max_features=auto  avg          0.7434   \n",
       "758     estimators=40 max_depht=6 max_features=auto  avg          0.5555   \n",
       "791     estimators=40 max_depht=6 max_features=None  avg          0.5685   \n",
       "538     estimators=20 max_depht=6 max_features=auto  avg          0.4246   \n",
       "571     estimators=20 max_depht=6 max_features=None  avg          0.4334   \n",
       "...                                             ...  ...             ...   \n",
       "1165    estimators=80 max_depht=4 max_features=sqrt  avg          0.2383   \n",
       "1440   estimators=100 max_depht=6 max_features=log2  avg          0.2434   \n",
       "945     estimators=60 max_depht=4 max_features=sqrt  avg          0.2330   \n",
       "1000    estimators=60 max_depht=6 max_features=log2  avg          0.2327   \n",
       "1363   estimators=100 max_depht=2 max_features=None  avg          0.5605   \n",
       "1330   estimators=100 max_depht=2 max_features=auto  avg          0.5628   \n",
       "890     estimators=60 max_depht=2 max_features=auto  avg          0.4354   \n",
       "923     estimators=60 max_depht=2 max_features=None  avg          0.4356   \n",
       "1143    estimators=80 max_depht=2 max_features=None  avg          0.5080   \n",
       "1110    estimators=80 max_depht=2 max_features=auto  avg          0.4938   \n",
       "670     estimators=40 max_depht=2 max_features=auto  avg          0.3294   \n",
       "703     estimators=40 max_depht=2 max_features=None  avg          0.3295   \n",
       "780     estimators=40 max_depht=6 max_features=log2  avg          0.2278   \n",
       "450     estimators=20 max_depht=2 max_features=auto  avg          0.3234   \n",
       "483     estimators=20 max_depht=2 max_features=None  avg          0.3230   \n",
       "516     estimators=20 max_depht=4 max_features=log2  avg          0.2223   \n",
       "736     estimators=40 max_depht=4 max_features=log2  avg          0.2278   \n",
       "1396   estimators=100 max_depht=4 max_features=log2  avg          0.2434   \n",
       "1176    estimators=80 max_depht=4 max_features=log2  avg          0.2378   \n",
       "956     estimators=60 max_depht=4 max_features=log2  avg          0.2329   \n",
       "1341   estimators=100 max_depht=2 max_features=sqrt  avg          0.2432   \n",
       "1121    estimators=80 max_depht=2 max_features=sqrt  avg          0.2383   \n",
       "461     estimators=20 max_depht=2 max_features=sqrt  avg          0.2223   \n",
       "901     estimators=60 max_depht=2 max_features=sqrt  avg          0.2331   \n",
       "681     estimators=40 max_depht=2 max_features=sqrt  avg          0.2275   \n",
       "472     estimators=20 max_depht=2 max_features=log2  avg          0.2224   \n",
       "1352   estimators=100 max_depht=2 max_features=log2  avg          0.2432   \n",
       "692     estimators=40 max_depht=2 max_features=log2  avg          0.2277   \n",
       "1132    estimators=80 max_depht=2 max_features=log2  avg          0.2379   \n",
       "912     estimators=60 max_depht=2 max_features=log2  avg          0.2327   \n",
       "\n",
       "         MAE     MSE Model  \n",
       "1539  0.0122  0.0035  None  \n",
       "1506  0.0122  0.0035  None  \n",
       "1286  0.0123  0.0035  None  \n",
       "1319  0.0123  0.0035  None  \n",
       "1066  0.0123  0.0036  None  \n",
       "1099  0.0123  0.0036  None  \n",
       "879   0.0124  0.0037  None  \n",
       "846   0.0124  0.0037  None  \n",
       "1495  0.0135  0.0039  None  \n",
       "1462  0.0135  0.0039  None  \n",
       "1055  0.0136  0.0040  None  \n",
       "1242  0.0136  0.0040  None  \n",
       "1275  0.0136  0.0040  None  \n",
       "1022  0.0136  0.0040  None  \n",
       "626   0.0125  0.0041  None  \n",
       "659   0.0125  0.0041  None  \n",
       "835   0.0137  0.0042  None  \n",
       "802   0.0137  0.0042  None  \n",
       "615   0.0138  0.0045  None  \n",
       "582   0.0138  0.0045  None  \n",
       "1451  0.0221  0.0066  None  \n",
       "1198  0.0221  0.0066  None  \n",
       "1011  0.0220  0.0066  None  \n",
       "1231  0.0221  0.0066  None  \n",
       "1418  0.0221  0.0066  None  \n",
       "978   0.0220  0.0066  None  \n",
       "758   0.0222  0.0068  None  \n",
       "791   0.0222  0.0068  None  \n",
       "538   0.0220  0.0071  None  \n",
       "571   0.0220  0.0071  None  \n",
       "...      ...     ...   ...  \n",
       "1165  0.1635  0.0547  None  \n",
       "1440  0.1644  0.0547  None  \n",
       "945   0.1636  0.0548  None  \n",
       "1000  0.1648  0.0551  None  \n",
       "1363  0.1172  0.0552  None  \n",
       "1330  0.1172  0.0552  None  \n",
       "890   0.1173  0.0553  None  \n",
       "923   0.1173  0.0553  None  \n",
       "1143  0.1171  0.0554  None  \n",
       "1110  0.1171  0.0554  None  \n",
       "670   0.1171  0.0555  None  \n",
       "703   0.1171  0.0555  None  \n",
       "780   0.1654  0.0556  None  \n",
       "450   0.1171  0.0558  None  \n",
       "483   0.1171  0.0558  None  \n",
       "516   0.1993  0.0737  None  \n",
       "736   0.2077  0.0774  None  \n",
       "1396  0.2086  0.0779  None  \n",
       "1176  0.2103  0.0785  None  \n",
       "956   0.2094  0.0785  None  \n",
       "1341  0.2311  0.0909  None  \n",
       "1121  0.2330  0.0927  None  \n",
       "461   0.2341  0.0930  None  \n",
       "901   0.2334  0.0930  None  \n",
       "681   0.2354  0.0937  None  \n",
       "472   0.2568  0.1079  None  \n",
       "1352  0.2627  0.1119  None  \n",
       "692   0.2631  0.1120  None  \n",
       "1132  0.2646  0.1131  None  \n",
       "912   0.2655  0.1141  None  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stats.loc[(reg_stats['Model Name'] == 'Random Forest') & (reg_stats['CV'] == 'avg')].sort_values(by=['MSE'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We return to Random Forest this time for a regression model to compare not only how it performs against a simple decision tree, but also to compare how parameters might effect the regressor differently than the classifier.  Straight away we saw better MSE scores compared to our simple decision tree with values all the way down to 0.0035. However, this comes at the cost of about 10x execution time. In our classifier we found the \"max_features\" had more importance and \"estimators\" count to be a big contributer, but here it appears that \"max_depth\" is the heaviest influencer of success. Even seeing the esimators drop from 100 to 20 only decreases the MSE score from 0.0035 to 0.0041 when \"max_depth\" is kept at 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree estimators=20 max_depht=2 max_features=auto\n",
      "Extra Tree estimators=20 max_depht=2 max_features=sqrt\n",
      "Extra Tree estimators=20 max_depht=2 max_features=log2\n",
      "Extra Tree estimators=20 max_depht=2 max_features=None\n",
      "Extra Tree estimators=20 max_depht=4 max_features=auto\n",
      "Extra Tree estimators=20 max_depht=4 max_features=sqrt\n",
      "Extra Tree estimators=20 max_depht=4 max_features=log2\n",
      "Extra Tree estimators=20 max_depht=4 max_features=None\n",
      "Extra Tree estimators=20 max_depht=6 max_features=auto\n",
      "Extra Tree estimators=20 max_depht=6 max_features=sqrt\n",
      "Extra Tree estimators=20 max_depht=6 max_features=log2\n",
      "Extra Tree estimators=20 max_depht=6 max_features=None\n",
      "Extra Tree estimators=20 max_depht=8 max_features=auto\n",
      "Extra Tree estimators=20 max_depht=8 max_features=sqrt\n",
      "Extra Tree estimators=20 max_depht=8 max_features=log2\n",
      "Extra Tree estimators=20 max_depht=8 max_features=None\n",
      "Extra Tree estimators=20 max_depht=10 max_features=auto\n",
      "Extra Tree estimators=20 max_depht=10 max_features=sqrt\n",
      "Extra Tree estimators=20 max_depht=10 max_features=log2\n",
      "Extra Tree estimators=20 max_depht=10 max_features=None\n",
      "Extra Tree estimators=40 max_depht=2 max_features=auto\n",
      "Extra Tree estimators=40 max_depht=2 max_features=sqrt\n",
      "Extra Tree estimators=40 max_depht=2 max_features=log2\n",
      "Extra Tree estimators=40 max_depht=2 max_features=None\n",
      "Extra Tree estimators=40 max_depht=4 max_features=auto\n",
      "Extra Tree estimators=40 max_depht=4 max_features=sqrt\n",
      "Extra Tree estimators=40 max_depht=4 max_features=log2\n",
      "Extra Tree estimators=40 max_depht=4 max_features=None\n",
      "Extra Tree estimators=40 max_depht=6 max_features=auto\n",
      "Extra Tree estimators=40 max_depht=6 max_features=sqrt\n",
      "Extra Tree estimators=40 max_depht=6 max_features=log2\n",
      "Extra Tree estimators=40 max_depht=6 max_features=None\n",
      "Extra Tree estimators=40 max_depht=8 max_features=auto\n",
      "Extra Tree estimators=40 max_depht=8 max_features=sqrt\n",
      "Extra Tree estimators=40 max_depht=8 max_features=log2\n",
      "Extra Tree estimators=40 max_depht=8 max_features=None\n",
      "Extra Tree estimators=40 max_depht=10 max_features=auto\n",
      "Extra Tree estimators=40 max_depht=10 max_features=sqrt\n",
      "Extra Tree estimators=40 max_depht=10 max_features=log2\n",
      "Extra Tree estimators=40 max_depht=10 max_features=None\n",
      "Extra Tree estimators=60 max_depht=2 max_features=auto\n",
      "Extra Tree estimators=60 max_depht=2 max_features=sqrt\n",
      "Extra Tree estimators=60 max_depht=2 max_features=log2\n",
      "Extra Tree estimators=60 max_depht=2 max_features=None\n",
      "Extra Tree estimators=60 max_depht=4 max_features=auto\n",
      "Extra Tree estimators=60 max_depht=4 max_features=sqrt\n",
      "Extra Tree estimators=60 max_depht=4 max_features=log2\n",
      "Extra Tree estimators=60 max_depht=4 max_features=None\n",
      "Extra Tree estimators=60 max_depht=6 max_features=auto\n",
      "Extra Tree estimators=60 max_depht=6 max_features=sqrt\n",
      "Extra Tree estimators=60 max_depht=6 max_features=log2\n",
      "Extra Tree estimators=60 max_depht=6 max_features=None\n",
      "Extra Tree estimators=60 max_depht=8 max_features=auto\n",
      "Extra Tree estimators=60 max_depht=8 max_features=sqrt\n",
      "Extra Tree estimators=60 max_depht=8 max_features=log2\n",
      "Extra Tree estimators=60 max_depht=8 max_features=None\n",
      "Extra Tree estimators=60 max_depht=10 max_features=auto\n",
      "Extra Tree estimators=60 max_depht=10 max_features=sqrt\n",
      "Extra Tree estimators=60 max_depht=10 max_features=log2\n",
      "Extra Tree estimators=60 max_depht=10 max_features=None\n",
      "Extra Tree estimators=80 max_depht=2 max_features=auto\n",
      "Extra Tree estimators=80 max_depht=2 max_features=sqrt\n",
      "Extra Tree estimators=80 max_depht=2 max_features=log2\n",
      "Extra Tree estimators=80 max_depht=2 max_features=None\n",
      "Extra Tree estimators=80 max_depht=4 max_features=auto\n",
      "Extra Tree estimators=80 max_depht=4 max_features=sqrt\n",
      "Extra Tree estimators=80 max_depht=4 max_features=log2\n",
      "Extra Tree estimators=80 max_depht=4 max_features=None\n",
      "Extra Tree estimators=80 max_depht=6 max_features=auto\n",
      "Extra Tree estimators=80 max_depht=6 max_features=sqrt\n",
      "Extra Tree estimators=80 max_depht=6 max_features=log2\n",
      "Extra Tree estimators=80 max_depht=6 max_features=None\n",
      "Extra Tree estimators=80 max_depht=8 max_features=auto\n",
      "Extra Tree estimators=80 max_depht=8 max_features=sqrt\n",
      "Extra Tree estimators=80 max_depht=8 max_features=log2\n",
      "Extra Tree estimators=80 max_depht=8 max_features=None\n",
      "Extra Tree estimators=80 max_depht=10 max_features=auto\n",
      "Extra Tree estimators=80 max_depht=10 max_features=sqrt\n",
      "Extra Tree estimators=80 max_depht=10 max_features=log2\n",
      "Extra Tree estimators=80 max_depht=10 max_features=None\n",
      "Extra Tree estimators=100 max_depht=2 max_features=auto\n",
      "Extra Tree estimators=100 max_depht=2 max_features=sqrt\n",
      "Extra Tree estimators=100 max_depht=2 max_features=log2\n",
      "Extra Tree estimators=100 max_depht=2 max_features=None\n",
      "Extra Tree estimators=100 max_depht=4 max_features=auto\n",
      "Extra Tree estimators=100 max_depht=4 max_features=sqrt\n",
      "Extra Tree estimators=100 max_depht=4 max_features=log2\n",
      "Extra Tree estimators=100 max_depht=4 max_features=None\n",
      "Extra Tree estimators=100 max_depht=6 max_features=auto\n",
      "Extra Tree estimators=100 max_depht=6 max_features=sqrt\n",
      "Extra Tree estimators=100 max_depht=6 max_features=log2\n",
      "Extra Tree estimators=100 max_depht=6 max_features=None\n",
      "Extra Tree estimators=100 max_depht=8 max_features=auto\n",
      "Extra Tree estimators=100 max_depht=8 max_features=sqrt\n",
      "Extra Tree estimators=100 max_depht=8 max_features=log2\n",
      "Extra Tree estimators=100 max_depht=8 max_features=None\n",
      "Extra Tree estimators=100 max_depht=10 max_features=auto\n",
      "Extra Tree estimators=100 max_depht=10 max_features=sqrt\n",
      "Extra Tree estimators=100 max_depht=10 max_features=log2\n",
      "Extra Tree estimators=100 max_depht=10 max_features=None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "model_name = \"Extra Tree\"\n",
    "for N in range(20, 120, 20):\n",
    "    for D in range(2, 12, 2):\n",
    "        model_para = \"estimators=\" + str(N) + \" max_depth=\" + str(D) + \" max_features=auto\"\n",
    "        et_reg = ExtraTreesRegressor(max_features='auto' , n_estimators=N, max_depth=D, random_state=101, n_jobs=-1)\n",
    "        run_model_reg(shuffle_split_reg, model_name, model_para, et_reg)\n",
    "        model_para = \"estimators=\" + str(N) + \" max_depth=\" + str(D) + \" max_features=sqrt\"\n",
    "        et_reg = ExtraTreesRegressor(max_features='sqrt' , n_estimators=N, max_depth=D, random_state=101, n_jobs=-1)\n",
    "        run_model_reg(shuffle_split_reg, model_name, model_para, et_reg)\n",
    "        model_para = \"estimators=\" + str(N) + \" max_depth=\" + str(D) + \" max_features=log2\"\n",
    "        et_reg = ExtraTreesRegressor(max_features='log2' , n_estimators=N, max_depth=D, random_state=101, n_jobs=-1)\n",
    "        run_model_reg(shuffle_split_reg, model_name, model_para, et_reg)\n",
    "        model_para = \"estimators=\" + str(N) + \" max_depth=\" + str(D) + \" max_features=None\"\n",
    "        et_reg = ExtraTreesRegressor(max_features=None , n_estimators=N, max_depth=D, random_state=101, n_jobs=-1)\n",
    "        run_model_reg(shuffle_split_reg, model_name, model_para, et_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Below are the preformance results for our regression model trained above sorted by our MSE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Paramaters</th>\n",
       "      <th>CV</th>\n",
       "      <th>Execution Time</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=100 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.6636</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=80 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.5830</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=40 max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.3340</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=40 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.3328</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=60 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=80 max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.5767</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2606</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=100 max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.6671</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2166</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=60 max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.4679</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=20 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2944</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=20 max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.3144</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=60 max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.4581</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=60 max_depht=8 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.4513</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=100 max_depht=8 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.6705</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=80 max_depht=8 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=40 max_depht=8 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.3318</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=80 max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.5589</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=40 max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.3331</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=100 max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.6241</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=20 max_depht=8 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2338</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=20 max_depht=8 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2436</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=80 max_depht=6 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.4704</td>\n",
       "      <td>0.0419</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=80 max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.4881</td>\n",
       "      <td>0.0419</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=60 max_depht=6 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.4526</td>\n",
       "      <td>0.0421</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2111</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=60 max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.4425</td>\n",
       "      <td>0.0421</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2551</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=100 max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.5907</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=100 max_depht=6 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.5921</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=40 max_depht=6 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.3323</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=40 max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.3338</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=20 max_depht=6 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2238</td>\n",
       "      <td>0.0429</td>\n",
       "      <td>0.0112</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=20 max_depht=6 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2237</td>\n",
       "      <td>0.0429</td>\n",
       "      <td>0.0112</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2584</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=100 max_depht=8 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2437</td>\n",
       "      <td>0.2385</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=60 max_depht=8 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2338</td>\n",
       "      <td>0.2391</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2364</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=80 max_depht=8 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2384</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>0.0967</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=40 max_depht=8 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2288</td>\n",
       "      <td>0.2406</td>\n",
       "      <td>0.0976</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=20 max_depht=8 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2230</td>\n",
       "      <td>0.2434</td>\n",
       "      <td>0.1008</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=40 max_depht=4 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.2531</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=60 max_depht=4 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2343</td>\n",
       "      <td>0.2546</td>\n",
       "      <td>0.1059</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=100 max_depht=4 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2432</td>\n",
       "      <td>0.2557</td>\n",
       "      <td>0.1065</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=20 max_depht=4 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2236</td>\n",
       "      <td>0.2547</td>\n",
       "      <td>0.1065</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=80 max_depht=4 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2396</td>\n",
       "      <td>0.2574</td>\n",
       "      <td>0.1079</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=100 max_depht=6 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2460</td>\n",
       "      <td>0.2609</td>\n",
       "      <td>0.1108</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=60 max_depht=6 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.2607</td>\n",
       "      <td>0.1109</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=80 max_depht=6 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2388</td>\n",
       "      <td>0.2616</td>\n",
       "      <td>0.1114</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=40 max_depht=6 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2283</td>\n",
       "      <td>0.2621</td>\n",
       "      <td>0.1120</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=20 max_depht=6 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2233</td>\n",
       "      <td>0.2645</td>\n",
       "      <td>0.1144</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=100 max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2435</td>\n",
       "      <td>0.2826</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=80 max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2386</td>\n",
       "      <td>0.2834</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2056</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=60 max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2346</td>\n",
       "      <td>0.2835</td>\n",
       "      <td>0.1278</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1836</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=40 max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>0.2848</td>\n",
       "      <td>0.1288</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=20 max_depht=4 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2243</td>\n",
       "      <td>0.2866</td>\n",
       "      <td>0.1309</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=40 max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2293</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>0.1309</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=20 max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2246</td>\n",
       "      <td>0.2903</td>\n",
       "      <td>0.1326</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=100 max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2440</td>\n",
       "      <td>0.2908</td>\n",
       "      <td>0.1328</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=60 max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2342</td>\n",
       "      <td>0.2909</td>\n",
       "      <td>0.1330</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=80 max_depht=2 max_features=sqrt</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2387</td>\n",
       "      <td>0.2922</td>\n",
       "      <td>0.1341</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=100 max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2442</td>\n",
       "      <td>0.3028</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=80 max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.3038</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=60 max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2345</td>\n",
       "      <td>0.3051</td>\n",
       "      <td>0.1453</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=40 max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2294</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>0.1456</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=20 max_depht=2 max_features=log2</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2233</td>\n",
       "      <td>0.3077</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Type  Model Name                                     Paramaters  \\\n",
       "2639  regression  Extra Tree  estimators=100 max_depht=10 max_features=None   \n",
       "2419  regression  Extra Tree   estimators=80 max_depht=10 max_features=None   \n",
       "1946  regression  Extra Tree   estimators=40 max_depht=10 max_features=auto   \n",
       "1979  regression  Extra Tree   estimators=40 max_depht=10 max_features=None   \n",
       "2199  regression  Extra Tree   estimators=60 max_depht=10 max_features=None   \n",
       "2386  regression  Extra Tree   estimators=80 max_depht=10 max_features=auto   \n",
       "2606  regression  Extra Tree  estimators=100 max_depht=10 max_features=auto   \n",
       "2166  regression  Extra Tree   estimators=60 max_depht=10 max_features=auto   \n",
       "1759  regression  Extra Tree   estimators=20 max_depht=10 max_features=None   \n",
       "1726  regression  Extra Tree   estimators=20 max_depht=10 max_features=auto   \n",
       "2155  regression  Extra Tree    estimators=60 max_depht=8 max_features=None   \n",
       "2122  regression  Extra Tree    estimators=60 max_depht=8 max_features=auto   \n",
       "2562  regression  Extra Tree   estimators=100 max_depht=8 max_features=auto   \n",
       "2342  regression  Extra Tree    estimators=80 max_depht=8 max_features=auto   \n",
       "1902  regression  Extra Tree    estimators=40 max_depht=8 max_features=auto   \n",
       "2375  regression  Extra Tree    estimators=80 max_depht=8 max_features=None   \n",
       "1935  regression  Extra Tree    estimators=40 max_depht=8 max_features=None   \n",
       "2595  regression  Extra Tree   estimators=100 max_depht=8 max_features=None   \n",
       "1715  regression  Extra Tree    estimators=20 max_depht=8 max_features=None   \n",
       "1682  regression  Extra Tree    estimators=20 max_depht=8 max_features=auto   \n",
       "2298  regression  Extra Tree    estimators=80 max_depht=6 max_features=auto   \n",
       "2331  regression  Extra Tree    estimators=80 max_depht=6 max_features=None   \n",
       "2078  regression  Extra Tree    estimators=60 max_depht=6 max_features=auto   \n",
       "2111  regression  Extra Tree    estimators=60 max_depht=6 max_features=None   \n",
       "2551  regression  Extra Tree   estimators=100 max_depht=6 max_features=None   \n",
       "2518  regression  Extra Tree   estimators=100 max_depht=6 max_features=auto   \n",
       "1858  regression  Extra Tree    estimators=40 max_depht=6 max_features=auto   \n",
       "1891  regression  Extra Tree    estimators=40 max_depht=6 max_features=None   \n",
       "1638  regression  Extra Tree    estimators=20 max_depht=6 max_features=auto   \n",
       "1671  regression  Extra Tree    estimators=20 max_depht=6 max_features=None   \n",
       "...          ...         ...                                            ...   \n",
       "2584  regression  Extra Tree   estimators=100 max_depht=8 max_features=log2   \n",
       "2144  regression  Extra Tree    estimators=60 max_depht=8 max_features=log2   \n",
       "2364  regression  Extra Tree    estimators=80 max_depht=8 max_features=log2   \n",
       "1924  regression  Extra Tree    estimators=40 max_depht=8 max_features=log2   \n",
       "1704  regression  Extra Tree    estimators=20 max_depht=8 max_features=log2   \n",
       "1825  regression  Extra Tree    estimators=40 max_depht=4 max_features=sqrt   \n",
       "2045  regression  Extra Tree    estimators=60 max_depht=4 max_features=sqrt   \n",
       "2485  regression  Extra Tree   estimators=100 max_depht=4 max_features=sqrt   \n",
       "1605  regression  Extra Tree    estimators=20 max_depht=4 max_features=sqrt   \n",
       "2265  regression  Extra Tree    estimators=80 max_depht=4 max_features=sqrt   \n",
       "2540  regression  Extra Tree   estimators=100 max_depht=6 max_features=log2   \n",
       "2100  regression  Extra Tree    estimators=60 max_depht=6 max_features=log2   \n",
       "2320  regression  Extra Tree    estimators=80 max_depht=6 max_features=log2   \n",
       "1880  regression  Extra Tree    estimators=40 max_depht=6 max_features=log2   \n",
       "1660  regression  Extra Tree    estimators=20 max_depht=6 max_features=log2   \n",
       "2496  regression  Extra Tree   estimators=100 max_depht=4 max_features=log2   \n",
       "2276  regression  Extra Tree    estimators=80 max_depht=4 max_features=log2   \n",
       "2056  regression  Extra Tree    estimators=60 max_depht=4 max_features=log2   \n",
       "1836  regression  Extra Tree    estimators=40 max_depht=4 max_features=log2   \n",
       "1616  regression  Extra Tree    estimators=20 max_depht=4 max_features=log2   \n",
       "1781  regression  Extra Tree    estimators=40 max_depht=2 max_features=sqrt   \n",
       "1561  regression  Extra Tree    estimators=20 max_depht=2 max_features=sqrt   \n",
       "2441  regression  Extra Tree   estimators=100 max_depht=2 max_features=sqrt   \n",
       "2001  regression  Extra Tree    estimators=60 max_depht=2 max_features=sqrt   \n",
       "2221  regression  Extra Tree    estimators=80 max_depht=2 max_features=sqrt   \n",
       "2452  regression  Extra Tree   estimators=100 max_depht=2 max_features=log2   \n",
       "2232  regression  Extra Tree    estimators=80 max_depht=2 max_features=log2   \n",
       "2012  regression  Extra Tree    estimators=60 max_depht=2 max_features=log2   \n",
       "1792  regression  Extra Tree    estimators=40 max_depht=2 max_features=log2   \n",
       "1572  regression  Extra Tree    estimators=20 max_depht=2 max_features=log2   \n",
       "\n",
       "       CV  Execution Time     MAE     MSE Model  \n",
       "2639  avg          0.6636  0.0128  0.0024  None  \n",
       "2419  avg          0.5830  0.0127  0.0024  None  \n",
       "1946  avg          0.3340  0.0126  0.0024  None  \n",
       "1979  avg          0.3328  0.0126  0.0024  None  \n",
       "2199  avg          0.4430  0.0127  0.0024  None  \n",
       "2386  avg          0.5767  0.0127  0.0024  None  \n",
       "2606  avg          0.6671  0.0128  0.0024  None  \n",
       "2166  avg          0.4679  0.0127  0.0024  None  \n",
       "1759  avg          0.2944  0.0126  0.0026  None  \n",
       "1726  avg          0.3144  0.0126  0.0026  None  \n",
       "2155  avg          0.4581  0.0199  0.0039  None  \n",
       "2122  avg          0.4513  0.0199  0.0039  None  \n",
       "2562  avg          0.6705  0.0203  0.0040  None  \n",
       "2342  avg          0.5423  0.0201  0.0040  None  \n",
       "1902  avg          0.3318  0.0203  0.0040  None  \n",
       "2375  avg          0.5589  0.0201  0.0040  None  \n",
       "1935  avg          0.3331  0.0203  0.0040  None  \n",
       "2595  avg          0.6241  0.0203  0.0040  None  \n",
       "1715  avg          0.2338  0.0200  0.0042  None  \n",
       "1682  avg          0.2436  0.0200  0.0042  None  \n",
       "2298  avg          0.4704  0.0419  0.0104  None  \n",
       "2331  avg          0.4881  0.0419  0.0104  None  \n",
       "2078  avg          0.4526  0.0421  0.0105  None  \n",
       "2111  avg          0.4425  0.0421  0.0105  None  \n",
       "2551  avg          0.5907  0.0420  0.0105  None  \n",
       "2518  avg          0.5921  0.0420  0.0105  None  \n",
       "1858  avg          0.3323  0.0422  0.0106  None  \n",
       "1891  avg          0.3338  0.0422  0.0106  None  \n",
       "1638  avg          0.2238  0.0429  0.0112  None  \n",
       "1671  avg          0.2237  0.0429  0.0112  None  \n",
       "...   ...             ...     ...     ...   ...  \n",
       "2584  avg          0.2437  0.2385  0.0958  None  \n",
       "2144  avg          0.2338  0.2391  0.0964  None  \n",
       "2364  avg          0.2384  0.2395  0.0967  None  \n",
       "1924  avg          0.2288  0.2406  0.0976  None  \n",
       "1704  avg          0.2230  0.2434  0.1008  None  \n",
       "1825  avg          0.2296  0.2531  0.1050  None  \n",
       "2045  avg          0.2343  0.2546  0.1059  None  \n",
       "2485  avg          0.2432  0.2557  0.1065  None  \n",
       "1605  avg          0.2236  0.2547  0.1065  None  \n",
       "2265  avg          0.2396  0.2574  0.1079  None  \n",
       "2540  avg          0.2460  0.2609  0.1108  None  \n",
       "2100  avg          0.2350  0.2607  0.1109  None  \n",
       "2320  avg          0.2388  0.2616  0.1114  None  \n",
       "1880  avg          0.2283  0.2621  0.1120  None  \n",
       "1660  avg          0.2233  0.2645  0.1144  None  \n",
       "2496  avg          0.2435  0.2826  0.1269  None  \n",
       "2276  avg          0.2386  0.2834  0.1276  None  \n",
       "2056  avg          0.2346  0.2835  0.1278  None  \n",
       "1836  avg          0.2277  0.2848  0.1288  None  \n",
       "1616  avg          0.2243  0.2866  0.1309  None  \n",
       "1781  avg          0.2293  0.2885  0.1309  None  \n",
       "1561  avg          0.2246  0.2903  0.1326  None  \n",
       "2441  avg          0.2440  0.2908  0.1328  None  \n",
       "2001  avg          0.2342  0.2909  0.1330  None  \n",
       "2221  avg          0.2387  0.2922  0.1341  None  \n",
       "2452  avg          0.2442  0.3028  0.1430  None  \n",
       "2232  avg          0.2419  0.3038  0.1440  None  \n",
       "2012  avg          0.2345  0.3051  0.1453  None  \n",
       "1792  avg          0.2294  0.3055  0.1456  None  \n",
       "1572  avg          0.2233  0.3077  0.1480  None  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stats.loc[(reg_stats['Model Name'] == 'Extra Tree') & (reg_stats['CV'] == 'avg')].sort_values(by=['MSE'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For our Extra Tree models we again saw \"max_depth\" return as king of performance on our MSE score of  0.0024.   However, it is noteworthy that we saw this score stay the same from 100,80,60,40 estimators and only dropped slightly when estimators moved to 20.  This lead us to prefer the lower estimator since the execution time was about half or lower.   As noted with other tree algorithms, it prefers when \"max_features\" are not limited, but sometimes a lower parameter model may be prefered when the goal is to make the model more generalized or when collecting all of the features may be more difficult because of time or expense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the results using your chosen method of evaluation. Use\n",
    "visualizations of the results to bolster the analysis. Explain any visuals and analyze why\n",
    "they are interesting to someone that might use this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Paramaters</th>\n",
       "      <th>CV</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Execution Time</th>\n",
       "      <th>True Positive</th>\n",
       "      <th>False Positive</th>\n",
       "      <th>False Negative</th>\n",
       "      <th>True Negative</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=1 algorithm=brute</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>1.2569</td>\n",
       "      <td>3881.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>982.4</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=1 algorithm=kd_tree</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>3881.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>982.4</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=1 algorithm=ball_tree</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.1456</td>\n",
       "      <td>3881.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>982.4</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=100 max_depht=4 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>5.4914</td>\n",
       "      <td>3881.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>979.8</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=100 max_depht=4 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>5.5422</td>\n",
       "      <td>3881.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>979.8</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=80 max_depht=4 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>4.4829</td>\n",
       "      <td>3881.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>979.2</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=80 max_depht=4 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>4.4239</td>\n",
       "      <td>3881.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>979.2</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=2 algorithm=brute</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>0.9589</td>\n",
       "      <td>3881.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>978.5</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=2 algorithm=kd_tree</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>0.1469</td>\n",
       "      <td>3881.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>978.5</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=2 algorithm=ball_tree</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>0.1433</td>\n",
       "      <td>3881.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>978.5</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Type         Model Name  \\\n",
       "472   classification                KNN   \n",
       "461   classification                KNN   \n",
       "450   classification                KNN   \n",
       "2672  classification  Gradient Boosting   \n",
       "2639  classification  Gradient Boosting   \n",
       "2419  classification  Gradient Boosting   \n",
       "2452  classification  Gradient Boosting   \n",
       "505   classification                KNN   \n",
       "494   classification                KNN   \n",
       "483   classification                KNN   \n",
       "\n",
       "                                        Paramaters   CV  Accuracy  \\\n",
       "472                    Neighbors=1 algorithm=brute  avg    0.9995   \n",
       "461                  Neighbors=1 algorithm=kd_tree  avg    0.9995   \n",
       "450                Neighbors=1 algorithm=ball_tree  avg    0.9995   \n",
       "2672  estimators=100 max_depht=4 max_features=None  avg    0.9991   \n",
       "2639  estimators=100 max_depht=4 max_features=auto  avg    0.9991   \n",
       "2419   estimators=80 max_depht=4 max_features=auto  avg    0.9989   \n",
       "2452   estimators=80 max_depht=4 max_features=None  avg    0.9989   \n",
       "505                    Neighbors=2 algorithm=brute  avg    0.9987   \n",
       "494                  Neighbors=2 algorithm=kd_tree  avg    0.9987   \n",
       "483                Neighbors=2 algorithm=ball_tree  avg    0.9987   \n",
       "\n",
       "      Execution Time True Positive False Positive False Negative  \\\n",
       "472           1.2569        3881.1            0.9            1.6   \n",
       "461           0.1465        3881.1            0.9            1.6   \n",
       "450           0.1456        3881.1            0.9            1.6   \n",
       "2672          5.4914        3881.6            0.4            4.2   \n",
       "2639          5.5422        3881.6            0.4            4.2   \n",
       "2419          4.4829        3881.4            0.6            4.8   \n",
       "2452          4.4239        3881.4            0.6            4.8   \n",
       "505           0.9589        3881.4            0.6            5.5   \n",
       "494           0.1469        3881.4            0.6            5.5   \n",
       "483           0.1433        3881.4            0.6            5.5   \n",
       "\n",
       "     True Negative  Precision  Recall Model  \n",
       "472          982.4     0.9998  0.9996  None  \n",
       "461          982.4     0.9998  0.9996  None  \n",
       "450          982.4     0.9998  0.9996  None  \n",
       "2672         979.8     0.9999  0.9989  None  \n",
       "2639         979.8     0.9999  0.9989  None  \n",
       "2419         979.2     0.9998  0.9988  None  \n",
       "2452         979.2     0.9998  0.9988  None  \n",
       "505          978.5     0.9998  0.9986  None  \n",
       "494          978.5     0.9998  0.9986  None  \n",
       "483          978.5     0.9998  0.9986  None  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_stats.loc[(class_stats['CV'] == 'avg')].sort_values(by=['Accuracy'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We started our analysis of the classification models by looking at the top 10 models by accuracy.  Here we noted that KNN takes the top 3 spots with the best accuracy and takes 6 spots out of the 10 total with Gradient Boosting taking the remaining 4 spots.  We also noted that the KNN models have a much faster execution time compared to the Griadient Boosting models.  It is also worth noting that all models almost never produce a False Positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEFCAYAAAASWssjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERGJpWQhIiKxlCxERCTW/wO5OKzrKMG+4AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15088e2bc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.boxplot(x=\"Model Name\", y=\"Accuracy\", data=class_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### From all of the accuracies in the above boxplots we saw that KNN is the clear winner in terms of accuracy score. It had the smallest range that was centered the closest to 100%. Gradient boosting also has many values with high accuracy but the second widest range in performance after Random Forest. However, we also saw that KNN was least affected by any prameter changes, whereas Random Forest and Gradient Boosting had great variance based off of how parameters were changed and in the best case could only hit accuracies near KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEFCAYAAADNFLE8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15088d40d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEFCAYAAAAPCDf9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15089179080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_class = class_stats.loc[(class_stats['CV'] == 'avg')].sort_values(by=['Accuracy'], ascending=False).head(50)\n",
    "ax = sns.boxplot(x=\"Model Name\", y=\"Accuracy\", data=temp_class)\n",
    "plt.show()\n",
    "ax = sns.countplot(x=\"Model Name\", data=temp_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### To further understand this, we looked at only the top 50 accuracies and we saw that logistic regression falls off the chart since it had 0 values in the top 50.  Accuracy-wise, we still see KNN as the front runner with Gradient Boosting in close second reaching high accuracies in many cases and wtih Random Forest falling into last place overall in terms of count and overall performance.  We also noted that Gradient Boosting has the most in the top 50 (~35) however this may be caused by more parameters that need to be tested when compared to KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEFCAYAAAAYKqc0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGshwl6QGMtwlqYEMd0lqoP8BUflSFh8g7gUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x150891b5240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.boxplot(x=\"Model Name\", y=\"Execution Time\", data=temp_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Finally, we looked at the execution time for our top 50 and saw that both KNN and Random Forest usually ran in less than 1 second while Gradient Boosting usually took much longer: on an average four to five seconds.  The accuracy over execution time would need to be considered depending on any production requirements for these models but in this case KNN clearly looks like the overall winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Paramaters</th>\n",
       "      <th>CV</th>\n",
       "      <th>Execution Time</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=100 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.6636</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2606</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=100 max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.6671</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=80 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.5830</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=80 max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.5767</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=60 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2166</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=60 max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.4679</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=40 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.3328</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=40 max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.3340</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=20 max_depht=10 max_features=None</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.2944</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=20 max_depht=10 max_features=auto</td>\n",
       "      <td>avg</td>\n",
       "      <td>0.3144</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Type  Model Name                                     Paramaters  \\\n",
       "2639  regression  Extra Tree  estimators=100 max_depht=10 max_features=None   \n",
       "2606  regression  Extra Tree  estimators=100 max_depht=10 max_features=auto   \n",
       "2419  regression  Extra Tree   estimators=80 max_depht=10 max_features=None   \n",
       "2386  regression  Extra Tree   estimators=80 max_depht=10 max_features=auto   \n",
       "2199  regression  Extra Tree   estimators=60 max_depht=10 max_features=None   \n",
       "2166  regression  Extra Tree   estimators=60 max_depht=10 max_features=auto   \n",
       "1979  regression  Extra Tree   estimators=40 max_depht=10 max_features=None   \n",
       "1946  regression  Extra Tree   estimators=40 max_depht=10 max_features=auto   \n",
       "1759  regression  Extra Tree   estimators=20 max_depht=10 max_features=None   \n",
       "1726  regression  Extra Tree   estimators=20 max_depht=10 max_features=auto   \n",
       "\n",
       "       CV  Execution Time     MAE     MSE Model  \n",
       "2639  avg          0.6636  0.0128  0.0024  None  \n",
       "2606  avg          0.6671  0.0128  0.0024  None  \n",
       "2419  avg          0.5830  0.0127  0.0024  None  \n",
       "2386  avg          0.5767  0.0127  0.0024  None  \n",
       "2199  avg          0.4430  0.0127  0.0024  None  \n",
       "2166  avg          0.4679  0.0127  0.0024  None  \n",
       "1979  avg          0.3328  0.0126  0.0024  None  \n",
       "1946  avg          0.3340  0.0126  0.0024  None  \n",
       "1759  avg          0.2944  0.0126  0.0026  None  \n",
       "1726  avg          0.3144  0.0126  0.0026  None  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stats.loc[(reg_stats['CV'] == 'avg')].sort_values(by=['MSE'], ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### First, we looked at the top 10 regression models by MSE similarly to how we looked at accuracy for our classification models.  Here we saw that \"Extra Tree\" takes every slot in our top 10 and that the top 5 have the same MSE value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQkFERGIKBRERiSkUREQk9v8BWwoYwh4U35gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15089179550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.boxplot(x=\"Model Name\", y=\"MSE\", data=reg_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Here we noticed that on average Random Forest performs better and has less fluctuation based on any parameter changes.  However, we know from the above output that this will change as we look at only higher performers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEICAYAAACuxNj9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9P8BRBi1vLqGm+gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x150891220f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEFCAYAAAAPCDf9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15088af5080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_class = reg_stats.loc[(reg_stats['CV'] == 'avg')].sort_values(by=['MSE'], ascending=True).head(50)\n",
    "ax = sns.boxplot(x=\"Model Name\", y=\"MSE\", data=temp_class)\n",
    "plt.show()\n",
    "ax = sns.countplot(x=\"Model Name\", data=temp_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We noticed that \"Decision Tree\" barely made the cut, only having about 2-3 in the top 50 and having expectedly poorer performance when compared to the average \"Extra Tree\" and \"Random Forest\" model.  We also saw a large number of \"Random Forest\" models present which further drives home the point that the models perform well and had less variance compared to the other model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEFCAYAAADqujDUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngbWw3MB/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15088d459b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.boxplot(x=\"Model Name\", y=\"Execution Time\", data=temp_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Finally, we looked at execution time for the top 50 regression models and found that while \"Decision Tree\" barely made the cut off for the top 50, it is by far the fastest executing of the three.   Here, \"Extra Trees\" both out performed \"Random Forest\" in the arena of execution time and MSE.  For this case, \"Extra tree\" is the best all around model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss the advantages of each model for each classification task, if any. If\n",
    "there are not advantages, explain why. Is any model better than another? Is the\n",
    "difference significant with 95% confidence? Use proper statistical comparison methods.\n",
    "You must use statistical comparison techniquesâ€”be sure they are appropriate for your\n",
    "chosen method of validation as discussed in unit 7 of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Logistic Regression\n",
    "- Classic method that is well established and thus well accepted in industry\n",
    "- Can be strong against muli-collinearity when tested with both L1 and L2 penalties to identify (however this data set should not have this issue\n",
    "- Is very probabilistic in nature\n",
    "- generally low variance\n",
    "\n",
    "###### KNN\n",
    "- Does not require/make any assumptions about the data or normality of its distribution\n",
    "- Due to the simplicity of the algorithm it can be very efficient in execution time (as noted in this project)\n",
    "\n",
    "###### Random Forest\n",
    "- Has less variance compared to other trees since it compares many trees\n",
    "- Is much stronger to over fitting than basic decision trees \n",
    "- Is generally strong to muli-collinearity however this might skew feature importance and thus should still be handled before training\n",
    "\n",
    "###### Gradient Boosting\n",
    "- Sequentially builds trees which makes it easier to interpret when compared to random forest\n",
    "- Can preform better than Random Forest but hyper parameters can be sensitive when adjusting.\n",
    "\n",
    "###### Decision Tree\n",
    "- Very easy to interpret visually\n",
    "- Very fast to train\n",
    "- Easily handle categorical features\n",
    "\n",
    "###### Extra Trees\n",
    "- Similar to Random Forest in performance generally\n",
    "- Always use random split over a calculated split like Random Forest uses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ci(metric, df, model_name, model_para):\n",
    "    alpha = 0.95\n",
    "    stats = list()\n",
    "    for i in range(1,10,1):\n",
    "        score = df.loc[(df['CV'] == i) & (df['Model Name'] == model_name) & (df['Paramaters'] == model_para)][metric].values\n",
    "        stats.append(score)        \n",
    "    acc = row[metric]\n",
    "    p = ((1.0-alpha)/2.0) * 100\n",
    "    lower = max(0.0, np.percentile(stats, p))\n",
    "    p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "    upper = min(1.0, np.percentile(stats, p))\n",
    "    name = metric + ' in ' + str(model_name) + ' ' + str(model_para)\n",
    "    runtime = round(df.loc[(df['Model Name'] == model_name) & (df['Paramaters'] == model_para)]['Execution Time'].mean(),4)\n",
    "#    print(name)\n",
    "    print('%.1f CI for %s %0.4f and %0.4f with an average runtime of %0.4f' % (alpha*100, name, lower, upper, runtime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  The above code generates confidence intravels for whatever given metric across the CV splits for a given model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.0 CI for Accuracy in Gradient Boosting estimators=60 max_depht=4 max_features=auto 0.9953 and 0.9996 with an average runtime of 3.3706\n",
      "95.0 CI for Accuracy in KNN Neighbors=1 algorithm=ball_tree 0.9980 and 1.0000 with an average runtime of 0.1456\n",
      "95.0 CI for Accuracy in Logistic L1 C=1.0 fit_intercept=True 0.9575 and 0.9615 with an average runtime of 1.5609\n",
      "95.0 CI for Accuracy in Random Forrest estimators=60 max_depht=10 max_features=None 0.9930 and 0.9990 with an average runtime of 0.8580\n"
     ]
    }
   ],
   "source": [
    "test_models = class_stats.loc[class_stats.groupby(['Type','Model Name'])['Accuracy'].idxmax(), :]\n",
    "for i, row in test_models.iterrows():\n",
    "    make_ci(\"Accuracy\",class_stats, row['Model Name'], row['Paramaters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For the classification models we picked only the best performing model of each time.  Here we see only KNN has 100% in its CI while Random Forest and Gradient Boost both have 99.6% in their upper bounds.  Only Logistic regression falls short of this with a  96% on its upper bound.   None of these preform poorly as each is above 95%.  After taking into account average runtime KNN and Random Forest look more appealing as they both run in <1 second with KNN being the fastest with the highest accuracy.  However with such high accuracies and low run times I would not call any of these models bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.0 CI for MSE in Decision Tree splitter=best max_depht=8 max_features=auto 0.0021 and 0.0104 with an average runtime of 0.1822\n",
      "95.0 CI for MSE in Extra Tree estimators=100 max_depht=10 max_features=auto 0.0014 and 0.0033 with an average runtime of 0.6671\n",
      "95.0 CI for MSE in Random Forest estimators=100 max_depht=10 max_features=auto 0.0024 and 0.0057 with an average runtime of 1.1974\n"
     ]
    }
   ],
   "source": [
    "test_models = reg_stats.loc[reg_stats.groupby(['Type','Model Name'])['MSE'].idxmin(), :]\n",
    "for i, row in test_models.iterrows():\n",
    "    make_ci(\"MSE\",reg_stats, row['Model Name'], row['Paramaters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For the Regression models we also only looked at the top parameter models.  Seeing each with a < 0.01 score in lower bounds and only reaching 0.01 on the upper bound on Decision Tree it is hard to call any of these models poor performers.  When we take execution time into account we see Extra Tree begin to emerge as it runs in  <1 second in run time and has the best lower and upper bounds.  Again none of these models are bad performers and should not be discounted for possible production deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which attributes from your analysis are most important? Use proper\n",
    "methods discussed in class to evaluate the importance of different attributes. Discuss\n",
    "the results and hypothesize about why certain attributes are more important than others\n",
    "for a given classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Paramaters</th>\n",
       "      <th>CV</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Execution Time</th>\n",
       "      <th>True Positive</th>\n",
       "      <th>False Positive</th>\n",
       "      <th>False Negative</th>\n",
       "      <th>True Negative</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2198</th>\n",
       "      <td>classification</td>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>estimators=60 max_depht=4 max_features=auto</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.4319</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>984</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>classification</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Neighbors=1 algorithm=ball_tree</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.1461</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>984</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>KNeighborsClassifier(algorithm='ball_tree', le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>classification</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>L1 C=1.0 fit_intercept=True</td>\n",
       "      <td>10</td>\n",
       "      <td>0.963420</td>\n",
       "      <td>1.5773</td>\n",
       "      <td>3784</td>\n",
       "      <td>98</td>\n",
       "      <td>80</td>\n",
       "      <td>904</td>\n",
       "      <td>0.974755</td>\n",
       "      <td>0.979296</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>classification</td>\n",
       "      <td>Random Forrest</td>\n",
       "      <td>estimators=60 max_depht=10 max_features=None</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999178</td>\n",
       "      <td>0.7887</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>980</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998971</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Type         Model Name  \\\n",
       "2198  classification  Gradient Boosting   \n",
       "441   classification                KNN   \n",
       "405   classification           Logistic   \n",
       "1260  classification     Random Forrest   \n",
       "\n",
       "                                        Paramaters  CV  Accuracy  \\\n",
       "2198   estimators=60 max_depht=4 max_features=auto  10  1.000000   \n",
       "441                Neighbors=1 algorithm=ball_tree   2  1.000000   \n",
       "405                    L1 C=1.0 fit_intercept=True  10  0.963420   \n",
       "1260  estimators=60 max_depht=10 max_features=None   7  0.999178   \n",
       "\n",
       "      Execution Time True Positive False Positive False Negative  \\\n",
       "2198          3.4319          3882              0              0   \n",
       "441           0.1461          3882              0              0   \n",
       "405           1.5773          3784             98             80   \n",
       "1260          0.7887          3882              0              4   \n",
       "\n",
       "     True Negative  Precision    Recall  \\\n",
       "2198           984   1.000000  1.000000   \n",
       "441            984   1.000000  1.000000   \n",
       "405            904   0.974755  0.979296   \n",
       "1260           980   1.000000  0.998971   \n",
       "\n",
       "                                                  Model  \n",
       "2198  ([DecisionTreeRegressor(criterion='friedman_ms...  \n",
       "441   KNeighborsClassifier(algorithm='ball_tree', le...  \n",
       "405   LogisticRegression(C=1.0, class_weight=None, d...  \n",
       "1260  (DecisionTreeClassifier(class_weight=None, cri...  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_stats.loc[class_stats.groupby(['Type','Model Name'])['Accuracy'].idxmax(), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We wish to get feature importance and luckily for us Random Forest provides a method for this.  However instead of just taking from from some unknown Random Forest we will get the parameters for our best running model from above and use the same for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lea_salary_expense_pct</th>\n",
       "      <td>0.405863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_federal_perpupil_num</th>\n",
       "      <td>0.252237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_ap_pct_3_or_above</th>\n",
       "      <td>0.093113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit_code</th>\n",
       "      <td>0.052215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_sat_participation_pct</th>\n",
       "      <td>0.031047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-3 Years_LEA_Exp_Pct_Prin</th>\n",
       "      <td>0.019070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_services_expense_pct</th>\n",
       "      <td>0.018776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_instruct_equip_exp_pct</th>\n",
       "      <td>0.015680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_local_perpupil_num</th>\n",
       "      <td>0.015528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10+ Years_LEA_Exp_Pct_Prin</th>\n",
       "      <td>0.012181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <td>0.010536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_ap_participation_pct</th>\n",
       "      <td>0.009417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_stud_internet_comp_num</th>\n",
       "      <td>0.008687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BlackMalePct</th>\n",
       "      <td>0.008553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BlackFemalePct</th>\n",
       "      <td>0.006538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_perpupil_num</th>\n",
       "      <td>0.005865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_state_perpupil_num</th>\n",
       "      <td>0.005491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_supplies_expense_pct</th>\n",
       "      <td>0.004511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-10 Years_LEA_Exp_Pct_Prin</th>\n",
       "      <td>0.003363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_total_expense_num</th>\n",
       "      <td>0.003193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             importance\n",
       "lea_salary_expense_pct         0.405863\n",
       "lea_federal_perpupil_num       0.252237\n",
       "lea_ap_pct_3_or_above          0.093113\n",
       "unit_code                      0.052215\n",
       "lea_sat_participation_pct      0.031047\n",
       "0-3 Years_LEA_Exp_Pct_Prin     0.019070\n",
       "lea_services_expense_pct       0.018776\n",
       "lea_instruct_equip_exp_pct     0.015680\n",
       "lea_local_perpupil_num         0.015528\n",
       "10+ Years_LEA_Exp_Pct_Prin     0.012181\n",
       "Unnamed: 0                     0.010536\n",
       "lea_ap_participation_pct       0.009417\n",
       "lea_stud_internet_comp_num     0.008687\n",
       "BlackMalePct                   0.008553\n",
       "BlackFemalePct                 0.006538\n",
       "total_perpupil_num             0.005865\n",
       "lea_state_perpupil_num         0.005491\n",
       "lea_supplies_expense_pct       0.004511\n",
       "4-10 Years_LEA_Exp_Pct_Prin    0.003363\n",
       "lea_total_expense_num          0.003193"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf =RandomForestClassifier(max_depth=10, n_estimators=60, max_features=None, n_jobs=-1, random_state=101)\n",
    "\n",
    "X = df_foo\n",
    "y = df['sat_above_average'].values #this is the subset for classification\n",
    "rf_clf.fit(X, y)\n",
    "\n",
    "feature_importances = pd.DataFrame(rf_clf.feature_importances_, index = X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "feature_importances.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A few of the top 20 stand out as most obvious \n",
    "- lea_salary_expense_pct - The more money a school gives to the teachers the better quality teaches you get and thus more prepared students\n",
    "- lea_federal_perpupil_num - The more money the government gives to the school the more likely they can get good teachers and materials\n",
    "- lea_sat_participation_pct - The number of students take the test usually is associated with the school pushing and preping\n",
    "- lea_ap_pct_3_or_above\t- The more students that participate in AP courses the better they will do since most AP courses are at higher levels than the SAT\n",
    "- lea_ap_participation_pct\t- Much like the SAT participation the more a school focuses on AP classes the more prepared in general the students will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Paramaters</th>\n",
       "      <th>CV</th>\n",
       "      <th>Execution Time</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>regression</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>splitter=best max_depht=8 max_features=auto</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1661</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>DecisionTreeRegressor(criterion='mse', max_dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597</th>\n",
       "      <td>regression</td>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>estimators=100 max_depht=10 max_features=auto</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6556</td>\n",
       "      <td>0.010386</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>(ExtraTreeRegressor(criterion='mse', max_depth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>regression</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>estimators=100 max_depht=10 max_features=auto</td>\n",
       "      <td>2</td>\n",
       "      <td>1.2090</td>\n",
       "      <td>0.010004</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>(DecisionTreeRegressor(criterion='mse', max_de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Type     Model Name  \\\n",
       "267   regression  Decision Tree   \n",
       "2597  regression     Extra Tree   \n",
       "1497  regression  Random Forest   \n",
       "\n",
       "                                         Paramaters CV  Execution Time  \\\n",
       "267     splitter=best max_depht=8 max_features=auto  4          0.1661   \n",
       "2597  estimators=100 max_depht=10 max_features=auto  2          0.6556   \n",
       "1497  estimators=100 max_depht=10 max_features=auto  2          1.2090   \n",
       "\n",
       "           MAE       MSE                                              Model  \n",
       "267   0.001989  0.001354  DecisionTreeRegressor(criterion='mse', max_dep...  \n",
       "2597  0.010386  0.001328  (ExtraTreeRegressor(criterion='mse', max_depth...  \n",
       "1497  0.010004  0.002304  (DecisionTreeRegressor(criterion='mse', max_de...  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stats.loc[reg_stats.groupby(['Type','Model Name'])['MSE'].idxmin(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lea_salary_expense_pct</th>\n",
       "      <td>0.348627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_federal_perpupil_num</th>\n",
       "      <td>0.202745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_ap_pct_3_or_above</th>\n",
       "      <td>0.169931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_sat_participation_pct</th>\n",
       "      <td>0.118545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_supplies_expense_pct</th>\n",
       "      <td>0.033748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MinorityFemalePct</th>\n",
       "      <td>0.020348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit_code</th>\n",
       "      <td>0.017390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_local_perpupil_num</th>\n",
       "      <td>0.011599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_ap_participation_pct</th>\n",
       "      <td>0.011341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_state_perpupil_num</th>\n",
       "      <td>0.011049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10+ Years_LEA_Exp_Pct_Prin</th>\n",
       "      <td>0.008941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_services_expense_pct</th>\n",
       "      <td>0.006128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-10 Years_LEA_Exp_Pct_Prin</th>\n",
       "      <td>0.005716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_perpupil_num</th>\n",
       "      <td>0.004362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_instruct_equip_exp_pct</th>\n",
       "      <td>0.004259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_total_expense_num</th>\n",
       "      <td>0.004250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-3 Years_LEA_Exp_Pct_Prin</th>\n",
       "      <td>0.003662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea_stud_internet_comp_num</th>\n",
       "      <td>0.003608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <td>0.003150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BlackMalePct</th>\n",
       "      <td>0.002589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             importance\n",
       "lea_salary_expense_pct         0.348627\n",
       "lea_federal_perpupil_num       0.202745\n",
       "lea_ap_pct_3_or_above          0.169931\n",
       "lea_sat_participation_pct      0.118545\n",
       "lea_supplies_expense_pct       0.033748\n",
       "MinorityFemalePct              0.020348\n",
       "unit_code                      0.017390\n",
       "lea_local_perpupil_num         0.011599\n",
       "lea_ap_participation_pct       0.011341\n",
       "lea_state_perpupil_num         0.011049\n",
       "10+ Years_LEA_Exp_Pct_Prin     0.008941\n",
       "lea_services_expense_pct       0.006128\n",
       "4-10 Years_LEA_Exp_Pct_Prin    0.005716\n",
       "total_perpupil_num             0.004362\n",
       "lea_instruct_equip_exp_pct     0.004259\n",
       "lea_total_expense_num          0.004250\n",
       "0-3 Years_LEA_Exp_Pct_Prin     0.003662\n",
       "lea_stud_internet_comp_num     0.003608\n",
       "Unnamed: 0                     0.003150\n",
       "BlackMalePct                   0.002589"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf =RandomForestRegressor(max_depth=10, n_estimators=100, max_features='auto', n_jobs=-1, random_state=101)\n",
    "\n",
    "X = df_foo\n",
    "y = df['lea_sat_avg_score_num'].values #this is the subset for regression\n",
    "rf_clf.fit(X, y)\n",
    "\n",
    "feature_importances = pd.DataFrame(rf_clf.feature_importances_, index = X.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "feature_importances.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Since both of our classification and regression problems are similar it is no surprise that we see most of the same values found here.  The only stand out is we now see MinorityFemalePct float toward the top.  This might be helpful since both Gender and Race have an effect on average SAT scores and now that we are predicting an actual score instead of classifying if they preform above the national average finer details like this matter more.\n",
    "- lea_salary_expense_pct - The more money a school gives to the teachers the better quality teaches you get and thus more prepared students\n",
    "- lea_federal_perpupil_num - The more money the government gives to the school the more likely they can get good teachers and materials\n",
    "- lea_sat_participation_pct - The number of students take the test usually is associated with the school pushing and peeping\n",
    "- lea_ap_pct_3_or_above\t- The more students that participate in AP courses the better they will do since most AP courses are at higher levels than the SAT\n",
    "- lea_ap_participation_pct\t- Much like the SAT participation the more a school focuses on AP classes the more prepared in general the students will be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How useful is your model for interested parties (i.e., the companies or organizations\n",
    "that might want to use it for prediction)? How would you measure the model's value if it\n",
    "was used by these parties? How would your deploy your model for interested parties?\n",
    "What other data should be collected? How often would the model need to be updated,\n",
    "etc.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  This model could be for interest for several organizations to its accuracy in predicting which attributes are most related to increased or above average SAT score.  For example, we could envision that school districts could leverage this model to:\n",
    "1.    Target teachers with a specific range of experience and education level\n",
    "2.    How and where to allocate funds to most effectively boost SAT scores.\n",
    "3.    Identify which social organization to partner with to offer after-school programs the may result in better performance.\n",
    "\n",
    "###### Additionally, this model could be monetized by business trying to setup tutoring center by identifying which districts have below and more importantly are predicted to have below average SAT scores.\n",
    "\n",
    "###### As for a deployment method the most financially savvy method is via a web API since we as the user consumes the model we still get to see when they consume and what data they send in for us to generate a prediction off off.  It is also easier to tie into third party applications and include a feedback loop to understand when/if our model is off for future retraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exceptional Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have free reign to provide additional analyses.\n",
    "\n",
    "One idea: grid search parameters in a parallelized fashion and visualize the\n",
    "performances across attributes. Which parameters are most significant for making a\n",
    "good model for each classification algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Model evaluation 3\n",
    "###### For out exceptional work we focused on using models we haven't had much exposure to in class.  For that we stayed with tree based algorithms suck as Random Forest and Extra Trees.  We also tested Gradient Boosting.  For each of our models we fictionalized the running and storing of our models along with their performance metrics\n",
    "\n",
    "###### Since we already had our model runs in a function it allowed us to play with a high level of parameter tweaking ending with over ~500 separate models and ~5000 models created when you count the CV runs.\n",
    "\n",
    "###### The storing of our data made it easy to regenerate any model like we used in model evaluation 6 for creating the list of feature importance. \n",
    "\n",
    "##### Model evaluation 5\n",
    "###### Again we  fictionalized   our methods so we can quickly and easily rerun the CI test with any test statistic.\n",
    "\n",
    "#### The overall idea of fictionalizing the code was so that this ipython notebook could easily be pushed to a standard .py file of only the functions that could then be called in some standard loops and be treated as a one stop first go for young data scientists to see performance across a wide range of models with only the data cleaning task left to them.  So for the deployment task the script itself could be considered for deployment and not just the most appropriate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
